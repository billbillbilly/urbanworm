{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Urban-Worm","text":"<p>A python package for studying urban environment imagery with Llama vison model</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Urban-Worm is a Python library that integrates remote sensing imagery, street view data, and multimodal model to assess urban units. Using APIs for data collection and Llama 3.2 vision for inference, Urban-Worm is designed to support the automation of the evaluation for urban environments, including roof integrity, structural condition, landscape quality, and urban perception.</p> <p> </p>"},{"location":"#features","title":"Features","text":"<ul> <li>run Llama 3.2 vision locally with local datasets and remain information privacy</li> <li>download building footprints from OSM and global building released by Bing map </li> <li>search and clip aerial and street view images (via APIs) based on urban units such as parcel and building footprint data</li> <li>automatically calibrate the oritation of panorama street view and the extent of aerial image</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The package is heavily built on Ollama client, Ollama-python, and Llama 3.2 Vision. Credit goes to the developers of these projects.</p> <ul> <li>ollama</li> <li>ollama-python</li> <li>structured outputs</li> <li>llama 3.2 vision</li> </ul> <p>The functionality about sourcing and processing GIS data (satellite &amp; street view imagery) is built on the following open projects. Credit goes to the developers of these projects.</p> <ul> <li>tms2geotiff</li> <li>GlobalMLBuildingFootprints</li> <li>Mapillary API</li> </ul> <p>The development of this package is supported and inspired by the city of Detroit.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#step-1-install-ollama-client","title":"Step 1: install Ollama client","text":"<p>Please make sure Ollama is installed before installing urban-worm</p>"},{"location":"installation/#linux","title":"Linux","text":"<p>For Linux, users can also install ollama by running in the terminal:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"installation/#mac","title":"MAC","text":"<p>For MacOS, users can also install ollama using <code>brew</code>:</p> <pre><code>brew install ollama\n</code></pre> <p>To install <code>brew</code>, run in the terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>After installing <code>brew</code>, you will see a following instruction:</p> <pre><code>==&gt; Next steps:\n- Run these commands in your terminal to add Homebrew to your PATH:\n    echo &gt;&gt; /Users/yourusername/.bash_profile\n    echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/yourusername/.bash_profile\n    eval \"$(/opt/homebrew/bin/brew shellenv)\"\n</code></pre>"},{"location":"installation/#windows","title":"Windows","text":"<p>Windows users should directly install the Ollama client</p>"},{"location":"installation/#step-2-install-gdal-first","title":"Step 2: install GDAL first","text":"<p>For macOS, Linux, and Windows users, <code>gdal</code> may need to be installed at very begining using <code>conda</code>. </p>"},{"location":"installation/#install-conda","title":"install conda","text":"<p>Please download and install Anaconda to use <code>conda</code>.</p>"},{"location":"installation/#install-gdal","title":"install GDAL","text":"<p>If the installation method above does not work, try to install with <code>conda</code>:</p> <pre><code> conda install -c conda-forge gdal\n</code></pre> <p>Mac users may install <code>gdal</code> (if the installation method below does not work, try to install with conda):</p> <pre><code> brew install gdal\n</code></pre>"},{"location":"installation/#note","title":"Note","text":"<p>if you come across error like <code>conda command not found</code> when using <code>conda</code>, please refer following solutions to add Conda to the PATH:</p> <ul> <li>Linux: <code>export PATH=~/anaconda3/bin:$PATH</code> (source)</li> <li>Mac: <code>export PATH=\"/home/username/miniconda/bin:$PATH\"</code>. Please make sure to replace <code>/home/username/miniconda</code> with your actual path (source)</li> <li>Windows: Open Anaconda Prompt &gt; Check Conda Installed Location: <code>where conda</code> &gt; Open Advanced System Settings &gt; Click on Environment Variables &gt; Edit Path &gt; Add New Path: </li> </ul> <pre><code> C:\\Users\\&lt;username&gt;\\Anaconda3\\Scripts\n C:\\Users\\&lt;username&gt;\\Anaconda3\n C:\\Users\\&lt;username&gt;\\Anaconda3\\Library\\bin\n</code></pre> <p>(source)</p>"},{"location":"installation/#step-3-install-urabn-worm-from-pypi","title":"Step 3: install urabn-worm from PyPi","text":"<p>The package urabnworm can be installed with <code>pip</code>:</p> <pre><code>pip install urban-worm \n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use urban-worm in a project:</p> <pre><code>from urbanworm import UrbanDataSet\n</code></pre>"},{"location":"usage/#single-image","title":"single-image","text":"<pre><code># load a local image\ndata = UrbanDataSet(image = './docs/data/test1.jpg')\nsystem = '''\n    Given a top view image, you are going to roughly estimate house conditions. Your answer should be based only on your observation. \n    The format of your response must include question, answer (yes or no), explanation (within 50 words)\n'''\nprompt = '''\n    Is there any damage on the roof?\n'''\ndata.oneImgChat(system=system, prompt=prompt)\n# output:\n# {'question': 'Is there any damage on the roof?',\n#  'answer': 'no',\n#  'explanation': 'No visible signs of damage or wear on the roof',\n#  'img': '/9j/4AAQSkZ...'}\n</code></pre>"},{"location":"usage/#multiple-images-using-osm-data-and-mapillary-api","title":"multiple images using OSM data and Mapillary API","text":"<p>Get building footprints as units and collect satellite and street view images based on each unit. Finally, chat with MLLM model for each unit based on collected images.</p> <p>To get a token/key to access data via mapillary api, please create an acount and apply on Mapillary</p> <pre><code>bbox = (-83.235572,42.348092,-83.235154,42.348806)\ndata = UrbanDataSet()\ndata.bbox2Buildings(bbox, source='osm')\n\nsystem = '''\n    Given a top view image or street view images, you are going to roughly estimate house conditions. \n    Your answer should be based only on your observation. \n    The format of your response must include question, answer (yes or no), explanation (within 50 words) for each question.\n'''\n\nprompt = {\n    'top': '''\n        Is there any damage on the roof?\n    ''',\n    'street': '''\n        Is the wall missing or damaged?\n        Is the yard maintained well?\n    '''\n}\n\n# add the Mapillary key\ndata.mapillary_key = 'MLY|......'\n# use both the aerial and street view images (with type='both')\ndata.loopUnitChat(system=system, prompt=prompt, type='both', epsg=2253)\n# convert results into GeoDataframe\ndata.to_gdf()\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/","title":"UrbanDataSet","text":"<p>Dataset class for urban imagery inference using MLLM.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image.</p> <code>None</code> <code>images</code> <code>list</code> <p>The list of image paths.</p> <code>None</code> <code>units</code> <code>str</code> <p>The path to the shapefile.</p> <code>None</code> <code>format</code> <code>Response</code> <p>The response format.</p> <code>None</code> <code>mapillary_key</code> <code>str</code> <p>The Mapillary API key.</p> <code>None</code> <code>random_sample</code> <code>int</code> <p>The number of random samples.</p> <code>None</code> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>class UrbanDataSet:\n    '''\n    Dataset class for urban imagery inference using MLLM.\n\n    Args:\n        image (str): The path to the image.\n        images (list): The list of image paths.\n        units (str): The path to the shapefile.\n        format (Response): The response format.\n        mapillary_key (str): The Mapillary API key.\n        random_sample (int): The number of random samples.\n    '''\n    def __init__(self, image=None, images:list=None, units:str=None, format=None, mapillary_key=None, random_sample=None):\n        if image != None and detect_input_type(image) == 'image_path':\n            self.img = encode_image_to_base64(image)\n        else:\n            self.img = image\n\n        if images != None and detect_input_type(images[0]) == 'image_path':\n            self.imgs = [encode_image_to_base64(im) for im in images]\n        else:\n            self.imgs = images\n\n        if random_sample != None and units != None:\n            self.units = loadSHP(units).sample(random_sample)\n        elif random_sample == None and units != None:\n            self.units = loadSHP(units)\n        else:\n            self.units = units\n\n        if format == None:\n            self.format = Response()\n        else:\n            self.format = format\n\n        self.mapillary_key = mapillary_key\n\n        self.results = None\n\n        self.preload_model()\n\n    def preload_model(self):\n        \"\"\"\n        Ensures that the required Ollama model is available.\n        If not, it automatically pulls the model.\n        \"\"\"\n\n        import ollama\n\n        model_name = \"llama3.2-vision\"\n        try:\n            ollama.pull(model_name)\n\n        except Exception as e:\n            print(f\"Warning: Ollama is not installed or failed to check models: {e}\")\n            print(\"Please install Ollama client: https://github.com/ollama/ollama/tree/main\")\n            raise RuntimeError(\"Ollama not available. Install it before running.\")\n\n    def bbox2Buildings(self, bbox, source='osm', min_area=0, max_area=None, random_sample=None):\n        '''\n        This function is used to extract buildings from OpenStreetMap using the bbox.\n\n        Args:\n            bbox (list): The bounding box.\n            source (str): The source of the buildings. ['osm', 'being']\n            min_area (int): The minimum area.\n            max_area (int): The maximum area.\n            random_sample (int): The number of random samples.\n\n        return (str): The number of buildings found in the bounding box\n        '''\n        if source == 'osm':\n            buildings = getOSMbuildings(bbox, min_area, max_area)\n        elif source == 'being':\n            buildings = getGlobalMLBuilding(bbox, min_area, max_area)\n        if buildings is None or buildings.empty:\n            if source == 'osm':\n                return \"No buildings found in the bounding box. Please check https://overpass-turbo.eu/ for areas with buildings.\"\n            if source == 'being':\n                return \"No buildings found in the bounding box. Please check https://github.com/microsoft/GlobalMLBuildingFootprints for areas with buildings.\"\n        if random_sample != None:\n            buildings = buildings.sample(random_sample)\n        self.units = buildings\n        return f\"{len(buildings)} buildings found in the bounding box.\"\n\n    def oneImgChat(self, system=None, prompt=None, \n                   temp=0.0, top_k=0.8, top_p=0.8, \n                   saveImg:bool=True):\n\n        '''\n        chat with MLLM model with one image.\n\n        Args:\n            system (optinal): The system message.\n            prompt (str): The prompt message.\n            img (str): The image path.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            saveImg (bool): The saveImg for save each image in base64 format in the output.\n\n        return (dict): A dictionary includes questions/messages, responses/answers, and image base64 (if required) \n        '''\n\n        print(\"Inference starts ...\")\n        r = self.LLM_chat(system=system, prompt=prompt, img=[self.img], \n                          temp=temp, top_k=top_k, top_p=top_p)\n        r = dict(r.responses[0])\n        if saveImg:\n            r['img'] = self.img\n        return r\n\n    def loopImgChat(self, system=None, prompt=None, \n                    temp=0.0, top_k=0.8, top_p=0.8,\n                    saveImg:bool=True, progressBar:bool=False):\n        '''\n        chat with MLLM model for each image.\n\n        Args:\n            system (optinal): The system message.\n            prompt (str): The prompt message.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            saveImg (bool): The saveImg for save each image in base64 format in the output.\n            progressBar (bool): The progress bar for showing the progress of data analysis over the units\n\n        return (list): A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)\n        '''\n\n        from tqdm import tqdm\n\n        res = []\n        for i in tqdm(range(len(self.imgs)), desc=\"Processing...\", ncols=75, disable=progressBar):\n        # for i in range(len(self.imgs)):\n            img = self.imgs[i]\n            r = self.LLM_chat(system=system, prompt=prompt, img=[img], \n                              temp=temp, top_k=top_k, top_p=top_p)\n            r = dict(r.responses[0])\n            if saveImg:\n                im = {'img': img}\n                res += [{**r, **im}]\n            else:\n                res += [r]\n        return res\n\n    def loopUnitChat(self, system=None, prompt:dict=None, \n                     temp:float=0.0, top_k:float=0.8, top_p:float=0.8, \n                     type:str='top', epsg:int=None, multi:bool=False, \n                     sv_fov:int=80, sv_pitch:int=10, sv_size:tuple=(300,400),\n                     saveImg:bool=True, progressBar:bool=False):\n        '''\n        chat with MLLM model for each unit in the shapefile.\n        example prompt:\n        prompt = {\n            'top': ''\n                Is there any damage on the roof?\n            '',\n            'street': ''\n                Is the wall missing or damaged?\n                Is the yard maintained well?\n            ''\n        }\n\n        Args:\n            system (optinal): The system message.\n            prompt (dict): The prompt message for either top or street view or both.\n            img (str): The image path.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            type (str): The type of image to process.\n            epsg (int): The EPSG code (required when type='street' or type='both').\n            multi (bool): The multi flag for multiple street view images for one unit.\n            sv_fov (int): The horizontal field of view of the image expressed in degrees(required when type='street' or type='both').\n            sv_pitch (int): The up or down angle of the camera relative to the Street View vehicle (required when type='street' or type='both').\n            sv_size (tuple): The height and width (height,width) for the street image (required when type='street' or type='both').\n            saveImg (bool): The saveImg for save each image in base64 format in the output.\n            progressBar (bool): The progress bar for showing the progress of data analysis over the units\n\n        return (dict): A dictionary includes questions/messages, responses/answers, and image base64 (if required) for each unit\n        '''\n\n        from tqdm import tqdm\n\n        if type == 'top' and 'top' not in prompt:\n            return \"Please provide prompt for top view images when type='top'\"\n        if type == 'street' and 'street' not in prompt:\n            return \"Please provide prompt for street view images when type='street'\"\n        if type == 'both' and 'top' not in prompt and 'street' not in prompt:\n            return \"Please provide prompt for both top and street view images when type='both'\"\n\n        dic = {\n            \"lon\": [],\n            \"lat\": [],\n        }\n\n        top_view_imgs = {'top_view_base64':[]}\n        street_view_imgs = {'street_view_base64':[]}\n\n        for i in tqdm(range(len(self.units)), desc=\"Processing...\", ncols=75, disable=progressBar):\n        # for i in range(len(self.units)):\n            # Get the extent of one polygon from the filtered GeoDataFrame\n            polygon = self.units.geometry.iloc[i]\n            centroid = polygon.centroid\n\n            dic['lon'].append(centroid.x)\n            dic['lat'].append(centroid.y)\n\n            if type == 'top' or type == 'both':\n                # Convert meters to degrees dynamically based on latitude\n                # Approximate adjustment (5 meters)\n                degree_offset = meters_to_degrees(5, centroid.y)  # Convert 5m to degrees\n                polygon = polygon.buffer(degree_offset)\n                # Compute bounding box\n                minx, miny, maxx, maxy = polygon.bounds\n                bbox = [minx, miny, maxx, maxy]\n\n                # Create a temporary file\n                with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as temp_file:\n                    image = temp_file.name\n                # Download data using tms_to_geotiff\n                tms_to_geotiff(output=image, bbox=bbox, zoom=22, \n                               source=\"SATELLITE\", \n                               overwrite=True)\n                # Clip the image with the polygon\n                with rasterio.open(image) as src:\n                    # Reproject the polygon back to match raster CRS\n                    polygon = self.units.to_crs(src.crs).geometry.iloc[i]\n                    out_image, out_transform = mask(src, [polygon], crop=True)\n                    out_meta = src.meta.copy()\n\n                out_meta.update({\n                    \"driver\": \"JPEG\",\n                    \"height\": out_image.shape[1],\n                    \"width\": out_image.shape[2],\n                    \"transform\": out_transform,\n                    \"count\": 3 #Ensure RGB (3 bands)\n                })\n\n                # Create a temporary file for the clipped JPEG\n                with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as temp_jpg:\n                    clipped_image = temp_jpg.name\n                with rasterio.open(clipped_image, \"w\", **out_meta) as dest:\n                    dest.write(out_image)\n                # clean up temp file\n                os.remove(image)\n\n                # convert image into base64\n                clipped_image_base64 = encode_image_to_base64(clipped_image)\n                top_view_imgs['top_view_base64'] += [clipped_image_base64]\n\n                # process aerial image\n                top_res = self.LLM_chat(system=system, \n                                    prompt=prompt[\"top\"], \n                                    img=[clipped_image], \n                                    temp=temp, \n                                    top_k=top_k, \n                                    top_p=top_p)\n                # initialize the list\n                if i == 0:\n                    dic['top_view'] = []\n                if saveImg:\n                    dic['top_view'].append(top_res.responses)\n\n                # clean up temp file\n                os.remove(clipped_image)\n\n            # process street view image\n            if (type == 'street' or type == 'both') and epsg != None and self.mapillary_key != None:\n                input_svis = getSV(centroid, epsg, self.mapillary_key, multi=multi, \n                                   fov=sv_fov, pitch=sv_pitch, height=sv_size[0], width=sv_size[1])\n                if None not in input_svis:\n                    # save imgs\n                    if saveImg:\n                        street_view_imgs['street_view_base64'] += [input_svis]\n                    # inference\n                    res = self.LLM_chat(system=system, \n                                        prompt=prompt[\"street\"], \n                                        img=input_svis, \n                                        temp=temp, \n                                        top_k=top_k, \n                                        top_p=top_p)\n                    # initialize the list\n                    if i == 0:\n                        dic['street_view'] = []\n                    if multi:\n                        dic['street_view'] += [res]\n                    else:\n                        dic['street_view'] += [res.responses]\n\n        self.results = {'from_loopUnitChat':dic, 'base64_imgs':{**top_view_imgs, **street_view_imgs}}\n        return dic\n\n    def to_gdf(self):\n        '''\n        Convert output from MLLM into a GeoDataframe,\n        including coordinates, questions, responses, input images (base64)\n\n        return (GeoDataframe): A GeoDataframe converted from the results \n        '''\n\n        import pandas as pd\n        import copy\n\n        if self.results is not None:\n            if 'from_loopUnitChat' in self.results:\n                res_df = response2gdf(self.results['from_loopUnitChat'])\n                img_dic = copy.deepcopy(self.results['base64_imgs'])\n                if img_dic['top_view_base64'] != [] or img_dic['street_view_base64'] != []:\n                    if img_dic['top_view_base64'] == []:\n                        img_dic.pop(\"top_view_base64\")\n                    if img_dic['street_view_base64'] == []:\n                        img_dic.pop(\"street_view_base64\")\n                    imgs_df = pd.DataFrame(img_dic)\n                    return pd.concat([res_df, imgs_df], axis=1)\n                else:\n                    return res_df\n            else:\n                return \"This method can only support the output of '.loopUnitChat()' method\"\n        else:\n            return \"This method can only be called after running the '.loopUnitChat()' method\"\n\n    def LLM_chat(self, system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None):\n        '''\n        This function is used to chat with the LLM model with a list of images.\n\n        Args:\n            system (str): The system message.\n            prompt (str): The user message.\n            img (list): The list of image paths.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n\n        return (Response/list): an Response object or a list of QnA objects\n        '''\n\n        if prompt != None and img != None:\n            if len(img) == 1:\n                return self.chat(system, prompt, img[0], temp, top_k, top_p)\n            elif len(img) == 3:\n                res = []\n                system = f'You are analyzing aerial or street view images. For street view, you should just foucus on the building and yard in the middle. {system}'\n                for i in range(len(img)):\n                    r = self.chat(system, prompt, img[i], temp, top_k, top_p)\n                    res += [r.responses]\n                return res\n\n    def chat(self, system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None):\n        '''\n        This function is used to chat with the LLM model.'\n\n        Args:\n            system (str): The system message.\n            prompt (str): The user message.\n            img (str): The image path.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n\n        return (Response): Response object\n        '''\n        res = ollama.chat(\n            model='llama3.2-vision',\n            format=self.format.model_json_schema(),\n            messages=[\n                {\n                    'role': 'system',\n                    'content': system\n                },\n                {\n                    'role': 'user',\n                    'content': prompt,\n                    'images': [img]\n                }\n            ],\n            options={\n                \"temperature\":temp,\n                \"top_k\":top_k,\n                \"top_p\":top_p\n            }\n        )\n        return self.format.model_validate_json(res.message.content)\n\n    def plotBase64(self, img):\n        '''\n        plot a single base64 image\n        '''\n        plot_base64_image(img)\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.LLM_chat","title":"<code>LLM_chat(system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None)</code>","text":"<p>This function is used to chat with the LLM model with a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The user message.</p> <code>None</code> <code>img</code> <code>list</code> <p>The list of image paths.</p> <code>None</code> <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>None</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>None</code> <p>return (Response/list): an Response object or a list of QnA objects</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def LLM_chat(self, system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None):\n    '''\n    This function is used to chat with the LLM model with a list of images.\n\n    Args:\n        system (str): The system message.\n        prompt (str): The user message.\n        img (list): The list of image paths.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n\n    return (Response/list): an Response object or a list of QnA objects\n    '''\n\n    if prompt != None and img != None:\n        if len(img) == 1:\n            return self.chat(system, prompt, img[0], temp, top_k, top_p)\n        elif len(img) == 3:\n            res = []\n            system = f'You are analyzing aerial or street view images. For street view, you should just foucus on the building and yard in the middle. {system}'\n            for i in range(len(img)):\n                r = self.chat(system, prompt, img[i], temp, top_k, top_p)\n                res += [r.responses]\n            return res\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.bbox2Buildings","title":"<code>bbox2Buildings(bbox, source='osm', min_area=0, max_area=None, random_sample=None)</code>","text":"<p>This function is used to extract buildings from OpenStreetMap using the bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list</code> <p>The bounding box.</p> required <code>source</code> <code>str</code> <p>The source of the buildings. ['osm', 'being']</p> <code>'osm'</code> <code>min_area</code> <code>int</code> <p>The minimum area.</p> <code>0</code> <code>max_area</code> <code>int</code> <p>The maximum area.</p> <code>None</code> <code>random_sample</code> <code>int</code> <p>The number of random samples.</p> <code>None</code> <p>return (str): The number of buildings found in the bounding box</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def bbox2Buildings(self, bbox, source='osm', min_area=0, max_area=None, random_sample=None):\n    '''\n    This function is used to extract buildings from OpenStreetMap using the bbox.\n\n    Args:\n        bbox (list): The bounding box.\n        source (str): The source of the buildings. ['osm', 'being']\n        min_area (int): The minimum area.\n        max_area (int): The maximum area.\n        random_sample (int): The number of random samples.\n\n    return (str): The number of buildings found in the bounding box\n    '''\n    if source == 'osm':\n        buildings = getOSMbuildings(bbox, min_area, max_area)\n    elif source == 'being':\n        buildings = getGlobalMLBuilding(bbox, min_area, max_area)\n    if buildings is None or buildings.empty:\n        if source == 'osm':\n            return \"No buildings found in the bounding box. Please check https://overpass-turbo.eu/ for areas with buildings.\"\n        if source == 'being':\n            return \"No buildings found in the bounding box. Please check https://github.com/microsoft/GlobalMLBuildingFootprints for areas with buildings.\"\n    if random_sample != None:\n        buildings = buildings.sample(random_sample)\n    self.units = buildings\n    return f\"{len(buildings)} buildings found in the bounding box.\"\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.chat","title":"<code>chat(system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None)</code>","text":"<p>This function is used to chat with the LLM model.'</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The user message.</p> <code>None</code> <code>img</code> <code>str</code> <p>The image path.</p> <code>None</code> <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>None</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>None</code> <p>return (Response): Response object</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def chat(self, system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None):\n    '''\n    This function is used to chat with the LLM model.'\n\n    Args:\n        system (str): The system message.\n        prompt (str): The user message.\n        img (str): The image path.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n\n    return (Response): Response object\n    '''\n    res = ollama.chat(\n        model='llama3.2-vision',\n        format=self.format.model_json_schema(),\n        messages=[\n            {\n                'role': 'system',\n                'content': system\n            },\n            {\n                'role': 'user',\n                'content': prompt,\n                'images': [img]\n            }\n        ],\n        options={\n            \"temperature\":temp,\n            \"top_k\":top_k,\n            \"top_p\":top_p\n        }\n    )\n    return self.format.model_validate_json(res.message.content)\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.loopImgChat","title":"<code>loopImgChat(system=None, prompt=None, temp=0.0, top_k=0.8, top_p=0.8, saveImg=True, progressBar=False)</code>","text":"<p>chat with MLLM model for each image.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>optinal</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The prompt message.</p> <code>None</code> <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> <code>saveImg</code> <code>bool</code> <p>The saveImg for save each image in base64 format in the output.</p> <code>True</code> <code>progressBar</code> <code>bool</code> <p>The progress bar for showing the progress of data analysis over the units</p> <code>False</code> <p>return (list): A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def loopImgChat(self, system=None, prompt=None, \n                temp=0.0, top_k=0.8, top_p=0.8,\n                saveImg:bool=True, progressBar:bool=False):\n    '''\n    chat with MLLM model for each image.\n\n    Args:\n        system (optinal): The system message.\n        prompt (str): The prompt message.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n        saveImg (bool): The saveImg for save each image in base64 format in the output.\n        progressBar (bool): The progress bar for showing the progress of data analysis over the units\n\n    return (list): A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)\n    '''\n\n    from tqdm import tqdm\n\n    res = []\n    for i in tqdm(range(len(self.imgs)), desc=\"Processing...\", ncols=75, disable=progressBar):\n    # for i in range(len(self.imgs)):\n        img = self.imgs[i]\n        r = self.LLM_chat(system=system, prompt=prompt, img=[img], \n                          temp=temp, top_k=top_k, top_p=top_p)\n        r = dict(r.responses[0])\n        if saveImg:\n            im = {'img': img}\n            res += [{**r, **im}]\n        else:\n            res += [r]\n    return res\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.loopUnitChat","title":"<code>loopUnitChat(system=None, prompt=None, temp=0.0, top_k=0.8, top_p=0.8, type='top', epsg=None, multi=False, sv_fov=80, sv_pitch=10, sv_size=(300, 400), saveImg=True, progressBar=False)</code>","text":"<p>chat with MLLM model for each unit in the shapefile. example prompt: prompt = {     'top': ''         Is there any damage on the roof?     '',     'street': ''         Is the wall missing or damaged?         Is the yard maintained well?     '' }</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>optinal</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>dict</code> <p>The prompt message for either top or street view or both.</p> <code>None</code> <code>img</code> <code>str</code> <p>The image path.</p> required <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> <code>type</code> <code>str</code> <p>The type of image to process.</p> <code>'top'</code> <code>epsg</code> <code>int</code> <p>The EPSG code (required when type='street' or type='both').</p> <code>None</code> <code>multi</code> <code>bool</code> <p>The multi flag for multiple street view images for one unit.</p> <code>False</code> <code>sv_fov</code> <code>int</code> <p>The horizontal field of view of the image expressed in degrees(required when type='street' or type='both').</p> <code>80</code> <code>sv_pitch</code> <code>int</code> <p>The up or down angle of the camera relative to the Street View vehicle (required when type='street' or type='both').</p> <code>10</code> <code>sv_size</code> <code>tuple</code> <p>The height and width (height,width) for the street image (required when type='street' or type='both').</p> <code>(300, 400)</code> <code>saveImg</code> <code>bool</code> <p>The saveImg for save each image in base64 format in the output.</p> <code>True</code> <code>progressBar</code> <code>bool</code> <p>The progress bar for showing the progress of data analysis over the units</p> <code>False</code> <p>return (dict): A dictionary includes questions/messages, responses/answers, and image base64 (if required) for each unit</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def loopUnitChat(self, system=None, prompt:dict=None, \n                 temp:float=0.0, top_k:float=0.8, top_p:float=0.8, \n                 type:str='top', epsg:int=None, multi:bool=False, \n                 sv_fov:int=80, sv_pitch:int=10, sv_size:tuple=(300,400),\n                 saveImg:bool=True, progressBar:bool=False):\n    '''\n    chat with MLLM model for each unit in the shapefile.\n    example prompt:\n    prompt = {\n        'top': ''\n            Is there any damage on the roof?\n        '',\n        'street': ''\n            Is the wall missing or damaged?\n            Is the yard maintained well?\n        ''\n    }\n\n    Args:\n        system (optinal): The system message.\n        prompt (dict): The prompt message for either top or street view or both.\n        img (str): The image path.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n        type (str): The type of image to process.\n        epsg (int): The EPSG code (required when type='street' or type='both').\n        multi (bool): The multi flag for multiple street view images for one unit.\n        sv_fov (int): The horizontal field of view of the image expressed in degrees(required when type='street' or type='both').\n        sv_pitch (int): The up or down angle of the camera relative to the Street View vehicle (required when type='street' or type='both').\n        sv_size (tuple): The height and width (height,width) for the street image (required when type='street' or type='both').\n        saveImg (bool): The saveImg for save each image in base64 format in the output.\n        progressBar (bool): The progress bar for showing the progress of data analysis over the units\n\n    return (dict): A dictionary includes questions/messages, responses/answers, and image base64 (if required) for each unit\n    '''\n\n    from tqdm import tqdm\n\n    if type == 'top' and 'top' not in prompt:\n        return \"Please provide prompt for top view images when type='top'\"\n    if type == 'street' and 'street' not in prompt:\n        return \"Please provide prompt for street view images when type='street'\"\n    if type == 'both' and 'top' not in prompt and 'street' not in prompt:\n        return \"Please provide prompt for both top and street view images when type='both'\"\n\n    dic = {\n        \"lon\": [],\n        \"lat\": [],\n    }\n\n    top_view_imgs = {'top_view_base64':[]}\n    street_view_imgs = {'street_view_base64':[]}\n\n    for i in tqdm(range(len(self.units)), desc=\"Processing...\", ncols=75, disable=progressBar):\n    # for i in range(len(self.units)):\n        # Get the extent of one polygon from the filtered GeoDataFrame\n        polygon = self.units.geometry.iloc[i]\n        centroid = polygon.centroid\n\n        dic['lon'].append(centroid.x)\n        dic['lat'].append(centroid.y)\n\n        if type == 'top' or type == 'both':\n            # Convert meters to degrees dynamically based on latitude\n            # Approximate adjustment (5 meters)\n            degree_offset = meters_to_degrees(5, centroid.y)  # Convert 5m to degrees\n            polygon = polygon.buffer(degree_offset)\n            # Compute bounding box\n            minx, miny, maxx, maxy = polygon.bounds\n            bbox = [minx, miny, maxx, maxy]\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as temp_file:\n                image = temp_file.name\n            # Download data using tms_to_geotiff\n            tms_to_geotiff(output=image, bbox=bbox, zoom=22, \n                           source=\"SATELLITE\", \n                           overwrite=True)\n            # Clip the image with the polygon\n            with rasterio.open(image) as src:\n                # Reproject the polygon back to match raster CRS\n                polygon = self.units.to_crs(src.crs).geometry.iloc[i]\n                out_image, out_transform = mask(src, [polygon], crop=True)\n                out_meta = src.meta.copy()\n\n            out_meta.update({\n                \"driver\": \"JPEG\",\n                \"height\": out_image.shape[1],\n                \"width\": out_image.shape[2],\n                \"transform\": out_transform,\n                \"count\": 3 #Ensure RGB (3 bands)\n            })\n\n            # Create a temporary file for the clipped JPEG\n            with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as temp_jpg:\n                clipped_image = temp_jpg.name\n            with rasterio.open(clipped_image, \"w\", **out_meta) as dest:\n                dest.write(out_image)\n            # clean up temp file\n            os.remove(image)\n\n            # convert image into base64\n            clipped_image_base64 = encode_image_to_base64(clipped_image)\n            top_view_imgs['top_view_base64'] += [clipped_image_base64]\n\n            # process aerial image\n            top_res = self.LLM_chat(system=system, \n                                prompt=prompt[\"top\"], \n                                img=[clipped_image], \n                                temp=temp, \n                                top_k=top_k, \n                                top_p=top_p)\n            # initialize the list\n            if i == 0:\n                dic['top_view'] = []\n            if saveImg:\n                dic['top_view'].append(top_res.responses)\n\n            # clean up temp file\n            os.remove(clipped_image)\n\n        # process street view image\n        if (type == 'street' or type == 'both') and epsg != None and self.mapillary_key != None:\n            input_svis = getSV(centroid, epsg, self.mapillary_key, multi=multi, \n                               fov=sv_fov, pitch=sv_pitch, height=sv_size[0], width=sv_size[1])\n            if None not in input_svis:\n                # save imgs\n                if saveImg:\n                    street_view_imgs['street_view_base64'] += [input_svis]\n                # inference\n                res = self.LLM_chat(system=system, \n                                    prompt=prompt[\"street\"], \n                                    img=input_svis, \n                                    temp=temp, \n                                    top_k=top_k, \n                                    top_p=top_p)\n                # initialize the list\n                if i == 0:\n                    dic['street_view'] = []\n                if multi:\n                    dic['street_view'] += [res]\n                else:\n                    dic['street_view'] += [res.responses]\n\n    self.results = {'from_loopUnitChat':dic, 'base64_imgs':{**top_view_imgs, **street_view_imgs}}\n    return dic\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.oneImgChat","title":"<code>oneImgChat(system=None, prompt=None, temp=0.0, top_k=0.8, top_p=0.8, saveImg=True)</code>","text":"<p>chat with MLLM model with one image.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>optinal</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The prompt message.</p> <code>None</code> <code>img</code> <code>str</code> <p>The image path.</p> required <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> <code>saveImg</code> <code>bool</code> <p>The saveImg for save each image in base64 format in the output.</p> <code>True</code> <p>return (dict): A dictionary includes questions/messages, responses/answers, and image base64 (if required)</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def oneImgChat(self, system=None, prompt=None, \n               temp=0.0, top_k=0.8, top_p=0.8, \n               saveImg:bool=True):\n\n    '''\n    chat with MLLM model with one image.\n\n    Args:\n        system (optinal): The system message.\n        prompt (str): The prompt message.\n        img (str): The image path.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n        saveImg (bool): The saveImg for save each image in base64 format in the output.\n\n    return (dict): A dictionary includes questions/messages, responses/answers, and image base64 (if required) \n    '''\n\n    print(\"Inference starts ...\")\n    r = self.LLM_chat(system=system, prompt=prompt, img=[self.img], \n                      temp=temp, top_k=top_k, top_p=top_p)\n    r = dict(r.responses[0])\n    if saveImg:\n        r['img'] = self.img\n    return r\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.plotBase64","title":"<code>plotBase64(img)</code>","text":"<p>plot a single base64 image</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def plotBase64(self, img):\n    '''\n    plot a single base64 image\n    '''\n    plot_base64_image(img)\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.preload_model","title":"<code>preload_model()</code>","text":"<p>Ensures that the required Ollama model is available. If not, it automatically pulls the model.</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def preload_model(self):\n    \"\"\"\n    Ensures that the required Ollama model is available.\n    If not, it automatically pulls the model.\n    \"\"\"\n\n    import ollama\n\n    model_name = \"llama3.2-vision\"\n    try:\n        ollama.pull(model_name)\n\n    except Exception as e:\n        print(f\"Warning: Ollama is not installed or failed to check models: {e}\")\n        print(\"Please install Ollama client: https://github.com/ollama/ollama/tree/main\")\n        raise RuntimeError(\"Ollama not available. Install it before running.\")\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.to_gdf","title":"<code>to_gdf()</code>","text":"<p>Convert output from MLLM into a GeoDataframe, including coordinates, questions, responses, input images (base64)</p> <p>return (GeoDataframe): A GeoDataframe converted from the results</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def to_gdf(self):\n    '''\n    Convert output from MLLM into a GeoDataframe,\n    including coordinates, questions, responses, input images (base64)\n\n    return (GeoDataframe): A GeoDataframe converted from the results \n    '''\n\n    import pandas as pd\n    import copy\n\n    if self.results is not None:\n        if 'from_loopUnitChat' in self.results:\n            res_df = response2gdf(self.results['from_loopUnitChat'])\n            img_dic = copy.deepcopy(self.results['base64_imgs'])\n            if img_dic['top_view_base64'] != [] or img_dic['street_view_base64'] != []:\n                if img_dic['top_view_base64'] == []:\n                    img_dic.pop(\"top_view_base64\")\n                if img_dic['street_view_base64'] == []:\n                    img_dic.pop(\"street_view_base64\")\n                imgs_df = pd.DataFrame(img_dic)\n                return pd.concat([res_df, imgs_df], axis=1)\n            else:\n                return res_df\n        else:\n            return \"This method can only support the output of '.loopUnitChat()' method\"\n    else:\n        return \"This method can only be called after running the '.loopUnitChat()' method\"\n</code></pre>"},{"location":"API_Reference/format_creation/","title":"Format creation","text":"<p>format a Response model with a customized QnA model.</p> <p>Parameters: - fields (dict): fields to add dynamically.</p> <p>Returns: - Pydantic model: Response class.</p> Source code in <code>urbanworm/format_creation.py</code> <pre><code>def create_format(fields: dict):\n    \"\"\"\n    format a Response model with a customized QnA model.\n\n    Parameters:\n    - fields (dict): fields to add dynamically.\n\n    Returns:\n    - Pydantic model: Response class.\n    \"\"\"\n\n    # Dynamically create the model\n    CustomQnA = schema(fields)\n\n    return Response[CustomQnA]\n</code></pre>"},{"location":"API_Reference/pano2pers/","title":"Pano2pers","text":"Source code in <code>urbanworm/pano2pers.py</code> <pre><code>class Equirectangular:\n    def __init__(self, img_path=None, img_url=None):\n        if img_path != None:\n            self._img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        elif img_url != None:\n            self._img = self.read_url2img(img_url)\n        [self._height, self._width, _] = self._img.shape\n\n    def read_url2img(self, url):\n        resp = urlopen(url, timeout=100)\n        image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n        return image\n\n    def GetPerspective(self, FOV, THETA, PHI, height, width, RADIUS = 128):\n        # THETA is left/right angle, PHI is up/down angle, both in degree\n        equ_h = self._height\n        equ_w = self._width\n        equ_cx = (equ_w - 1) / 2.0\n        equ_cy = (equ_h - 1) / 2.0\n\n        wFOV = FOV\n        hFOV = float(height) / width * wFOV\n\n        c_x = (width - 1) / 2.0\n        c_y = (height - 1) / 2.0\n\n        wangle = (180 - wFOV) / 2.0\n        w_len = 2 * RADIUS * np.sin(np.radians(wFOV / 2.0)) / np.sin(np.radians(wangle))\n        w_interval = w_len / (width - 1)\n\n        hangle = (180 - hFOV) / 2.0\n        h_len = 2 * RADIUS * np.sin(np.radians(hFOV / 2.0)) / np.sin(np.radians(hangle))\n        h_interval = h_len / (height - 1)\n        x_map = np.zeros([height, width], np.float32) + RADIUS\n        y_map = np.tile((np.arange(0, width) - c_x) * w_interval, [height, 1])\n        z_map = -np.tile((np.arange(0, height) - c_y) * h_interval, [width, 1]).T\n        D = np.sqrt(x_map**2 + y_map**2 + z_map**2)\n        # xyz = np.zeros([height, width, 3], np.float)\n        xyz = np.zeros([height, width, 3], np.float32)\n        xyz[:, :, 0] = (RADIUS / D * x_map)[:, :]\n        xyz[:, :, 1] = (RADIUS / D * y_map)[:, :]\n        xyz[:, :, 2] = (RADIUS / D * z_map)[:, :]\n\n        y_axis = np.array([0.0, 1.0, 0.0], np.float32)\n        z_axis = np.array([0.0, 0.0, 1.0], np.float32)\n        [R1, _] = cv2.Rodrigues(z_axis * np.radians(THETA))\n        [R2, _] = cv2.Rodrigues(np.dot(R1, y_axis) * np.radians(-PHI))\n\n        xyz = xyz.reshape([height * width, 3]).T\n        xyz = np.dot(R1, xyz)\n        xyz = np.dot(R2, xyz).T\n        lat = np.arcsin(xyz[:, 2] / RADIUS)\n        # lon = np.zeros([height * width], np.float)\n        lon = np.zeros([height * width], np.float32)\n        theta = np.arctan(xyz[:, 1] / xyz[:, 0])\n        idx1 = xyz[:, 0] &gt; 0\n        idx2 = xyz[:, 1] &gt; 0\n\n        idx3 = ((1 - idx1) * idx2).astype(np.bool_)\n        idx4 = ((1 - idx1) * (1 - idx2)).astype(np.bool_)\n\n        lon[idx1] = theta[idx1]\n        lon[idx3] = theta[idx3] + np.pi\n        lon[idx4] = theta[idx4] - np.pi\n\n        lon = lon.reshape([height, width]) / np.pi * 180\n        lat = -lat.reshape([height, width]) / np.pi * 180\n        lon = lon / 180 * equ_cx + equ_cx\n        lat = lat / 90 * equ_cy + equ_cy\n\n        persp = cv2.remap(self._img, lon.astype(np.float32), lat.astype(np.float32), cv2.INTER_CUBIC, borderMode=cv2.BORDER_WRAP)\n        # Convert for Ollama\n        _, buffer = cv2.imencode('.png', persp)\n        img_base64 = base64.b64encode(buffer).decode('utf-8')\n        return img_base64\n</code></pre>"},{"location":"API_Reference/utils/","title":"Utils","text":"<p>Get closest SVI(s) with given coordinates using APIs</p> Source code in <code>urbanworm/utils.py</code> <pre><code>def getSV(centroid, epsg:int, key:str, multi:bool=False, \n          fov:int=80, heading:int=None, pitch:int=10, \n          height:int=300, width:int=400):\n    \"\"\"Get closest SVI(s) with given coordinates using APIs\"\"\"\n    bbox = projection(centroid, epsg)\n    url = f\"https://graph.mapillary.com/images?access_token={key}&amp;fields=id,compass_angle,thumb_2048_url,geometry&amp;bbox={bbox}&amp;is_pano=true\"\n    # while not response or 'data' not in response:\n    try:\n        response = requests.get(url).json()\n        # find the closest image\n        response = closest(centroid, response, multi)\n\n        svis = []\n        for i in range(len(response)):\n            # Extract Image ID, Compass Angle, image url, and coordinates\n            img_heading = float(response.iloc[i,1])\n            img_url = response.iloc[i,2]\n            image_lon, image_lat = response.iloc[i,5]\n            if heading == None:\n                # calculate bearing to the house\n                bearing_to_house = calculate_bearing(image_lat, image_lon, centroid.y, centroid.x)\n                relative_heading = (bearing_to_house - img_heading) % 360\n            else:\n                relative_heading = heading\n            # reframe image\n            svi = Equirectangular(img_url=img_url)\n            sv = svi.GetPerspective(fov, relative_heading, pitch, height, width, 128)\n            svis.append(sv)\n        return svis\n    except:\n        print(\"no street view image found\")\n        return None\n</code></pre> <p>Get building footprints with a bounding box from OSM.</p> Source code in <code>urbanworm/utils.py</code> <pre><code>def getOSMbuildings(bbox, min_area=0, max_area=None):\n    \"\"\"Get building footprints with a bounding box from OSM.\"\"\"\n    # Extract bounding box coordinates\n    min_lon, min_lat, max_lon, max_lat = bbox\n\n    url = \"https://overpass-api.de/api/interpreter\"\n    query = f\"\"\"\n    [bbox:{max_lat},{max_lon},{min_lat},{min_lon}]\n    [out:json]\n    [timeout:900];\n    (\n        way[\"building\"]({min_lat},{min_lon},{max_lat},{max_lon});\n        relation[\"building\"]({min_lat},{min_lon},{max_lat},{max_lon});\n    );\n    out geom;\n    \"\"\"\n\n    payload = \"data=\" + requests.utils.quote(query)\n    response = requests.post(url, data=payload)\n    data = response.json()\n\n    buildings = []\n    for element in data.get(\"elements\", []):\n        if \"geometry\" in element:\n            coords = [(node[\"lon\"], node[\"lat\"]) for node in element[\"geometry\"]]\n            if len(coords) &gt; 2:  \n                polygon = Polygon(coords)\n                # Approx. conversion to square meters\n                area_m2 = polygon.area * (111320 ** 2)  \n                # Filter buildings by area\n                if area_m2 &gt;= min_area and (max_area is None or area_m2 &lt;= max_area):\n                    buildings.append(polygon)\n\n    if len(buildings) == 0:\n        return None\n    # Convert to GeoDataFrame\n    gdf = gpd.GeoDataFrame(geometry=buildings, crs=\"EPSG:4326\")\n    return gdf\n</code></pre> <p>Fetches Global ML Building footprints within a bounding box.</p> <p>Args: - bbox (tuple/list): (min_lon, min_lat, max_lon, max_lat) - min_area (float): Minimum building area in square meters (default=0) - max_area (float): Maximum building area in square meters (default=None, no limit)</p> <p>Returns: - gpd.GeoDataFrame: Filtered building footprints.</p> Source code in <code>urbanworm/utils.py</code> <pre><code>def getGlobalMLBuilding(bbox, min_area:float=0.0, max_area:float=None):\n    \"\"\"\n    Fetches Global ML Building footprints within a bounding box.\n\n    Args:\n    - bbox (tuple/list): (min_lon, min_lat, max_lon, max_lat)\n    - min_area (float): Minimum building area in square meters (default=0)\n    - max_area (float): Maximum building area in square meters (default=None, no limit)\n\n    Returns:\n    - gpd.GeoDataFrame: Filtered building footprints.\n    \"\"\"\n    import mercantile\n    from tqdm import tqdm\n    import tempfile\n    from shapely import geometry\n\n    min_lon, min_lat, max_lon, max_lat = bbox\n    aoi_geom = {\n        \"coordinates\": [\n            [\n                [min_lon, min_lat],\n                [min_lon, max_lat],\n                [max_lon, max_lat],\n                [max_lon, min_lat],\n                [min_lon, min_lat]\n            ]\n        ],\n        \"type\": \"Polygon\"\n    }\n    aoi_shape = geometry.shape(aoi_geom)\n    # Extract bounding box coordinates\n    minx, miny, maxx, maxy = aoi_shape.bounds\n    # get tiles intersect bbox\n    quad_keys = set()\n    for tile in list(mercantile.tiles(minx, miny, maxx, maxy, zooms=9)):\n        quad_keys.add(mercantile.quadkey(tile))\n    quad_keys = list(quad_keys)\n    # Download the building footprints for each tile and crop with bbox\n    df = pd.read_csv(\n        \"https://minedbuildings.z5.web.core.windows.net/global-buildings/dataset-links.csv\", dtype=str\n    )\n\n    idx = 0\n    combined_gdf = gpd.GeoDataFrame()\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Download the GeoJSON files for each tile that intersects the input geometry\n        tmp_fns = []\n        for quad_key in tqdm(quad_keys):\n            rows = df[df[\"QuadKey\"] == quad_key]\n            if rows.shape[0] == 1:\n                url = rows.iloc[0][\"Url\"]\n\n                df2 = pd.read_json(url, lines=True)\n                df2[\"geometry\"] = df2[\"geometry\"].apply(geometry.shape)\n\n                gdf = gpd.GeoDataFrame(df2, crs=4326)\n                fn = os.path.join(tmpdir, f\"{quad_key}.geojson\")\n                tmp_fns.append(fn)\n                if not os.path.exists(fn): # Skip if file already exists\n                    gdf.to_file(fn, driver=\"GeoJSON\")\n            elif rows.shape[0] &gt; 1:\n                raise ValueError(f\"Multiple rows found for QuadKey: {quad_key}\")\n            else:\n                raise ValueError(f\"QuadKey not found in dataset: {quad_key}\")\n        # Merge the GeoJSON files into a single file\n        for fn in tmp_fns:\n            gdf = gpd.read_file(fn)  # Read each file into a GeoDataFrame\n            gdf = gdf[gdf.geometry.within(aoi_shape)]  # Filter geometries within the AOI\n            gdf['id'] = range(idx, idx + len(gdf))  # Update 'id' based on idx\n            idx += len(gdf)\n            combined_gdf = pd.concat([combined_gdf,gdf],ignore_index=True)\n\n    # Reproject to a UTM CRS for accurate area measurement\n    utm_crs = gdf.estimate_utm_crs()  \n    # Compute area and filter buildings by area\n    gdf = gdf.to_crs(utm_crs)\n    gdf[\"area_m2\"] = gdf.geometry.area\n    gdf = gdf[gdf[\"area_m2\"] &gt;= min_area]  # Filter min area\n    if max_area:\n        gdf = gdf[gdf[\"area_m2\"] &lt;= max_area]  # Filter max area\n    # Reproject back to WGS84\n    combined_gdf.to_crs('EPSG:4326')\n    return combined_gdf\n</code></pre>"}]}