{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Urban-Worm","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Urban-WORM (Workflow Of Reproducible Multimodal Inference) is a user-friendly high-level interface that  is designed for adding rich and meaningful captions for crowdsourced data with geotags using multimodal models (MMs).  Urban-WORM can support the batched analysis of images and sounds for investigating urban environments at scales.  The investigation may cover topics about building conditions, street appearance, people's activities, etc.</p> <p> </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Collect geotagged data (Mapillary street views, Flickr photos, and Freesound audios) via APIs  within the proximity of building footprints (or other POIs)</li> <li>Calibrate the orientation of the panorama street views to look at given locations</li> <li>Filter out personal photo using face detection</li> <li>Divide sound recording to multiple clips with given duration</li> <li>Support (batched) multiple data input with multimodal models</li> </ul>"},{"location":"#computing-resources","title":"Computing Resources","text":"<p>MMs typically require substantial RAM and can be quite slow when running solely on a CPU. While macOS devices with Apple  Silicon can accelerate computation using Metal, performance is still not on par with high-end NVIDIA GPUs, which are  strongly recommended for optimal speed and efficiency. This package is built on Ollama, which provides fast performance\u2014especially  for local use with quantized models. However, the number of parameters in a model directly affects VRAM requirements.  More powerful models need more VRAM to run entirely on the GPU. For instance, the Llama3.2-Vision model with 11 billion  parameters requires approximately 10 GB of GPU memory. Additionally, image resolution plays a crucial role in inference  time\u2014particularly for high-resolution inputs like satellite imagery or street views. The higher the resolution, the longer  a VLM may take to process the image.</p>"},{"location":"#legal-notice","title":"Legal Notice","text":"<p>This repository and its content are provided for educational purposes only. By using the information and code provided,  users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws  and regulations. </p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The package is heavily built on llamacpp and Ollama. Credit goes to the developers of these projects.</p> <ul> <li>llama.cpp</li> <li>ollama</li> <li>ollama-python</li> </ul> <p>The functionality about sourcing and processing GIS data and image processing is built on the following open projects.  Credit goes to the developers of these projects. - GlobalMLBuildingFootprints - Equirec2Perspec - Mapillary API - Flickr API - Freesound API</p> <p>The development of this package is supported and inspired by the city of Detroit.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#1-install-the-package","title":"1 install the package","text":"<p>The package <code>urban-worm</code> can be installed with <code>pip</code>:</p> <pre><code>pip install urban-worm\n</code></pre>"},{"location":"installation/#2-inference-with-llamacpp","title":"2 Inference with llama.cpp","text":"<p>To run more pre-quantized models with vision capabilities, please install pre-built version of llama.cpp:</p> <pre><code># Windows\nwinget install llama.cpp\n\n# Mac and Linux\nbrew install llama.cpp\n</code></pre> <p>More information about the installation  here</p> <p>More GGUF models can be found at the Hugging Face pages  here and here</p>"},{"location":"installation/#3-inference-with-ollama-client","title":"3 Inference with Ollama client","text":"<p>Please make sure Ollama is installed before using urban-worm if you plan to rely on Ollama</p> <p>For Linux, users can also install ollama by running in the terminal:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>For MacOS, users can also install ollama using <code>brew</code>:</p> <pre><code>brew install ollama\n</code></pre> <p>To install <code>brew</code>, run in the terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Windows users should directly install the Ollama client</p> <p>To install the development version from this repo:</p> <pre><code>pip install -e git+https://github.com/billbillbilly/urbanworm.git#egg=urban-worm\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use urban-worm in a project:</p> <pre><code>from urbanworm.inference.llama import InferenceOllama\n\ndata = InferenceOllama(image = 'docs/data/img_1.jpg')\nsystem = '''\n    Your answer should be based only on your observation. \n    The format of your response must include answer (yes/True or no/False), explanation (within 50 words)\n'''\nprompt = '''\n    Is there a tree?\n'''\n\ndata.schema = {\n    \"answer\": (bool, ...),\n    \"explanation\": (str, ...)\n}\n\ndata.one_inference(system=system, prompt=prompt)\n</code></pre>"},{"location":"API_Reference/dataset/","title":"Dataset","text":"Source code in <code>urbanworm/dataset.py</code> <pre><code>class GeoTaggedData:\n    def __init__(self,\n                 locations: list|tuple|dict|pd.DataFrame=None,\n                 units: GeoDataFrame=None):\n        '''\n        Args:\n            locations (list|tuple|dict|Dataframe): A list of coordinates (longitude/x and latitude/y) or a dictionary keyed by longitude and latitude or a dataframe with columns \"longitude\" and \"latitude\".\n            units (GeoDataFrame): The path to the shapefile or geojson file, or GeoDataFrame.\n\n        Examples:\n            # retrieve street view with building footprints (OSM)\n            gtd = GeoTaggedData()\n            gtd.getBuildingFootprints(bbox=(-83.235572,42.348092,-83.235154,42.348806))\n            gtd.get_svi_from_locations(key=\"your Mapillary token\")\n\n            # locations - a nested list of coordinates\n            gtd = GeoTaggedData(location=[[-83.235572,42.348092],[-83.235154,42.348806]])\n            # locations - a dataframe with columns \"longitude\" and \"latitude\"\n            df = pd.Dataframe({\"longitude\":[-83.235572, -83.235154], \"latitude\":[42.348092, 42.348806]})\n            gtd = GeoTaggedData(locations=df)\n        '''\n\n        self.images = None\n        self.locations = locations\n        self.units = units\n        if locations is not None and units is None:\n            self.construct_units()\n\n        self.svis = self.photos = self.audios = {\n            'loc_id': [],\n            'id': [],\n            'data': [],\n            'path':[],\n        }\n\n        self.svi_metadata = None\n        self.photo_metadata = None\n        self.audio_metadata = None\n        self.plot = None\n\n    def construct_units(self):\n        if isinstance(self.locations, list):\n            if isinstance(self.locations[0], list):\n                coor = {\n                    'x': [],\n                    'y': []\n                }\n                for location in self.locations:\n                    coor['x'].append(location[0])\n                    coor['y'].append(location[1])\n                df = pd.DataFrame(coor)\n                geometry = gpd.points_from_xy(df['x'], df['y'])\n                id_df = pd.DataFrame({'loc_id':[i for i in range(len(df))]})\n                self.units = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n            else:\n                print(\"coordinates should be stored in a nested list\")\n                return None\n        elif isinstance(self.locations, dict):\n            if 'longitude' in self.locations and 'latitude' in self.locations:\n                geometry = gpd.points_from_xy(self.locations['longitude'], self.locations['latitude'])\n                id_df = pd.DataFrame({'loc_id': [i for i in range(len(self.locations['longitude']))]})\n            else:\n                print(\"the dictionary of coordinates should be keyed by longitude and latitude\")\n                return None\n        elif isinstance(self.locations, pd.DataFrame):\n            if 'longitude' in self.locations.columns and 'latitude' in self.locations.columns:\n                geometry = gpd.points_from_xy(self.locations['longitude'], self.locations['latitude'])\n                id_df = pd.DataFrame({'loc_id': [i for i in range(len(self.locations['longitude']))]})\n            else:\n                print(\"the dataframe of coordinates should include columns of longitude and latitude\")\n                return None\n        else:\n            return None\n        self.units = gpd.GeoDataFrame(id_df, geometry=geometry, crs=\"EPSG:4326\")\n        return None\n\n    def getBuildings(self,\n                     bbox: list | tuple = None,\n                     source: str = 'osm',\n                     min_area: float | int = 0,\n                     max_area: float | int = None,\n                     random_sample: int = None)-&gt; None:\n        '''\n            Extract buildings from OpenStreetMap using the bbox.\n\n            Args:\n                bbox (list or tuple): The bounding box.\n                source (str): The source of the buildings. ['osm', 'microsoft']\n                min_area (float or int): The minimum area.\n                max_area (float or int): The maximum area.\n                random_sample (int): The number of random samples.\n        '''\n\n        if source not in ['osm', 'microsoft']:\n            raise Exception(f'{source} is not supported')\n\n        if source == 'osm':\n            buildings = getOSMbuildings(bbox, min_area, max_area)\n        elif source == 'microsoft':\n            buildings = getGlobalMLBuilding(bbox, min_area, max_area)\n        if buildings is None or buildings.empty:\n            if source == 'osm':\n                print(\"No buildings found in the bounding box. Please check https://overpass-turbo.eu/ for areas with buildings.\")\n                return None\n            if source == 'microsoft':\n                print(\"No buildings found in the bounding box. Please check https://github.com/microsoft/GlobalMLBuildingFootprints for areas with buildings.\")\n                return None\n        if random_sample is not None:\n            buildings = buildings.sample(random_sample)\n        self.units = buildings.to_crs(4326)\n        print(f\"{len(buildings)} buildings found in the bounding box.\")\n        return None\n\n    def get_svi_from_locations(self,\n                               id_column:str=None,\n                               distance:int = 50,\n                               key: str = None,\n                               pano: bool = True, reoriented: bool = True,\n                               multi_num: int = 1, interval: int = 1,\n                               fov: int = 80, heading: int = None, pitch: int = 5,\n                               height: int = 500, width: int = 700,\n                               year: list | tuple = None, season: str = None, time_of_day: str = 'day',\n                               silent: bool = True):\n        \"\"\"\n            get_svi_from_locations\n\n            Retrieve the closest street view image(s) near each coordinate using the Mapillary API.\n            The street view image will be reoriented to look at the coordinate when `reoriented = True`.\n\n            Args:\n                id_column (str, optional): The name of column that has unique identifier (or something similar) for each location.\n                distance (int): The max distance in meters between the centroid and the street view\n                key (str): Mapillary API access token.\n                pano (bool): Whether to search for pano street view images only. (Default is True)\n                reoriented (bool): Whether to reorient and crop street view images. (Default is True)\n                multi_num (int): The number of multiple SVIs (Default is 1).\n                interval (int): The interval in meters between each SVI (Default is 1).\n                fov (int): Field of view in degrees for the perspective image. (Defaults is 80).\n                heading (int): Camera heading in degrees. If None, it will be computed based on the house orientation.\n                pitch (int): Camera pitch angle. (Default is 10).\n                height (int): Height in pixels of the returned image. (Default is 480).\n                width (int): Width in pixels of the returned image. (Default is 640).\n                year (list[str], optional): Year of data (start year, end year).\n                season (str, optional): Season of data. One of [\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"]\n                time_of_day (str, optional): Time of data. One of [\"day\",\"night\"] (Default is 'day')\n                silent (bool): If True, do not show error traceback (Default is True).\n            \"\"\"\n\n        self.svis = {\n            'loc_id': [],\n            'id': [],\n            'data': [],\n            'path': [],\n        }\n        self.svi_metadata = None\n\n        if id_column is None:\n            id_column = 'loc_id'\n            if id_column not in self.units.columns:\n                self.units[id_column] = [i for i in range(len(self.units))]\n        res_df = None\n        skip_count = 0\n        for index, row in tqdm(self.units.iterrows(), total=len(self.units)):\n            loc_id = row[id_column]\n            try:\n                svis, output_df = getSV([row.geometry.centroid.x, row.geometry.centroid.y],\n                                        loc_id,\n                                        distance,\n                                        key,\n                                        pano,\n                                        reoriented,\n                                        multi_num,\n                                        interval,\n                                        fov, heading, pitch,\n                                        height,\n                                        width,\n                                        year,\n                                        season,\n                                        time_of_day,\n                                        silent = silent\n                                        )\n                if svis is None:\n                    skip_count += 1\n                    continue\n\n                self.svis['data'] += svis\n                self.svis['loc_id'] += output_df['loc_id'].tolist()\n                self.svis['id'] += output_df['id'].tolist()\n\n                if res_df is None:\n                    res_df = output_df\n                else:\n                    res_df = pd.concat([res_df, output_df])\n            except Exception as e:\n                if not silent: print(f'skipping {[row.geometry.centroid.x, row.geometry.centroid.y]}: {e}')\n                skip_count += 1\n                continue\n        self.svi_metadata = res_df\n        if skip_count &gt; 0:\n            print(f'Collect data for {len(self.units) - skip_count} locations and skipped {skip_count} locations due to no data found.')\n        return None\n\n    def get_photo_from_location(self,\n                                id_column:str=None,\n                                distance: int = 50,\n                                key: str = None,\n                                query: str | list[str] = None,\n                                tag: str | list[str] = None,\n                                max_return: int = 1,\n                                year: list | tuple = None,\n                                season: str = None,\n                                time_of_day: str = None,\n                                exclude_personal_photo: bool = True,\n                                exclude_from_location:int = None,\n                                silent = True,\n                                ):\n        '''\n            get_photo_from_location\n\n            Retrieve geotagged photos from Flickr\n\n            Args:\n                id_column: (str, optional): The name of column that has unique identifier (or something similar) for each location.\n                distance (int): Search radius in meters (converted to km; Flickr radius max is 32 km).\n                key (str): Flickr API key. If None, reads env var FLICKR_API_KEY.\n                query (str, optional): Query string to search for.\n                tag (str | list[str]): Tag string or list of tags (comma-separated). Acts as a \"limiting agent\" for geo queries.\n                max_return (int): Number of photos to return (after filters).\n                year: [Y] or (Y,) or (Y1, Y2) inclusive. Filters by taken date range.\n                season (str): One of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by taken month).\n                time_of_day (str): One of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by taken hour).\n                exclude_personal_photo (bool): If True, exclude personal photo from locations. (Default is True)\n                exclude_from_location (int, optional): Drop retrieved data with a distance from the given location.\n                silent (bool): If True, do not show error traceback (Default is True).\n        '''\n\n        from .utils.pano2pers import read_url2img\n        from importlib.resources import files, as_file\n\n        self.photos = {\n            'loc_id': [],\n            'id': [],\n            'data': [],\n            'path': [],\n        }\n        self.photo_metadata = None\n\n        if id_column is None:\n            id_column = 'loc_id'\n            if id_column not in self.units.columns:\n                self.units[id_column] = [i for i in range(len(self.units))]\n        res_df = None\n        skip_count = 0\n        for index, row in tqdm(self.units.iterrows(), total=len(self.units)):\n            loc_id = row['loc_id']\n            try:\n                output_df = getPhoto([row.geometry.centroid.x, row.geometry.centroid.y],\n                                     loc_id,\n                                     distance,\n                                     key,\n                                     query,\n                                     tag,\n                                     max_return,\n                                     year,\n                                     season,\n                                     time_of_day,\n                                     exclude_from_location,\n                                     output_df=True)\n                if exclude_personal_photo:\n                    model_res = files(\"urbanworm.models\") / \"face_detection_yunet_2023mar.onnx\"\n                    drop_list = []\n                    for ind, r in output_df.iterrows():\n                        with as_file(model_res) as model_path:\n                            is_selfie = is_selfie_photo(model_path, r['url'])\n                            if is_selfie:\n                                drop_list += [ind]\n                    if len(drop_list) &gt; 0:\n                        output_df.drop(drop_list, axis=0, inplace=True)\n                        if len(output_df) == 0:\n                            continue\n\n                self.photos['loc_id'] += output_df['loc_id'].tolist()\n                self.photos['data'] += output_df['url'].tolist()\n                self.photos['id'] += output_df['id'].tolist()\n                if res_df is None:\n                    res_df = output_df\n                else:\n                    res_df = pd.concat([res_df, output_df])\n            except Exception as e:\n                if not silent: print(e)\n                skip_count += 1\n                continue\n        self.photo_metadata = res_df\n        if skip_count &gt; 0:\n            print(f'Collect data for {len(self.units) - skip_count} locations and skipped {skip_count} locations due to no data found.')\n        return None\n\n    def get_sound_from_location(self,\n                                id_column: str = None,\n                                distance: int = 50,\n                                key: str = None,\n                                query: str | list[str] = None,\n                                tag: str | list[str] = None,\n                                max_return: int = 1,\n                                year: list | tuple = None,\n                                season: str = None,\n                                time_of_day: str = None,\n                                duration: int = None,\n                                exclude_from_location: int = None,\n                                slice_duration: int = None,\n                                slice_max_num: int = None,\n                                silent: bool = True\n                                ):\n\n        '''\n            get_sound_from_location\n\n            Retrieve geotagged sound recordings from Freesound\n\n            Args:\n                id_column (str, optional): The name of column that has unique identifier (or something similar) for each location.\n                distance (int): radius in meters (converted to km for Freesound geofilt).\n                key (str): Freesound API key. If None, reads env var FREESOUND_API_KEY.\n                query (str, optional): Query string to search for.\n                tag (str | list[str]): tag string or list of tags (used as filters).\n                max_return (int): number of sounds to return (after post-filters).\n                year (int | list): [Y] or (Y,) or (Y1, Y2) inclusive (filters by upload date \"created\").\n                season (str): one of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by created month).\n                time_of_day (str): one of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by created hour).\n                duration (int | list[int] | tuple[int]): maximum duration in seconds (&lt;= duration). If you want a range, pass a tuple/list (min,max).\n                exclude_from_location (int, optional): Drop retrieved data with a distance from the given location.\n                slice_duration (int, optional): Split the original sound signal into clips with the given duration.\n                slice_max_num (int, optional): Maximum number of clips sliced from the original sound signal.\n                silent (bool): If True, do not show error traceback (Default is True).\n        '''\n\n        self.audios = {\n            'loc_id': [],\n            'id': [],\n            'data': [],\n            'path': [],\n        }\n        self.audio_metadata = None\n\n        if slice_duration is not None:\n            self.audios['slice'] = []\n\n        if id_column is None:\n            id_column = 'loc_id'\n            if id_column not in self.units.columns:\n                self.units[id_column] = [i for i in range(len(self.units))]\n        res_df = None\n        skip_count = 0\n        for index, row in tqdm(self.units.iterrows(), total=len(self.units)):\n            loc_id = row['loc_id']\n            try:\n                output_df = getSound([row.geometry.centroid.x, row.geometry.centroid.y],\n                                     loc_id,\n                                     distance,\n                                     key,\n                                     query,\n                                     tag,\n                                     max_return,\n                                     year,\n                                     season,\n                                     time_of_day,\n                                     duration,\n                                     exclude_from_location,\n                                     slice_duration,\n                                     slice_max_num,\n                                     output_df = True)\n\n                if slice_duration is not None:\n                    slice_list = output_df['slice'].tolist()\n                    loc_id_list = output_df['loc_id'].tolist()\n                    data_list = output_df['preview-hq-mp3'].tolist()\n                    id_list = output_df['id'].tolist()\n\n                    slice_num = 1\n                    if isinstance(slice_list[0][0], list):\n                        slice_num = len(slice_list[0])\n                        flattened_slice_list = [item for sublist in slice_list for item in sublist]\n                    if slice_num &gt; 1:\n                        loc_id_list_ = []\n                        data_list_ = []\n                        id_list_ = []\n                        for item in loc_id_list:\n                            loc_id_list_.extend([item] * slice_num)\n                        for item in data_list:\n                            data_list_.extend([item] * slice_num)\n                        for item in id_list:\n                            id_list_.extend([item] * slice_num)\n                        self.audios['loc_id'] += loc_id_list_\n                        self.audios['data'] += data_list_\n                        self.audios['id'] += id_list_\n                        self.audios['slice'] += flattened_slice_list\n                    else:\n                        self.audios['loc_id'] += loc_id_list\n                        self.audios['data'] += data_list\n                        self.audios['id'] += id_list\n                        self.audios['slice'] += flattened_slice_list\n                else:\n                    self.audios['loc_id'] += output_df['loc_id'].tolist()\n                    self.audios['data'] += output_df['preview-hq-mp3'].tolist()\n                    self.audios['id'] += output_df['id'].tolist()\n\n                if res_df is None:\n                    res_df = output_df\n                else:\n                    res_df = pd.concat([res_df, output_df])\n            except Exception as e:\n                if not silent: print(e)\n                skip_count += 1\n                continue\n        self.audio_metadata = res_df\n        if skip_count &gt; 0:\n            print(f'Collect data for {len(self.units) - skip_count} locations and skipped {skip_count} locations due to no data found.')\n        return None\n\n    def download_to_dir(self, data:str = None, to_dir:str = None, prefix: str = None)-&gt; None:\n        '''\n            download_to_dir\n\n            Download retrieved data to a directory.\n\n            Args:\n                data (str): Type of data to download: ['svi', 'audio', 'photo'].\n                to_dir (str): the directory to save the downloaded data.\n                prefix (str, optional):  The prefix to add to the output filename.\n        '''\n        if data not in ['svi', 'audio', 'photo']:\n            raise ValueError('Invalid data type provided. It has to be one of [\"svi\", \"audio\", \"photo\"].')\n        if to_dir is not None:\n            if not os.path.exists(to_dir):\n                print(\"The directory doesn't exist.\")\n                print(\"The directory is created now.\")\n                out_dir = Path(to_dir)\n                out_dir.mkdir(parents=True, exist_ok=True)\n        else:\n            print(\"You need to specify a directory to download.\")\n            return None\n        if data == 'svi':\n            if len(self.svis['id']) == 0:\n                return None\n            self.svis['path'] = []\n            for i in tqdm(range(len(self.svis['data'])), total=len(self.svis['data'])):\n                loc_id = self.svis['loc_id'][i]\n                img_id = self.svis['id'][i]\n                path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n                p = path + f'_{img_id}.png'\n                try:\n                    if is_base64(self.svis['data'][i]):\n                        save_base64(self.svis['data'][i], p)\n                    else:\n                        download_image_requests(self.svis['data'][i], p)\n                except:\n                    self.svis['path'] += [\" \"]\n                    continue\n                self.svis['path'] += [p]\n        elif data == 'audio':\n            if len(self.audios['id']) == 0:\n                return None\n            self.audios['path'] = []\n            if 'slice' in self.audios:\n                for i in tqdm(range(len(self.audios['data'])), total=len(self.audios['data'])):\n                    loc_id = self.audios['loc_id'][i]\n                    audio_id = self.audios['id'][i]\n                    slices = self.audios['slice'][i]\n                    path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n                    start = slices[0]\n                    end = slices[1]\n                    p = path + f'_{audio_id}_clip_{start}_{end}.mp3'\n                    try:\n                        clip(self.audios['data'][i], start, end, p)\n                    except:\n                        continue\n                    self.audios['path'] += [p]\n            else:\n                for i in tqdm(range(len(self.audios['data'])), total=len(self.audios['data'])):\n                    loc_id = self.audios['loc_id'][i]\n                    audio_id = self.audios['id'][i]\n                    path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n                    p = path + f'_{audio_id}.mp3'\n                    try:\n                        download_freesound_preview(self.audios['data'][i], p)\n                    except:\n                        self.audios['path'] += [\" \"]\n                        continue\n                    self.audios['path'] += [p]\n        elif data == 'photo':\n            if len(self.photos['id']) == 0:\n                return None\n            self.photos['path'] = []\n            for i in tqdm(range(len(self.photos['data'])), total=len(self.photos['data'])):\n                loc_id = self.photos['loc_id'][i]\n                photo_id = self.photos['id'][i]\n                path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n                p = path + f'_{photo_id}.png'\n                try:\n                    download_image_requests(self.photos['data'][i], p)\n                except:\n                    self.photos['path'] += [\" \"]\n                self.photos['path'] += [p]\n        return None\n\n    def set_images(self, img_type: str):\n        '''\n            set_images\n\n            Set retrieved street view images or Flickr photos as images dataset\n\n            Args:\n                img_type (str): 'photo' or 'svi'\n        '''\n        if img_type == 'svi':\n            self.images = self.svis\n        elif img_type == 'photo':\n            self.images = self.photos\n        return None\n\n    def plot_data(self, data:str = None, export_gdf: bool = False) -&gt; None:\n        '''\n\n        Args:\n            data (str): Type of data to download: ['svi', 'audio', 'photo'].\n            export_gdf (bool): Export gpd.GeoDataFrame.\n        '''\n        if data is not None:\n            return None\n\n        if data == 'svi':\n            temp = self.svi_metadata\n            geometry = gpd.points_from_xy(temp['image_lon'], temp['image_lat'])\n            temp['detail'] = temp.apply(\n                lambda row: f'&lt;a href=\"{row[\"url\"]}\"&gt;View image details&lt;/a&gt;',\n                axis=1\n            )\n            gdf = gpd.GeoDataFrame(temp, geometry=geometry, crs=\"EPSG:4326\")\n            popup = [\"id\", \"captured_at\", \"detail\"]\n        elif data == 'photo':\n            temp = self.photo_metadata\n            geometry = gpd.points_from_xy(temp['longitude'], temp['latitude'])\n            temp['detail'] = temp.apply(\n                lambda row: f'&lt;a href=\"{row[\"url\"]}\"&gt;View photo details&lt;/a&gt;',\n                axis=1\n            )\n            gdf = gpd.GeoDataFrame(temp, geometry=geometry, crs=\"EPSG:4326\")\n            popup = [\"id\", \"datetaken\", \"detail\"]\n        elif data == 'audio':\n            geometry = gpd.points_from_xy(self.audio_metadata['longitude'], self.audio_metadata['latitude'])\n            gdf = gpd.GeoDataFrame(self.audio_metadata, geometry=geometry, crs=\"EPSG:4326\")\n            popup = [\"id\", \"datetaken\", \"detail\"]\n        else:\n            raise ValueError('Invalid data type provided. It has to be one of [\"svi\", \"audio\", \"photo\"].')\n\n        self.plot = gdf.explore(\n            popup=popup,\n            color=\"red\",\n            marker_kwds=dict(radius=5, fill=True),\n            tiles=\"CartoDB positron\",\n            name=\"map\",\n        )\n        return gdf if export_gdf else self.plot\n</code></pre> <p>getSV</p> <p>Retrieve the closest street view image(s) near a coordinate using the Mapillary API. The street view image will be reoriented to look at the coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>list | tuple</code> <p>coordinates (longitude/x and latitude/y)</p> required <code>loc_id</code> <code>int | str</code> <p>The id of the location</p> <code>None</code> <code>distance</code> <code>int</code> <p>The max distance in meters between the centroid and the street view</p> <code>50</code> <code>key</code> <code>str</code> <p>Mapillary API access token.</p> <code>None</code> <code>pano</code> <code>bool</code> <p>Whether to search for pano street view images only. (Default is True)</p> <code>False</code> <code>reoriented</code> <code>bool</code> <p>Whether to reorient and crop street view images. (Default is True)</p> <code>False</code> <code>multi_num</code> <code>int</code> <p>The number of multiple SVIs (Default is 1).</p> <code>1</code> <code>interval</code> <code>int</code> <p>The interval in meters between each SVI (Default is 1).</p> <code>1</code> <code>fov</code> <code>int</code> <p>Field of view in degrees for the perspective image. Defaults to 80.</p> <code>80</code> <code>heading</code> <code>int</code> <p>Camera heading in degrees. If None, it will be computed based on the location orientation.</p> <code>None</code> <code>pitch</code> <code>int</code> <p>Camera pitch angle. (Default is 10).</p> <code>5</code> <code>height</code> <code>int</code> <p>Height in pixels of the returned image. (Default is 480).</p> <code>500</code> <code>width</code> <code>int</code> <p>Width in pixels of the returned image. (Default is 640).</p> <code>700</code> <code>year</code> <code>list[str]</code> <p>Year of data (start year, end year).</p> <code>None</code> <code>season</code> <code>str</code> <p>Season of data.</p> <code>None</code> <code>time_of_day</code> <code>str</code> <p>Time of data.</p> <code>None</code> <code>output_df</code> <code>bool</code> <p>Whether to return a dataframe containing only the closest. (Default is True)</p> <code>True</code> <code>silent</code> <code>bool</code> <p>Whether to silence output (Default is False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DataFrame | list | None</code> <p>list[str]: A list of images in base64 format</p> <code>DataFrame</code> <code>DataFrame | list | None</code> <p>A dataframe containing metadata about the closest street view images.</p> Source code in <code>urbanworm/dataset.py</code> <pre><code>def getSV(location: list|tuple,\n          loc_id: int | str = None,\n          distance:int = 50,\n          key: str = None,\n          pano: bool = False,\n          reoriented: bool = False,\n          multi_num: int = 1,\n          interval: int = 1,\n          fov: int = 80, heading: int = None, pitch: int = 5,\n          height: int = 500, width: int = 700,\n          year: list | tuple = None,\n          season: str = None,\n          time_of_day: str = None,\n          output_df: bool = True,\n          silent: bool = False) -&gt; pd.DataFrame | list | None:\n    \"\"\"\n        getSV\n\n        Retrieve the closest street view image(s) near a coordinate using the Mapillary API.\n        The street view image will be reoriented to look at the coordinate.\n\n        Args:\n            location: coordinates (longitude/x and latitude/y)\n            loc_id (int|str, optional): The id of the location\n            distance (int): The max distance in meters between the centroid and the street view\n            key (str): Mapillary API access token.\n            pano (bool): Whether to search for pano street view images only. (Default is True)\n            reoriented (bool): Whether to reorient and crop street view images. (Default is True)\n            multi_num (int): The number of multiple SVIs (Default is 1).\n            interval (int): The interval in meters between each SVI (Default is 1).\n            fov (int): Field of view in degrees for the perspective image. Defaults to 80.\n            heading (int): Camera heading in degrees. If None, it will be computed based on the location orientation.\n            pitch (int): Camera pitch angle. (Default is 10).\n            height (int): Height in pixels of the returned image. (Default is 480).\n            width (int): Width in pixels of the returned image. (Default is 640).\n            year (list[str], optional): Year of data (start year, end year).\n            season (str, optional): Season of data.\n            time_of_day (str, optional): Time of data.\n            output_df (bool, optional): Whether to return a dataframe containing only the closest. (Default is True)\n            silent (bool, optional): Whether to silence output (Default is False).\n\n        Returns:\n            list[str]: A list of images in base64 format\n            DataFrame: A dataframe containing metadata about the closest street view images.\n    \"\"\"\n\n    bbox = projection(location, r=distance)\n    url = f\"https://graph.mapillary.com/images?access_token={key}&amp;fields=id,compass_angle,thumb_original_url,captured_at,geometry,sequence&amp;bbox={bbox}\"\n    # 2048 -&gt; original to get higher resolution\n    if pano:\n        url += \"&amp;is_pano=true\"\n    if pano == False and reoriented == True:\n        reoriented = False\n\n    svis = []\n    svi_df = {\n        \"id\": [],\n        \"sequence\": [],\n        \"captured_at\": [],\n        \"compass_angle\": [],\n        \"image_lon\": [],\n        \"image_lat\": [],\n        'url': [],\n        'loc_id': []\n    }\n    if loc_id is None:\n        del svi_df['loc_id']\n\n    try:\n        response = retry_request(url)\n        if response is None:\n            if not silent: print(f'skip location: {location} due to no data found')\n            if output_df:\n                return None, None\n            return None\n        response = response.json()\n        # find the closest image\n        response = closest(location, response, multi_num, interval, year, season, time_of_day, key)\n        if response is None:\n            if not silent: print(f'skip location: {location} due to no data found')\n            if output_df:\n                return None, None\n            return None\n\n        for index, row in response.iterrows():\n            # Extract Image ID, Compass Angle, image url, and coordinates\n            img_heading = float(row['compass_angle'])\n            img_url = row['thumb_original_url']\n            image_lon, image_lat = row['coordinates']\n            if heading is None:\n                # calculate bearing to the house\n                bearing_to_house = calculate_bearing(image_lat, image_lon, location[1], location[0])\n                relative_heading = (bearing_to_house - img_heading) % 360\n            else:\n                relative_heading = heading\n            # reframe image\n            if reoriented:\n                svi = Equirectangular(img_url=img_url)\n                sv = svi.GetPerspective(fov, relative_heading, pitch, height, width, 128)\n                svis.append(sv)\n            else:\n                svis.append(img_url)\n\n            if output_df:\n                svi_df['id'].append(row['id'])\n                svi_df['sequence'].append(row['sequence'])\n                svi_df['captured_at'].append(f'{row[\"year\"]}-{row[\"month\"]}-{row[\"day\"]}-{row[\"hour\"]}')\n                svi_df['image_lon'].append(image_lon)\n                svi_df['image_lat'].append(image_lat)\n                svi_df['compass_angle'].append(img_heading)\n                svi_df['url'].append(img_url)\n                if 'loc_id' in svi_df:\n                    svi_df['loc_id'].append(loc_id)\n        if output_df:\n            return svis, pd.DataFrame(svi_df)\n        else:\n            return svis\n    except Exception as e:\n        if not silent: print(f'skip location: {location} due to {e}')\n        if output_df:\n            return None, None\n        return None\n</code></pre> <p>getPhoto</p> <p>Fetch public Flickr photos with geotags near a location (or within a Flickr place).</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>list | tuple</code> <p>(lon, lat) required. Coordinates of location (longitude, latitude) for searching for geotagged photos</p> required <code>loc_id</code> <code>int | str</code> <p>The id of the location.</p> <code>None</code> <code>distance</code> <code>int</code> <p>Search radius in meters (converted to km; Flickr radius max is 32 km).</p> <code>50</code> <code>key</code> <code>str</code> <p>Flickr API key. If None, reads env var FLICKR_API_KEY.</p> <code>None</code> <code>query</code> <code>str | list[str]</code> <p>Query parameters to pass to Flickr API (free text search).</p> <code>None</code> <code>tag</code> <code>str | list[str]</code> <p>Tag string or list of tags (comma-separated). Acts as a \"limiting agent\" for geo queries.</p> <code>None</code> <code>max_return</code> <code>int</code> <p>Number of photos to return (after filters).</p> <code>1</code> <code>year</code> <code>str | tuple</code> <p>[Y] or (Y,) or (Y1, Y2) inclusive. Filters by taken date range.</p> <code>None</code> <code>season</code> <code>str</code> <p>One of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by taken month).</p> <code>None</code> <code>time_of_day</code> <code>str</code> <p>One of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by taken hour).</p> <code>None</code> <code>exclude_from_location</code> <code>int</code> <p>drop retrieved photos within a distance (in meter) from the given location. (Default is None)</p> <code>None</code> <code>output_df</code> <code>bool</code> <p>If True, return a pandas.DataFrame; otherwise return dict (if max_return==1)        or list[dict].</p> <code>True</code> <p>Returns:</p> Type Description <p>dict | list[dict] | pandas.DataFrame</p> Source code in <code>urbanworm/dataset.py</code> <pre><code>def getPhoto(\n        location: list | tuple,\n        loc_id: int | str = None,\n        distance: int = 50,\n        key: str = None,\n        query: str | list[str] = None,\n        tag: str | list[str] = None,\n        max_return: int = 1,\n        year: list | tuple = None,\n        season: str = None,\n        time_of_day: str = None,\n        exclude_from_location:int = None,\n        output_df: bool = True\n):\n    \"\"\"\n        getPhoto\n\n        Fetch public Flickr photos with geotags near a location (or within a Flickr place).\n\n        Args:\n            location (list|tuple): (lon, lat) required. Coordinates of location (longitude, latitude) for searching for geotagged photos\n            loc_id (int | str): The id of the location.\n            distance (int): Search radius in meters (converted to km; Flickr radius max is 32 km).\n            key (str): Flickr API key. If None, reads env var FLICKR_API_KEY.\n            query (str | list[str]): Query parameters to pass to Flickr API (free text search).\n            tag: Tag string or list of tags (comma-separated). Acts as a \"limiting agent\" for geo queries.\n            max_return: Number of photos to return (after filters).\n            year (str | tuple): [Y] or (Y,) or (Y1, Y2) inclusive. Filters by taken date range.\n            season (str): One of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by taken month).\n            time_of_day (str): One of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by taken hour).\n            exclude_from_location (int, optional): drop retrieved photos within a distance (in meter) from the given location. (Default is None)\n            output_df (bool): If True, return a pandas.DataFrame; otherwise return dict (if max_return==1)\n                       or list[dict].\n\n        Returns:\n            dict | list[dict] | pandas.DataFrame\n    \"\"\"\n\n    import os\n    import requests\n    from datetime import datetime, timedelta, timezone\n\n    if exclude_from_location is not None:\n        drop_area = projection(location, r=distance)\n\n    # -------------------------\n    # Validate inputs\n    # -------------------------\n    if max_return is None or int(max_return) &lt; 1:\n        raise ValueError(\"max_return must be &gt;= 1.\")\n    max_return = int(max_return)\n\n    api_key = key or os.getenv(\"FLICKR_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Missing Flickr API key. Pass key=... or set env var FLICKR_API_KEY.\")\n\n    lon, lat = location\n    months = season_months(season)\n    hours = tod_hours(time_of_day)\n    y_range = year_range(year)\n\n    # Radius in km (Flickr max 32km) :contentReference[oaicite:3]{index=3}\n    radius_km = max(float(distance) / 1000.0, 0.01)\n    radius_km = min(radius_km, 32.0)\n\n    # Geo queries need a \"limiting agent\"; tags or min/max dates qualify. :contentReference[oaicite:4]{index=4}\n    # If user provided none, default to last 365 days so results aren\u2019t silently limited to ~12 hours.\n    now_utc = datetime.now(timezone.utc)\n    default_min_upload_date = int((now_utc - timedelta(days=365)).timestamp())\n\n    # -------------------------\n    # Build Flickr request\n    # -------------------------\n    endpoint = \"https://api.flickr.com/services/rest/\"\n\n    extras = \",\".join(\n        [\n            \"description\",\n            \"license\",\n            \"date_upload\",\n            \"date_taken\",\n            \"owner_name\",\n            \"geo\",\n            \"tags\",\n            \"views\",\n            \"media\",\n            \"url_sq\",\n            \"url_t\",\n            \"url_s\",\n            \"url_q\",\n            \"url_m\",\n            \"url_n\",\n            \"url_z\",\n            \"url_c\",\n            \"url_l\",\n            \"url_o\",\n        ]\n    )\n\n    params = {\n        \"method\": \"flickr.photos.search\",\n        \"api_key\": api_key,\n        \"format\": \"json\",\n        \"nojsoncallback\": 1,\n        \"extras\": extras,\n        \"safe_search\": 1, # safe only for un-authed calls\n        \"media\": \"photos\",\n        \"has_geo\": 1,\n        \"content_types\": 0, # photos\n        \"sort\": \"relevance\",\n        \"lat\": lat,\n        \"lon\": lon,\n        \"radius\": radius_km,\n        \"radius_units\": \"km\"\n    }\n\n    if query:\n        q = query_string(query)\n        if q:\n            params[\"text\"] = q\n\n    # tags\n    if tag:\n        if isinstance(tag, (list, tuple)):\n            tags = \",\".join([str(t).strip() for t in tag if str(t).strip()])\n            params[\"tags\"] = tags\n            params[\"tag_mode\"] = \"all\"\n        else:\n            params[\"tags\"] = str(tag).strip()\n\n    # date range (taken) if specified\n    if y_range is not None:\n        params[\"min_taken_date\"], params[\"max_taken_date\"] = y_range\n    else:\n        # If no explicit limiting agent, set min_upload_date (acts as limiting agent for geo queries). :contentReference[oaicite:7]{index=7}\n        if not tag and season is None and time_of_day is None:\n            params[\"min_upload_date\"] = default_min_upload_date\n\n    # -------------------------\n    # Fetch + post-filter\n    # -------------------------\n    session = requests.Session()\n\n    # Geo/bbox queries only return up to 250/page. :contentReference[oaicite:8]{index=8}\n    per_page = min(250, max(50, max_return * 20))\n    params[\"per_page\"] = per_page\n\n    results = []\n    seen = set()\n\n    max_pages = 150\n    for page in range(1, max_pages + 1):\n        params[\"page\"] = page\n        r = session.get(endpoint, params=params, timeout=30)\n        r.raise_for_status()\n        data = r.json()\n\n        if data.get(\"stat\") != \"ok\":\n            msg = data.get(\"message\") or data.get(\"error\") or str(data)\n            raise RuntimeError(f\"Flickr API error: {msg}\")\n\n        photos = (data.get(\"photos\") or {}).get(\"photo\") or []\n        if not photos:\n            break\n\n        for p in photos:\n            if exclude_from_location is not None:\n                if is_coordinate_in_bbox(p[\"longitude\"], p[\"latitude\"], drop_area):\n                    continue\n            pid = p.get(\"id\")\n            if not pid or pid in seen:\n                continue\n            seen.add(pid)\n\n            taken_dt = parse_taken(p)\n            if months and taken_dt and taken_dt.month not in months:\n                continue\n            if hours and taken_dt and taken_dt.hour not in hours:\n                continue\n\n            s_lat = float(p[\"latitude\"]) if \"latitude\" in p and p[\"latitude\"] not in (None, \"\") else None\n            s_lon = float(p[\"longitude\"]) if \"longitude\" in p and p[\"longitude\"] not in (None, \"\") else None\n\n            url = best_url(p)\n            out = {\n                \"loc_id\": '',\n                \"id\": pid,\n                \"title\": p.get(\"title\"),\n                \"owner\": p.get(\"owner\"),\n                # \"ownername\": p.get(\"ownername\"),\n                \"datetaken\": p.get(\"datetaken\") or p.get(\"date_taken\"),\n                \"latitude\": s_lat,\n                \"longitude\": s_lon,\n                # \"accuracy\": int(p[\"accuracy\"]) if \"accuracy\" in p and str(p[\"accuracy\"]).isdigit() else None,\n                \"distance_m\": haversine_m(lat, lon, s_lat, s_lon) if (s_lat is not None and s_lon is not None) else None,\n                \"tags\": p.get(\"tags\"),\n                \"description\": p.get(\"description\"),\n                \"views\": int(p[\"views\"]) if \"views\" in p and str(p[\"views\"]).isdigit() else None,\n                \"license\": p.get(\"license\"),\n                \"url\": url,\n                # \"page_url\": f\"https://www.flickr.com/photos/{p.get('owner')}/{pid}\",\n            }\n\n            if loc_id is not None:\n                out[\"loc_id\"] = loc_id\n            else:\n                del out[\"loc_id\"]\n\n            results.append(out)\n\n            # if len(results) &gt;= max_return:\n            #     break\n\n        if len(results) &gt;= max_return:\n            break\n\n    if output_df:\n        import pandas as pd\n        df = pd.DataFrame(results)\n        df = df.sort_values(by='distance_m', ascending=True)\n        return df.head(max_return)\n\n    if max_return == 1:\n        return results[0] if results else None\n    return results\n</code></pre> <p>getSound</p> <p>Fetch geotagged Freesound audio near a point, using Freesound API v2 search + geospatial filter.</p> <p>Notes: - Uses token authentication (API key) via Authorization header. - Returns preview URLs (mp3/ogg). Downloading original audio requires OAuth2.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>list | tuple</code> <p>(lon, lat) required.</p> required <code>loc_id</code> <code>int | str</code> <p>.</p> <code>None</code> <code>distance</code> <code>int</code> <p>radius in meters (converted to km for Freesound geofilt).</p> <code>50</code> <code>key</code> <code>str</code> <p>Freesound API key. If None, reads env var FREESOUND_API_KEY.</p> <code>None</code> <code>query</code> <code>str</code> <p>Freesound API query (e.g., 'traffic', '\"bird song\" -crow').</p> <code>None</code> <code>tag</code> <code>str | list[str]</code> <p>tag string or list of tags (used as filters).</p> <code>None</code> <code>max_return</code> <code>int</code> <p>number of sounds to return (after post-filters).</p> <code>1</code> <code>year</code> <code>list | tuple</code> <p>[Y] or (Y,) or (Y1, Y2) inclusive (filters by upload date \"created\").</p> <code>None</code> <code>season</code> <code>str</code> <p>one of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by created month).</p> <code>None</code> <code>time_of_day</code> <code>str</code> <p>one of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by created hour).</p> <code>None</code> <code>duration</code> <code>int</code> <p>maximum duration in seconds (&lt;= duration). If you want a range, pass a tuple/list (min,max). (Default is 300)</p> <code>300</code> <code>exclude_from_location</code> <code>int</code> <p>Drop retrieved photos within a distance (in meter) from the given location.</p> <code>None</code> <code>slice_duration</code> <code>int</code> <p>Split the original sound signal into clips with the given duration.</p> <code>None</code> <code>slice_max_num</code> <code>int</code> <p>Maximum number of clips sliced from the original sound signal.</p> <code>None</code> <code>output_df</code> <code>bool</code> <p>if True, return a pandas.DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dict | list[dict] | pandas.DataFrame</p> Source code in <code>urbanworm/dataset.py</code> <pre><code>def getSound(\n        location: list | tuple,\n        loc_id: int | str = None,\n        distance: int = 50,\n        key: str = None,\n        query: str | list[str] | None = None,\n        tag: str | list[str] = None,\n        max_return: int = 1,\n        year: list | tuple = None,\n        season: str = None,\n        time_of_day: str = None,\n        duration: int = 300,\n        exclude_from_location:int = None,\n        slice_duration:int = None,\n        slice_max_num:int = None,\n        output_df: bool = True,\n) -&gt; pd.DataFrame:\n\n    \"\"\"\n        getSound\n\n        Fetch geotagged Freesound audio near a point, using Freesound API v2 search + geospatial filter.\n\n        Notes:\n        - Uses token authentication (API key) via Authorization header.\n        - Returns preview URLs (mp3/ogg). Downloading original audio requires OAuth2.\n\n        Args:\n            location: (lon, lat) required.\n            loc_id (int | str, optional): .\n            distance (int): radius in meters (converted to km for Freesound geofilt).\n            key (str): Freesound API key. If None, reads env var FREESOUND_API_KEY.\n            query (str, optional): Freesound API query (e.g., 'traffic', '\"bird song\" -crow').\n            tag: tag string or list of tags (used as filters).\n            max_return: number of sounds to return (after post-filters).\n            year: [Y] or (Y,) or (Y1, Y2) inclusive (filters by upload date \"created\").\n            season (str): one of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by created month).\n            time_of_day (str): one of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by created hour).\n            duration (int): maximum duration in seconds (&lt;= duration). If you want a range, pass a tuple/list (min,max). (Default is 300)\n            exclude_from_location (int, optional): Drop retrieved photos within a distance (in meter) from the given location.\n            slice_duration (int, optional): Split the original sound signal into clips with the given duration.\n            slice_max_num (int, optional): Maximum number of clips sliced from the original sound signal.\n            output_df (bool): if True, return a pandas.DataFrame.\n\n        Returns:\n            dict | list[dict] | pandas.DataFrame\n    \"\"\"\n    import os\n    import requests\n    from datetime import datetime\n\n    if exclude_from_location is not None:\n        drop_area = projection(location, r=distance)\n\n    # -------------------------\n    # Helpers\n    # -------------------------\n    def _parse_created(s):\n        if not s:\n            return None\n        # Examples look like \"2014-04-16T20:07:11.145\" (no timezone).\n        # Try a couple variants.\n        for fmt in (\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%S\"):\n            try:\n                return datetime.strptime(s, fmt)\n            except Exception:\n                pass\n        # last resort: fromisoformat (py3.11+ handles many cases)\n        try:\n            return datetime.fromisoformat(s.replace(\"Z\", \"\"))\n        except Exception:\n            return None\n\n    def _year_range(y, with_z: bool=False):\n        if y is None:\n            return None\n        if not isinstance(y, (list, tuple)) or len(y) == 0:\n            raise ValueError(\"year must be a list/tuple like [2020] or (2020, 2022).\")\n        if len(y) == 1:\n            y1 = y2 = int(y[0])\n        else:\n            y1, y2 = int(y[0]), int(y[1])\n            if y2 &lt; y1:\n                y1, y2 = y2, y1\n\n        # Use standard Solr-like ISO range; we will retry without 'Z' if needed.\n        z = \"Z\" if with_z else \"\"\n        start = f\"{y1:04d}-01-01T00:00:00{z}\"\n        end = f\"{y2:04d}-12-31T23:59:59{z}\"\n        return start, end\n\n    # -------------------------\n    # Validate inputs\n    # -------------------------\n    if max_return is None or int(max_return) &lt; 1:\n        raise ValueError(\"max_return must be &gt;= 1.\")\n    max_return = int(max_return)\n\n    api_key = key or os.getenv(\"FREESOUND_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Missing Freesound API key. Pass key=... or set env var FREESOUND_API_KEY.\")\n\n    lon, lat = location\n\n    # meters -&gt; km for geofilt d=&lt;km&gt;\n    radius_km = max(float(distance) / 1000.0, 0.01)\n\n    months = season_months(season)\n    hours = tod_hours(time_of_day)\n\n    # duration: allow int (max seconds) or tuple/list (min,max)\n    dur_filter = None\n    if duration is not None:\n        if isinstance(duration, (list, tuple)) and len(duration) == 2:\n            dmin = float(duration[0])\n            dmax = float(duration[1])\n            if dmax &lt; dmin:\n                dmin, dmax = dmax, dmin\n            dur_filter = f\"duration:[{dmin} TO {dmax}]\"\n        else:\n            dmax = float(duration)\n            dur_filter = f\"duration:[0 TO {dmax}]\"\n\n    # -------------------------\n    # Build request\n    # -------------------------\n    endpoint = \"https://freesound.org/apiv2/search/\"\n    headers = {\"Authorization\": f\"Token {api_key}\"}  # token auth\n\n    # Request useful fields, including previews (mp3/ogg URLs) and geotag.\n    fields = \",\".join(\n        [\n            \"id\",\n            \"name\",\n            \"username\",\n            \"license\",\n            \"created\",\n            \"duration\",\n            \"geotag\",\n            \"tags\",\n            \"previews\",\n            \"url\",\n            \"num_downloads\",\n            \"avg_rating\",\n            \"description\"\n        ]\n    )\n\n    # Base filter parts\n    filter_parts = []\n    filter_parts.append(\"is_geotagged:1\")\n    filter_parts.append(f\"{{!geofilt sfield=geotag pt={lat},{lon} d={radius_km}}}\")\n\n    # tag filters\n    if tag:\n        if isinstance(tag, (list, tuple)):\n            for t in tag:\n                t = str(t).strip()\n                if t:\n                    filter_parts.append(f\"tag:{t}\")\n        else:\n            t = str(tag).strip()\n            if t:\n                filter_parts.append(f\"tag:{t}\")\n\n    if dur_filter:\n        filter_parts.append(dur_filter)\n\n    # year filter (created range): try with Z, retry without if API complains\n    created_range_z = _year_range(year, with_z=True)\n    created_range_noz = _year_range(year, with_z=False)\n\n    qstr = query_string(query)\n\n    def _do_request(created_range):\n        fp = list(filter_parts)\n        if created_range is not None:\n            start, end = created_range\n            fp.append(f\"created:[{start} TO {end}]\")\n        params = {\n            \"query\": qstr,  # empty query allowed\n            \"filter\": \" \".join(fp),\n            \"fields\": fields,\n            \"page\": 1,\n            \"page_size\": min(150, max(50, max_return * 25)),\n            \"sort\": \"score\",\n        }\n        return params\n\n    session = requests.Session()\n\n    # -------------------------\n    # Fetch + post-filter\n    # -------------------------\n    results = []\n    seen = set()\n    max_pages = 150\n\n    # First attempt (with Z)\n    params = _do_request(created_range_z)\n\n    for attempt in (1, 2):\n        try:\n            for page in range(1, max_pages + 1):\n                params[\"page\"] = page\n                r = session.get(endpoint, params=params, headers=headers, timeout=999)\n\n                if r.status_code == 400 and attempt == 1 and year is not None:\n                    # likely date format issue; retry without Z\n                    raise ValueError(\"Date filter rejected; retrying without 'Z'.\")\n                if r.status_code == 404:\n                    break\n                r.raise_for_status()\n                data = r.json()\n\n                page_results = data.get(\"results\") or []\n                if not page_results:\n                    break\n\n                for s in page_results:\n                    sid = s.get(\"id\")\n                    if sid is None or sid in seen:\n                        continue\n                    seen.add(sid)\n\n                    created_dt = _parse_created(s.get(\"created\"))\n                    if months and created_dt and created_dt.month not in months:\n                        continue\n                    if hours and created_dt and created_dt.hour not in hours:\n                        continue\n\n                    # Parse geotag \"lat lon\"\n                    s_lat = s_lon = None\n                    if s.get(\"geotag\"):\n                        parts = str(s[\"geotag\"]).split()\n                        if len(parts) == 2:\n                            try:\n                                s_lat = float(parts[0])\n                                s_lon = float(parts[1])\n                                if exclude_from_location is not None:\n                                    if is_coordinate_in_bbox(s_lon, s_lat, drop_area):\n                                        continue\n                            except Exception:\n                                pass\n\n                    out = {\n                        \"loc_id\": '',\n                        \"id\": sid,\n                        \"name\": s.get(\"name\"),\n                        \"username\": s.get(\"username\"),\n                        \"license\": s.get(\"license\"),\n                        \"created\": s.get(\"created\"),\n                        \"duration\": s.get(\"duration\"),\n                        \"tags\": s.get(\"tags\"),\n                        \"geotag\": s.get(\"geotag\"),\n                        \"latitude\": s_lat,\n                        \"longitude\": s_lon,\n                        \"distance_m\": haversine_m(lat, lon, s_lat, s_lon) if (s_lat is not None and s_lon is not None) else None,\n                        \"previews\": s.get(\"previews\"),\n                        \"url\": s.get(\"url\"),\n                        \"page_url\": f\"https://freesound.org/people/{s.get('username')}/sounds/{sid}/\" if s.get(\"username\") and sid else None,\n                        \"description\": s.get(\"description\"),\n                        \"num_downloads\": s.get(\"num_downloads\"),\n                        \"avg_rating\": s.get(\"avg_rating\"),\n                        \"slice\": []\n                    }\n                    if loc_id is not None:\n                        out[\"loc_id\"] = loc_id\n                    else:\n                        del out[\"loc_id\"]\n                    if slice_duration is None:\n                        del out[\"slice\"]\n                    else:\n                        out[\"slice\"] = sliced_duration(int(out[\"duration\"]), slice_duration, slice_max_num)\n                    results.append(out)\n                    # if len(results) &gt;= max_return:\n                    #     break\n                if not data.get(\"next\"):\n                    break\n                if len(results) &gt;= max_return:\n                    break\n\n            break  # success, don\u2019t do second attempt\n        except ValueError:\n            # Retry without Z in created range\n            if attempt == 1 and year is not None:\n                params = _do_request(created_range_noz)\n                continue\n            raise\n\n    # -------------------------\n    # Return shape\n    # -------------------------\n    if output_df:\n        import pandas as pd\n        df = pd.DataFrame(results)\n        df = df.sort_values(by='distance_m', ascending=True)\n        previews_df = df['previews'].apply(pd.Series)\n        previews_df.columns = [f'{col}' for col in previews_df.columns]\n        df = pd.concat([df.drop('previews', axis=1), previews_df], axis=1)\n        return df.head(max_return)\n\n    if max_return == 1:\n        return results[0] if results else None\n    return results\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.__init__","title":"<code>__init__(locations=None, units=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>locations</code> <code>list | tuple | dict | Dataframe</code> <p>A list of coordinates (longitude/x and latitude/y) or a dictionary keyed by longitude and latitude or a dataframe with columns \"longitude\" and \"latitude\".</p> <code>None</code> <code>units</code> <code>GeoDataFrame</code> <p>The path to the shapefile or geojson file, or GeoDataFrame.</p> <code>None</code> <p>Examples:</p>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.__init__--retrieve-street-view-with-building-footprints-osm","title":"retrieve street view with building footprints (OSM)","text":"<p>gtd = GeoTaggedData() gtd.getBuildingFootprints(bbox=(-83.235572,42.348092,-83.235154,42.348806)) gtd.get_svi_from_locations(key=\"your Mapillary token\")</p>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.__init__--locations-a-nested-list-of-coordinates","title":"locations - a nested list of coordinates","text":"<p>gtd = GeoTaggedData(location=[[-83.235572,42.348092],[-83.235154,42.348806]])</p>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.__init__--locations-a-dataframe-with-columns-longitude-and-latitude","title":"locations - a dataframe with columns \"longitude\" and \"latitude\"","text":"<p>df = pd.Dataframe({\"longitude\":[-83.235572, -83.235154], \"latitude\":[42.348092, 42.348806]}) gtd = GeoTaggedData(locations=df)</p> Source code in <code>urbanworm/dataset.py</code> <pre><code>def __init__(self,\n             locations: list|tuple|dict|pd.DataFrame=None,\n             units: GeoDataFrame=None):\n    '''\n    Args:\n        locations (list|tuple|dict|Dataframe): A list of coordinates (longitude/x and latitude/y) or a dictionary keyed by longitude and latitude or a dataframe with columns \"longitude\" and \"latitude\".\n        units (GeoDataFrame): The path to the shapefile or geojson file, or GeoDataFrame.\n\n    Examples:\n        # retrieve street view with building footprints (OSM)\n        gtd = GeoTaggedData()\n        gtd.getBuildingFootprints(bbox=(-83.235572,42.348092,-83.235154,42.348806))\n        gtd.get_svi_from_locations(key=\"your Mapillary token\")\n\n        # locations - a nested list of coordinates\n        gtd = GeoTaggedData(location=[[-83.235572,42.348092],[-83.235154,42.348806]])\n        # locations - a dataframe with columns \"longitude\" and \"latitude\"\n        df = pd.Dataframe({\"longitude\":[-83.235572, -83.235154], \"latitude\":[42.348092, 42.348806]})\n        gtd = GeoTaggedData(locations=df)\n    '''\n\n    self.images = None\n    self.locations = locations\n    self.units = units\n    if locations is not None and units is None:\n        self.construct_units()\n\n    self.svis = self.photos = self.audios = {\n        'loc_id': [],\n        'id': [],\n        'data': [],\n        'path':[],\n    }\n\n    self.svi_metadata = None\n    self.photo_metadata = None\n    self.audio_metadata = None\n    self.plot = None\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.download_to_dir","title":"<code>download_to_dir(data=None, to_dir=None, prefix=None)</code>","text":"<p>download_to_dir</p> <p>Download retrieved data to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Type of data to download: ['svi', 'audio', 'photo'].</p> <code>None</code> <code>to_dir</code> <code>str</code> <p>the directory to save the downloaded data.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to add to the output filename.</p> <code>None</code> Source code in <code>urbanworm/dataset.py</code> <pre><code>def download_to_dir(self, data:str = None, to_dir:str = None, prefix: str = None)-&gt; None:\n    '''\n        download_to_dir\n\n        Download retrieved data to a directory.\n\n        Args:\n            data (str): Type of data to download: ['svi', 'audio', 'photo'].\n            to_dir (str): the directory to save the downloaded data.\n            prefix (str, optional):  The prefix to add to the output filename.\n    '''\n    if data not in ['svi', 'audio', 'photo']:\n        raise ValueError('Invalid data type provided. It has to be one of [\"svi\", \"audio\", \"photo\"].')\n    if to_dir is not None:\n        if not os.path.exists(to_dir):\n            print(\"The directory doesn't exist.\")\n            print(\"The directory is created now.\")\n            out_dir = Path(to_dir)\n            out_dir.mkdir(parents=True, exist_ok=True)\n    else:\n        print(\"You need to specify a directory to download.\")\n        return None\n    if data == 'svi':\n        if len(self.svis['id']) == 0:\n            return None\n        self.svis['path'] = []\n        for i in tqdm(range(len(self.svis['data'])), total=len(self.svis['data'])):\n            loc_id = self.svis['loc_id'][i]\n            img_id = self.svis['id'][i]\n            path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n            p = path + f'_{img_id}.png'\n            try:\n                if is_base64(self.svis['data'][i]):\n                    save_base64(self.svis['data'][i], p)\n                else:\n                    download_image_requests(self.svis['data'][i], p)\n            except:\n                self.svis['path'] += [\" \"]\n                continue\n            self.svis['path'] += [p]\n    elif data == 'audio':\n        if len(self.audios['id']) == 0:\n            return None\n        self.audios['path'] = []\n        if 'slice' in self.audios:\n            for i in tqdm(range(len(self.audios['data'])), total=len(self.audios['data'])):\n                loc_id = self.audios['loc_id'][i]\n                audio_id = self.audios['id'][i]\n                slices = self.audios['slice'][i]\n                path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n                start = slices[0]\n                end = slices[1]\n                p = path + f'_{audio_id}_clip_{start}_{end}.mp3'\n                try:\n                    clip(self.audios['data'][i], start, end, p)\n                except:\n                    continue\n                self.audios['path'] += [p]\n        else:\n            for i in tqdm(range(len(self.audios['data'])), total=len(self.audios['data'])):\n                loc_id = self.audios['loc_id'][i]\n                audio_id = self.audios['id'][i]\n                path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n                p = path + f'_{audio_id}.mp3'\n                try:\n                    download_freesound_preview(self.audios['data'][i], p)\n                except:\n                    self.audios['path'] += [\" \"]\n                    continue\n                self.audios['path'] += [p]\n    elif data == 'photo':\n        if len(self.photos['id']) == 0:\n            return None\n        self.photos['path'] = []\n        for i in tqdm(range(len(self.photos['data'])), total=len(self.photos['data'])):\n            loc_id = self.photos['loc_id'][i]\n            photo_id = self.photos['id'][i]\n            path = f'{to_dir}/{prefix}_{loc_id}' if prefix is not None else f'./{to_dir}/{loc_id}'\n            p = path + f'_{photo_id}.png'\n            try:\n                download_image_requests(self.photos['data'][i], p)\n            except:\n                self.photos['path'] += [\" \"]\n            self.photos['path'] += [p]\n    return None\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.getBuildings","title":"<code>getBuildings(bbox=None, source='osm', min_area=0, max_area=None, random_sample=None)</code>","text":"<p>Extract buildings from OpenStreetMap using the bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list or tuple</code> <p>The bounding box.</p> <code>None</code> <code>source</code> <code>str</code> <p>The source of the buildings. ['osm', 'microsoft']</p> <code>'osm'</code> <code>min_area</code> <code>float or int</code> <p>The minimum area.</p> <code>0</code> <code>max_area</code> <code>float or int</code> <p>The maximum area.</p> <code>None</code> <code>random_sample</code> <code>int</code> <p>The number of random samples.</p> <code>None</code> Source code in <code>urbanworm/dataset.py</code> <pre><code>def getBuildings(self,\n                 bbox: list | tuple = None,\n                 source: str = 'osm',\n                 min_area: float | int = 0,\n                 max_area: float | int = None,\n                 random_sample: int = None)-&gt; None:\n    '''\n        Extract buildings from OpenStreetMap using the bbox.\n\n        Args:\n            bbox (list or tuple): The bounding box.\n            source (str): The source of the buildings. ['osm', 'microsoft']\n            min_area (float or int): The minimum area.\n            max_area (float or int): The maximum area.\n            random_sample (int): The number of random samples.\n    '''\n\n    if source not in ['osm', 'microsoft']:\n        raise Exception(f'{source} is not supported')\n\n    if source == 'osm':\n        buildings = getOSMbuildings(bbox, min_area, max_area)\n    elif source == 'microsoft':\n        buildings = getGlobalMLBuilding(bbox, min_area, max_area)\n    if buildings is None or buildings.empty:\n        if source == 'osm':\n            print(\"No buildings found in the bounding box. Please check https://overpass-turbo.eu/ for areas with buildings.\")\n            return None\n        if source == 'microsoft':\n            print(\"No buildings found in the bounding box. Please check https://github.com/microsoft/GlobalMLBuildingFootprints for areas with buildings.\")\n            return None\n    if random_sample is not None:\n        buildings = buildings.sample(random_sample)\n    self.units = buildings.to_crs(4326)\n    print(f\"{len(buildings)} buildings found in the bounding box.\")\n    return None\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.get_photo_from_location","title":"<code>get_photo_from_location(id_column=None, distance=50, key=None, query=None, tag=None, max_return=1, year=None, season=None, time_of_day=None, exclude_personal_photo=True, exclude_from_location=None, silent=True)</code>","text":"<p>get_photo_from_location</p> <p>Retrieve geotagged photos from Flickr</p> <p>Parameters:</p> Name Type Description Default <code>id_column</code> <code>str</code> <p>(str, optional): The name of column that has unique identifier (or something similar) for each location.</p> <code>None</code> <code>distance</code> <code>int</code> <p>Search radius in meters (converted to km; Flickr radius max is 32 km).</p> <code>50</code> <code>key</code> <code>str</code> <p>Flickr API key. If None, reads env var FLICKR_API_KEY.</p> <code>None</code> <code>query</code> <code>str</code> <p>Query string to search for.</p> <code>None</code> <code>tag</code> <code>str | list[str]</code> <p>Tag string or list of tags (comma-separated). Acts as a \"limiting agent\" for geo queries.</p> <code>None</code> <code>max_return</code> <code>int</code> <p>Number of photos to return (after filters).</p> <code>1</code> <code>year</code> <code>list | tuple</code> <p>[Y] or (Y,) or (Y1, Y2) inclusive. Filters by taken date range.</p> <code>None</code> <code>season</code> <code>str</code> <p>One of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by taken month).</p> <code>None</code> <code>time_of_day</code> <code>str</code> <p>One of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by taken hour).</p> <code>None</code> <code>exclude_personal_photo</code> <code>bool</code> <p>If True, exclude personal photo from locations. (Default is True)</p> <code>True</code> <code>exclude_from_location</code> <code>int</code> <p>Drop retrieved data with a distance from the given location.</p> <code>None</code> <code>silent</code> <code>bool</code> <p>If True, do not show error traceback (Default is True).</p> <code>True</code> Source code in <code>urbanworm/dataset.py</code> <pre><code>def get_photo_from_location(self,\n                            id_column:str=None,\n                            distance: int = 50,\n                            key: str = None,\n                            query: str | list[str] = None,\n                            tag: str | list[str] = None,\n                            max_return: int = 1,\n                            year: list | tuple = None,\n                            season: str = None,\n                            time_of_day: str = None,\n                            exclude_personal_photo: bool = True,\n                            exclude_from_location:int = None,\n                            silent = True,\n                            ):\n    '''\n        get_photo_from_location\n\n        Retrieve geotagged photos from Flickr\n\n        Args:\n            id_column: (str, optional): The name of column that has unique identifier (or something similar) for each location.\n            distance (int): Search radius in meters (converted to km; Flickr radius max is 32 km).\n            key (str): Flickr API key. If None, reads env var FLICKR_API_KEY.\n            query (str, optional): Query string to search for.\n            tag (str | list[str]): Tag string or list of tags (comma-separated). Acts as a \"limiting agent\" for geo queries.\n            max_return (int): Number of photos to return (after filters).\n            year: [Y] or (Y,) or (Y1, Y2) inclusive. Filters by taken date range.\n            season (str): One of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by taken month).\n            time_of_day (str): One of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by taken hour).\n            exclude_personal_photo (bool): If True, exclude personal photo from locations. (Default is True)\n            exclude_from_location (int, optional): Drop retrieved data with a distance from the given location.\n            silent (bool): If True, do not show error traceback (Default is True).\n    '''\n\n    from .utils.pano2pers import read_url2img\n    from importlib.resources import files, as_file\n\n    self.photos = {\n        'loc_id': [],\n        'id': [],\n        'data': [],\n        'path': [],\n    }\n    self.photo_metadata = None\n\n    if id_column is None:\n        id_column = 'loc_id'\n        if id_column not in self.units.columns:\n            self.units[id_column] = [i for i in range(len(self.units))]\n    res_df = None\n    skip_count = 0\n    for index, row in tqdm(self.units.iterrows(), total=len(self.units)):\n        loc_id = row['loc_id']\n        try:\n            output_df = getPhoto([row.geometry.centroid.x, row.geometry.centroid.y],\n                                 loc_id,\n                                 distance,\n                                 key,\n                                 query,\n                                 tag,\n                                 max_return,\n                                 year,\n                                 season,\n                                 time_of_day,\n                                 exclude_from_location,\n                                 output_df=True)\n            if exclude_personal_photo:\n                model_res = files(\"urbanworm.models\") / \"face_detection_yunet_2023mar.onnx\"\n                drop_list = []\n                for ind, r in output_df.iterrows():\n                    with as_file(model_res) as model_path:\n                        is_selfie = is_selfie_photo(model_path, r['url'])\n                        if is_selfie:\n                            drop_list += [ind]\n                if len(drop_list) &gt; 0:\n                    output_df.drop(drop_list, axis=0, inplace=True)\n                    if len(output_df) == 0:\n                        continue\n\n            self.photos['loc_id'] += output_df['loc_id'].tolist()\n            self.photos['data'] += output_df['url'].tolist()\n            self.photos['id'] += output_df['id'].tolist()\n            if res_df is None:\n                res_df = output_df\n            else:\n                res_df = pd.concat([res_df, output_df])\n        except Exception as e:\n            if not silent: print(e)\n            skip_count += 1\n            continue\n    self.photo_metadata = res_df\n    if skip_count &gt; 0:\n        print(f'Collect data for {len(self.units) - skip_count} locations and skipped {skip_count} locations due to no data found.')\n    return None\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.get_sound_from_location","title":"<code>get_sound_from_location(id_column=None, distance=50, key=None, query=None, tag=None, max_return=1, year=None, season=None, time_of_day=None, duration=None, exclude_from_location=None, slice_duration=None, slice_max_num=None, silent=True)</code>","text":"<p>get_sound_from_location</p> <p>Retrieve geotagged sound recordings from Freesound</p> <p>Parameters:</p> Name Type Description Default <code>id_column</code> <code>str</code> <p>The name of column that has unique identifier (or something similar) for each location.</p> <code>None</code> <code>distance</code> <code>int</code> <p>radius in meters (converted to km for Freesound geofilt).</p> <code>50</code> <code>key</code> <code>str</code> <p>Freesound API key. If None, reads env var FREESOUND_API_KEY.</p> <code>None</code> <code>query</code> <code>str</code> <p>Query string to search for.</p> <code>None</code> <code>tag</code> <code>str | list[str]</code> <p>tag string or list of tags (used as filters).</p> <code>None</code> <code>max_return</code> <code>int</code> <p>number of sounds to return (after post-filters).</p> <code>1</code> <code>year</code> <code>int | list</code> <p>[Y] or (Y,) or (Y1, Y2) inclusive (filters by upload date \"created\").</p> <code>None</code> <code>season</code> <code>str</code> <p>one of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by created month).</p> <code>None</code> <code>time_of_day</code> <code>str</code> <p>one of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by created hour).</p> <code>None</code> <code>duration</code> <code>int | list[int] | tuple[int]</code> <p>maximum duration in seconds (&lt;= duration). If you want a range, pass a tuple/list (min,max).</p> <code>None</code> <code>exclude_from_location</code> <code>int</code> <p>Drop retrieved data with a distance from the given location.</p> <code>None</code> <code>slice_duration</code> <code>int</code> <p>Split the original sound signal into clips with the given duration.</p> <code>None</code> <code>slice_max_num</code> <code>int</code> <p>Maximum number of clips sliced from the original sound signal.</p> <code>None</code> <code>silent</code> <code>bool</code> <p>If True, do not show error traceback (Default is True).</p> <code>True</code> Source code in <code>urbanworm/dataset.py</code> <pre><code>def get_sound_from_location(self,\n                            id_column: str = None,\n                            distance: int = 50,\n                            key: str = None,\n                            query: str | list[str] = None,\n                            tag: str | list[str] = None,\n                            max_return: int = 1,\n                            year: list | tuple = None,\n                            season: str = None,\n                            time_of_day: str = None,\n                            duration: int = None,\n                            exclude_from_location: int = None,\n                            slice_duration: int = None,\n                            slice_max_num: int = None,\n                            silent: bool = True\n                            ):\n\n    '''\n        get_sound_from_location\n\n        Retrieve geotagged sound recordings from Freesound\n\n        Args:\n            id_column (str, optional): The name of column that has unique identifier (or something similar) for each location.\n            distance (int): radius in meters (converted to km for Freesound geofilt).\n            key (str): Freesound API key. If None, reads env var FREESOUND_API_KEY.\n            query (str, optional): Query string to search for.\n            tag (str | list[str]): tag string or list of tags (used as filters).\n            max_return (int): number of sounds to return (after post-filters).\n            year (int | list): [Y] or (Y,) or (Y1, Y2) inclusive (filters by upload date \"created\").\n            season (str): one of {\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"} (post-filter by created month).\n            time_of_day (str): one of {\"morning\",\"afternoon\",\"evening\",\"night\"} (post-filter by created hour).\n            duration (int | list[int] | tuple[int]): maximum duration in seconds (&lt;= duration). If you want a range, pass a tuple/list (min,max).\n            exclude_from_location (int, optional): Drop retrieved data with a distance from the given location.\n            slice_duration (int, optional): Split the original sound signal into clips with the given duration.\n            slice_max_num (int, optional): Maximum number of clips sliced from the original sound signal.\n            silent (bool): If True, do not show error traceback (Default is True).\n    '''\n\n    self.audios = {\n        'loc_id': [],\n        'id': [],\n        'data': [],\n        'path': [],\n    }\n    self.audio_metadata = None\n\n    if slice_duration is not None:\n        self.audios['slice'] = []\n\n    if id_column is None:\n        id_column = 'loc_id'\n        if id_column not in self.units.columns:\n            self.units[id_column] = [i for i in range(len(self.units))]\n    res_df = None\n    skip_count = 0\n    for index, row in tqdm(self.units.iterrows(), total=len(self.units)):\n        loc_id = row['loc_id']\n        try:\n            output_df = getSound([row.geometry.centroid.x, row.geometry.centroid.y],\n                                 loc_id,\n                                 distance,\n                                 key,\n                                 query,\n                                 tag,\n                                 max_return,\n                                 year,\n                                 season,\n                                 time_of_day,\n                                 duration,\n                                 exclude_from_location,\n                                 slice_duration,\n                                 slice_max_num,\n                                 output_df = True)\n\n            if slice_duration is not None:\n                slice_list = output_df['slice'].tolist()\n                loc_id_list = output_df['loc_id'].tolist()\n                data_list = output_df['preview-hq-mp3'].tolist()\n                id_list = output_df['id'].tolist()\n\n                slice_num = 1\n                if isinstance(slice_list[0][0], list):\n                    slice_num = len(slice_list[0])\n                    flattened_slice_list = [item for sublist in slice_list for item in sublist]\n                if slice_num &gt; 1:\n                    loc_id_list_ = []\n                    data_list_ = []\n                    id_list_ = []\n                    for item in loc_id_list:\n                        loc_id_list_.extend([item] * slice_num)\n                    for item in data_list:\n                        data_list_.extend([item] * slice_num)\n                    for item in id_list:\n                        id_list_.extend([item] * slice_num)\n                    self.audios['loc_id'] += loc_id_list_\n                    self.audios['data'] += data_list_\n                    self.audios['id'] += id_list_\n                    self.audios['slice'] += flattened_slice_list\n                else:\n                    self.audios['loc_id'] += loc_id_list\n                    self.audios['data'] += data_list\n                    self.audios['id'] += id_list\n                    self.audios['slice'] += flattened_slice_list\n            else:\n                self.audios['loc_id'] += output_df['loc_id'].tolist()\n                self.audios['data'] += output_df['preview-hq-mp3'].tolist()\n                self.audios['id'] += output_df['id'].tolist()\n\n            if res_df is None:\n                res_df = output_df\n            else:\n                res_df = pd.concat([res_df, output_df])\n        except Exception as e:\n            if not silent: print(e)\n            skip_count += 1\n            continue\n    self.audio_metadata = res_df\n    if skip_count &gt; 0:\n        print(f'Collect data for {len(self.units) - skip_count} locations and skipped {skip_count} locations due to no data found.')\n    return None\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.get_svi_from_locations","title":"<code>get_svi_from_locations(id_column=None, distance=50, key=None, pano=True, reoriented=True, multi_num=1, interval=1, fov=80, heading=None, pitch=5, height=500, width=700, year=None, season=None, time_of_day='day', silent=True)</code>","text":"<p>get_svi_from_locations</p> <p>Retrieve the closest street view image(s) near each coordinate using the Mapillary API. The street view image will be reoriented to look at the coordinate when <code>reoriented = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>id_column</code> <code>str</code> <p>The name of column that has unique identifier (or something similar) for each location.</p> <code>None</code> <code>distance</code> <code>int</code> <p>The max distance in meters between the centroid and the street view</p> <code>50</code> <code>key</code> <code>str</code> <p>Mapillary API access token.</p> <code>None</code> <code>pano</code> <code>bool</code> <p>Whether to search for pano street view images only. (Default is True)</p> <code>True</code> <code>reoriented</code> <code>bool</code> <p>Whether to reorient and crop street view images. (Default is True)</p> <code>True</code> <code>multi_num</code> <code>int</code> <p>The number of multiple SVIs (Default is 1).</p> <code>1</code> <code>interval</code> <code>int</code> <p>The interval in meters between each SVI (Default is 1).</p> <code>1</code> <code>fov</code> <code>int</code> <p>Field of view in degrees for the perspective image. (Defaults is 80).</p> <code>80</code> <code>heading</code> <code>int</code> <p>Camera heading in degrees. If None, it will be computed based on the house orientation.</p> <code>None</code> <code>pitch</code> <code>int</code> <p>Camera pitch angle. (Default is 10).</p> <code>5</code> <code>height</code> <code>int</code> <p>Height in pixels of the returned image. (Default is 480).</p> <code>500</code> <code>width</code> <code>int</code> <p>Width in pixels of the returned image. (Default is 640).</p> <code>700</code> <code>year</code> <code>list[str]</code> <p>Year of data (start year, end year).</p> <code>None</code> <code>season</code> <code>str</code> <p>Season of data. One of [\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"]</p> <code>None</code> <code>time_of_day</code> <code>str</code> <p>Time of data. One of [\"day\",\"night\"] (Default is 'day')</p> <code>'day'</code> <code>silent</code> <code>bool</code> <p>If True, do not show error traceback (Default is True).</p> <code>True</code> Source code in <code>urbanworm/dataset.py</code> <pre><code>def get_svi_from_locations(self,\n                           id_column:str=None,\n                           distance:int = 50,\n                           key: str = None,\n                           pano: bool = True, reoriented: bool = True,\n                           multi_num: int = 1, interval: int = 1,\n                           fov: int = 80, heading: int = None, pitch: int = 5,\n                           height: int = 500, width: int = 700,\n                           year: list | tuple = None, season: str = None, time_of_day: str = 'day',\n                           silent: bool = True):\n    \"\"\"\n        get_svi_from_locations\n\n        Retrieve the closest street view image(s) near each coordinate using the Mapillary API.\n        The street view image will be reoriented to look at the coordinate when `reoriented = True`.\n\n        Args:\n            id_column (str, optional): The name of column that has unique identifier (or something similar) for each location.\n            distance (int): The max distance in meters between the centroid and the street view\n            key (str): Mapillary API access token.\n            pano (bool): Whether to search for pano street view images only. (Default is True)\n            reoriented (bool): Whether to reorient and crop street view images. (Default is True)\n            multi_num (int): The number of multiple SVIs (Default is 1).\n            interval (int): The interval in meters between each SVI (Default is 1).\n            fov (int): Field of view in degrees for the perspective image. (Defaults is 80).\n            heading (int): Camera heading in degrees. If None, it will be computed based on the house orientation.\n            pitch (int): Camera pitch angle. (Default is 10).\n            height (int): Height in pixels of the returned image. (Default is 480).\n            width (int): Width in pixels of the returned image. (Default is 640).\n            year (list[str], optional): Year of data (start year, end year).\n            season (str, optional): Season of data. One of [\"spring\",\"summer\",\"fall\",\"autumn\",\"winter\"]\n            time_of_day (str, optional): Time of data. One of [\"day\",\"night\"] (Default is 'day')\n            silent (bool): If True, do not show error traceback (Default is True).\n        \"\"\"\n\n    self.svis = {\n        'loc_id': [],\n        'id': [],\n        'data': [],\n        'path': [],\n    }\n    self.svi_metadata = None\n\n    if id_column is None:\n        id_column = 'loc_id'\n        if id_column not in self.units.columns:\n            self.units[id_column] = [i for i in range(len(self.units))]\n    res_df = None\n    skip_count = 0\n    for index, row in tqdm(self.units.iterrows(), total=len(self.units)):\n        loc_id = row[id_column]\n        try:\n            svis, output_df = getSV([row.geometry.centroid.x, row.geometry.centroid.y],\n                                    loc_id,\n                                    distance,\n                                    key,\n                                    pano,\n                                    reoriented,\n                                    multi_num,\n                                    interval,\n                                    fov, heading, pitch,\n                                    height,\n                                    width,\n                                    year,\n                                    season,\n                                    time_of_day,\n                                    silent = silent\n                                    )\n            if svis is None:\n                skip_count += 1\n                continue\n\n            self.svis['data'] += svis\n            self.svis['loc_id'] += output_df['loc_id'].tolist()\n            self.svis['id'] += output_df['id'].tolist()\n\n            if res_df is None:\n                res_df = output_df\n            else:\n                res_df = pd.concat([res_df, output_df])\n        except Exception as e:\n            if not silent: print(f'skipping {[row.geometry.centroid.x, row.geometry.centroid.y]}: {e}')\n            skip_count += 1\n            continue\n    self.svi_metadata = res_df\n    if skip_count &gt; 0:\n        print(f'Collect data for {len(self.units) - skip_count} locations and skipped {skip_count} locations due to no data found.')\n    return None\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.plot_data","title":"<code>plot_data(data=None, export_gdf=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Type of data to download: ['svi', 'audio', 'photo'].</p> <code>None</code> <code>export_gdf</code> <code>bool</code> <p>Export gpd.GeoDataFrame.</p> <code>False</code> Source code in <code>urbanworm/dataset.py</code> <pre><code>def plot_data(self, data:str = None, export_gdf: bool = False) -&gt; None:\n    '''\n\n    Args:\n        data (str): Type of data to download: ['svi', 'audio', 'photo'].\n        export_gdf (bool): Export gpd.GeoDataFrame.\n    '''\n    if data is not None:\n        return None\n\n    if data == 'svi':\n        temp = self.svi_metadata\n        geometry = gpd.points_from_xy(temp['image_lon'], temp['image_lat'])\n        temp['detail'] = temp.apply(\n            lambda row: f'&lt;a href=\"{row[\"url\"]}\"&gt;View image details&lt;/a&gt;',\n            axis=1\n        )\n        gdf = gpd.GeoDataFrame(temp, geometry=geometry, crs=\"EPSG:4326\")\n        popup = [\"id\", \"captured_at\", \"detail\"]\n    elif data == 'photo':\n        temp = self.photo_metadata\n        geometry = gpd.points_from_xy(temp['longitude'], temp['latitude'])\n        temp['detail'] = temp.apply(\n            lambda row: f'&lt;a href=\"{row[\"url\"]}\"&gt;View photo details&lt;/a&gt;',\n            axis=1\n        )\n        gdf = gpd.GeoDataFrame(temp, geometry=geometry, crs=\"EPSG:4326\")\n        popup = [\"id\", \"datetaken\", \"detail\"]\n    elif data == 'audio':\n        geometry = gpd.points_from_xy(self.audio_metadata['longitude'], self.audio_metadata['latitude'])\n        gdf = gpd.GeoDataFrame(self.audio_metadata, geometry=geometry, crs=\"EPSG:4326\")\n        popup = [\"id\", \"datetaken\", \"detail\"]\n    else:\n        raise ValueError('Invalid data type provided. It has to be one of [\"svi\", \"audio\", \"photo\"].')\n\n    self.plot = gdf.explore(\n        popup=popup,\n        color=\"red\",\n        marker_kwds=dict(radius=5, fill=True),\n        tiles=\"CartoDB positron\",\n        name=\"map\",\n    )\n    return gdf if export_gdf else self.plot\n</code></pre>"},{"location":"API_Reference/dataset/#urbanworm.dataset.GeoTaggedData.set_images","title":"<code>set_images(img_type)</code>","text":"<p>set_images</p> <p>Set retrieved street view images or Flickr photos as images dataset</p> <p>Parameters:</p> Name Type Description Default <code>img_type</code> <code>str</code> <p>'photo' or 'svi'</p> required Source code in <code>urbanworm/dataset.py</code> <pre><code>def set_images(self, img_type: str):\n    '''\n        set_images\n\n        Set retrieved street view images or Flickr photos as images dataset\n\n        Args:\n            img_type (str): 'photo' or 'svi'\n    '''\n    if img_type == 'svi':\n        self.images = self.svis\n    elif img_type == 'photo':\n        self.images = self.photos\n    return None\n</code></pre>"},{"location":"API_Reference/inference/","title":"Inference","text":"<p>               Bases: <code>Inference</code></p> <p>Constructor for vision inference using MLLMs with Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>str</code> <p>model checkpoint.</p> <code>None</code> <code>ollama_key</code> <code>str</code> <p>The Ollama API key.</p> <code>None</code> <code>**kwargs</code> <p>image (str|list[str]|tuple[str]), images (list|tuple), data constructor (GeoTaggedData), and schema (dict)</p> <code>{}</code> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>class InferenceOllama(Inference):\n    '''\n    Constructor for vision inference using MLLMs with Ollama.\n\n    Args:\n        llm (str): model checkpoint.\n        ollama_key (str): The Ollama API key.\n        **kwargs: image (str|list[str]|tuple[str]), images (list|tuple), data constructor (GeoTaggedData), and schema (dict)\n    '''\n\n    def __init__(self,\n                 llm: str = None,\n                 ollama_key: str = None,\n                 **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.llm = llm\n        self.skip_errors = True\n        self.ollama_key = ollama_key\n\n    def one_inference(self,\n                      system: str = '',\n                      prompt: str = '',\n                      image: str | list[str] | tuple[str] = None,\n                      audio: str | list[str] | tuple[str] = None,\n                      temp: float = 0.0,\n                      top_k: int = 20.0,\n                      top_p: float = 0.8):\n\n        '''\n        Chat with MLLM model with one image.\n\n        Args:\n            system (str, optional): The system message.\n            prompt (str): The prompt message.\n            image (str | list[str] | tuple[str]): The image path.\n            audio (str | list[str] | tuple[str]): The audio path.\n            temp (float): The temperature value.\n            top_k (int): The top_k value.\n            top_p (float): The top_p value.\n\n        Notes:\n            Ollama currently does not support audio input.\n            The argument `audio` is just a placeholder for the future development.\n\n        Returns:\n            dict: A dictionary includes questions/messages, responses/answers\n        '''\n\n        ollama.pull(self.llm, stream=True)\n        audio_input = False\n        multiImg = False\n        if image is None and audio is not None:\n            image = audio\n            audio_input = True\n        if image is not None:\n            img = image\n        else:\n            img = self.img\n        if isinstance(img, list) or isinstance(img, tuple):\n            if not isinstance(img[0], str):\n                self.logger.warning(\"a list of images can only be a flatten list\")\n            multiImg = True\n        else:\n            img = [img]\n\n        schema = create_format(self.schema)\n\n        dic = {'responses': [], 'data': []}\n        r = self._mtmd(model=self.llm,\n                       system=system, prompt=prompt,\n                       img=img,\n                       temp=temp, top_k=top_k, top_p=top_p,\n                       schema=schema,\n                       one_shot_lr=[],\n                       multiImgInput=multiImg)\n        dic['responses'] += [r.responses]\n        dic['data'] += [img]\n        return response2df(dic)\n\n    def batch_inference(self,\n                        system: str = '',\n                        prompt: str = '',\n                        temp: float = 0.0,\n                        top_k: int = 20,\n                        top_p: float = 0.8,\n                        disableProgressBar: bool = False) -&gt; dict:\n        '''\n        Chat with MLLM model for each image.\n\n        Args:\n            system (str, optinal): The system message.\n            prompt (str): The prompt message.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            disableProgressBar (bool): The progress bar for showing the progress of data analysis over the units.\n\n        Returns:\n            list A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)\n        '''\n\n        ollama.pull(self.llm, stream=True)\n        dic = {'responses': [], 'data': []}\n\n        if self.batch_images is not None:\n            imgs = self.batch_images\n        else:\n            imgs = self.imgs\n\n        schema = create_format(self.schema)\n\n        multiImgInput = False\n        if isinstance(imgs[0], list) or isinstance(imgs[0], tuple):\n            multiImgInput = True\n\n        for i in tqdm(range(len(imgs)), desc=\"Processing...\", ncols=75, disable=disableProgressBar):\n            img = imgs[i]\n            try:\n                r = self._mtmd(model=self.llm,\n                               system=system, prompt=prompt,\n                               img=img if multiImgInput else [img],\n                               temp=temp, top_k=top_k, top_p=top_p,\n                               schema=schema,\n                               one_shot_lr=[],\n                               multiImgInput=multiImgInput)\n                rr = r.responses\n            except Exception as e:\n                # Log and continue; capture an error stub so downstream stays consistent\n                self.logger.warning(\"batch_inference: image %d failed (%s). Continuing.\", i, e)\n                rr = {'error': str(e), 'data': None}\n\n            dic['responses'] += [rr]\n            dic['data'] += [imgs[i]]\n        self.results = dic\n        return self.to_df(output=True)\n\n    def to_df(self, output: bool = True) -&gt; pd.DataFrame | str:\n        \"\"\"\n        Convert the output from an MLLM reponse (from .batch_inference) into a DataFrame.\n\n        Args:\n            output (bool): Whether to return a DataFrame. Defaults to True.\n        Returns:\n            pd.DataFrame: A DataFrame containing responses and associated metadata.\n            str: An error message if `.batch_inference()` has not been run or if the format is unsupported.\n        \"\"\"\n\n        if self.results is not None:\n            self.df = response2df(self.results)\n            if output:\n                return self.df\n        return None\n\n    def _mtmd(self, model: str = None, system: str = None, prompt: str = None,\n              img: list[str] = None, temp: float = None, top_k: float = None, top_p: float = None,\n              schema = None,\n              one_shot_lr: list | tuple = [], multiImgInput: bool = False, audio_input: bool = False):\n\n        if prompt is not None and img is not None:\n            if len(img) == 1:\n                return self._customized_chat(model, system, prompt, img[0], temp, top_k, top_p, schema, one_shot_lr)\n            elif len(img) &gt;= 2:\n                system = f'You are analyzing aerial or street view images. For street view, you should just focus on the building and yard in the middle. {system}'\n                if multiImgInput:\n                    return self._customized_chat(model, system, prompt, img, temp, top_k, top_p, schema, one_shot_lr)\n                else:\n                    res = []\n\n                    for i in range(len(img)):\n                        r = self._customized_chat(model, system, prompt, img, temp, top_k, top_p, schema, one_shot_lr)\n                        res += [r.responses]\n                    return res\n            return None\n        else:\n            raise Exception(\"Prompt or image(s) is missing.\")\n\n    def _customized_chat(self, model: str = None,\n                         system: str = None, prompt: str = None, img: str | list | tuple = None,\n                         temp: float = None, top_k: float = None, top_p: float = None,\n                         schema=None,\n                         one_shot_lr: list = [],\n                         audio_input: bool = False) -&gt; Response:\n\n        if isinstance(one_shot_lr, list):\n            if len(one_shot_lr) &gt; 0:\n                if not isinstance(one_shot_lr[0], dict):\n                    raise Exception(\"Please provide a list of dictionaries.\")\n\n        if img is not None:\n            if isinstance(img, str):\n                messages = [\n                               {\n                                   'role': 'system',\n                                   'content': system\n                               }] + one_shot_lr + [\n                               {\n                                   'role': 'user',\n                                   'content': prompt,\n                                   'images': [img]\n                               }\n                           ]\n            elif isinstance(img, list) or isinstance(img, tuple):\n                th = ['st', 'nd', 'rd', 'th']\n                img_messages = [{'role': 'system', 'content': system}] + one_shot_lr + [\n                    {'role': 'user', 'content': f'{i + 1}{th[i] if i &lt; 3 else th[3]} image', 'images': [img[i]]} for i\n                    in range(len(img))]\n                messages = img_messages + [\n                    {\n                        'role': 'user',\n                        'content': 'You have to answer all questions based on all given images\\n' + prompt,\n                    }\n                ]\n        else:\n            messages = [\n                           {\n                               'role': 'system',\n                               'content': system\n                           }] + one_shot_lr + [\n                           {\n                               'role': 'user',\n                               'content': prompt,\n                           }\n                       ]\n\n        if (self.ollama_key is not None) and (self.ollama_key != ''):\n            client = Client(\n                host=\"https://ollama.com\",\n                headers={'Authorization': 'Bearer ' + self.ollama_key},\n            )\n            res = client.chat(\n                model=model,\n                format=schema.model_json_schema(),\n                messages=messages,\n                options={\n                    \"temperature\": temp,\n                    \"top_k\": top_k,\n                    \"top_p\": top_p\n                }\n            )\n        else:\n            res = ollama.chat(\n                model=model,\n                format=schema.model_json_schema(),\n                messages=messages,\n                options={\n                    \"temperature\": temp,\n                    \"top_k\": top_k,\n                    \"top_p\": top_p\n                }\n            )\n\n        raw_text = res.message.content\n        try:\n            return schema.model_validate_json(raw_text)\n        except Exception:\n            if self.skip_errors:\n                raise\n            else:\n                pass\n\n        repaired = sanitize_json_text(str(raw_text))\n        try:\n            return schema.model_validate_json(repaired)\n        except Exception:\n            pass\n\n        extracted = extract_json_from_text(repaired) or repaired\n        try:\n            return schema.model_validate_json(extracted)\n        except Exception:\n            raise\n</code></pre> <p>               Bases: <code>Inference</code></p> <p>Constructor for vision inference using MLLMs with llama.cpp</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>str</code> <p>model checkpoint to download (e.g. ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0) or</p> <code>None</code> <code>mp</code> <code>str</code> <p>If <code>llm</code> is provided as a local path to model file (.gguf),</p> <code>None</code> <code>**kwargs</code> <p>image (str|list[str]|tuple[str]), images (list|tuple), data constructor (GeoTaggedData), and schema (dict)</p> <code>{}</code> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>class InferenceLlamacpp(Inference):\n    '''\n    Constructor for vision inference using MLLMs with llama.cpp\n\n    Args:\n        llm (str, optional): model checkpoint to download (e.g. ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0) or\n        a local path to model file (.gguf)\n        mp (str, optional): If `llm` is provided as a local path to model file (.gguf),\n        `mp` has to be provided as a local path to multimodal projector file (*mproj*.gguf).\n        **kwargs: image (str|list[str]|tuple[str]), images (list|tuple), data constructor (GeoTaggedData), and schema (dict)\n    '''\n\n    def __init__(self, llm:str = None, mp:str = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.llm = llm\n        self.mp = mp\n\n    def one_inference(self,\n                      system: str = '',\n                      prompt: str = '',\n                      image: str | list | tuple = None,\n                      audio: str | list | tuple = None,\n                      temp: float = 0.2,\n                      top_k: int = 20,\n                      top_p: float = 0.8,\n                      ctx_size: int = 4096,\n                      audio_input: bool = False\n                      ) -&gt; Any:\n        '''\n            Chat with MLLM model with one image.\n            Args:\n                 system (str, optional): The system message.\n                 prompt (str): The prompt message.\n                 image (str | list | tuple, optional): The image path.\n                 audio (str | list | tuple, optional): The audio path.\n                 temp (float): The temperature value.\n                 top_k (int): The top_k value.\n                 top_p (float): The top_p value.\n                 ctx_size (int): Size of context (The default is 4096)\n                 audio_input (bool, optional): Whether to run inference with audio input\n\n            Returns: response from MLLM as a dataframe\n        '''\n\n        llm = self.llm\n        mp = self.mp\n\n        if not audio_input:\n            if image is not None:\n                im = [image] if isinstance(image, str) else image\n            else:\n                im = [self.img] if isinstance(self.img, str) else self.img\n\n        else:\n            if audio is not None:\n                im = [audio] if isinstance(audio, str) else audio\n            else:\n                im = [self.audio] if isinstance(self.audio, str) else self.audio\n\n        if isinstance(im, list) or isinstance(im, tuple):\n            if not isinstance(im[0], str):\n                self.logger.warning(\"a list of images can only be a flatten list\")\n                return None\n\n        # ims_origin = None\n        im_ = []\n        if not audio_input:\n            for i in im:\n                if is_base64(i):\n                    temp = base64img2temp(i)\n                    im_ += [temp]\n                elif is_url(i):\n                    temp = url2temp(i)\n                    im_ += [temp]\n                else:\n                    pass\n        else:\n            for i in range(len(im)):\n                if is_url(im[i]):\n                    temp = sound_url_to_temp(im[i])\n                    im_ += [temp]\n                else:\n                    pass\n\n        if len(im_) == len(im):\n            # ims_origin = im\n            im = im_\n\n        if llm is None:\n            self.logger.warning(\"model cannot be None\")\n            return None\n\n        schema = create_format(self.schema)\n\n        r = self._mtmd(llm, mp,\n                       system,\n                       prompt,\n                       im,\n                       temperature=temp,\n                       top_k=top_k,\n                       top_p=top_p,\n                       ctx_size=ctx_size,\n                       schema=schema,\n                       audio_input=audio_input)\n        r = extract_last_json(r)\n        r = pd.DataFrame(r['responses'])\n        df = responses_to_wide_all_columns(r)\n        # df['data'] = ''\n        # df.loc[0, 'data'] = im\n        if len(im_) &gt;= 1:\n            for each in im_:\n                try:\n                    os.remove(each)\n                except:\n                    pass\n        return df\n\n    def batch_inference(self,\n                        system: str = '',\n                        prompt: str = '',\n                        temp: float = 0.2,\n                        top_k: int = 20,\n                        top_p: float = 0.8,\n                        ctx_size: int = 4096,\n                        audio_input = False):\n        '''\n            Chat with MLLM model for each image in a list.\n            Args:\n                system (str, optional): The system message.\n                prompt (str): The prompt message.\n                temp (float): The temperature value.\n                top_k (float): The top_k value.\n                top_p (float): The top_p value.\n                ctx_size (int): Size of context (The default is 4096)\n                audio_input (bool): False\n            Returns: response from MLLM as a dataframe\n        '''\n\n        dic = {'responses': [], 'data': []}\n        llm = self.llm\n        mp = self.mp\n        clips = None\n        if not audio_input:\n            if self.batch_images is not None:\n                imgs = self.batch_images\n            else:\n                imgs = self.imgs\n        else:\n            if self.batch_audios is not None:\n                imgs = self.batch_audios\n                clips = self.batch_audios_slice\n            else:\n                imgs = self.audios\n\n        schema = create_format(self.schema)\n\n        for i in tqdm(range(len(imgs)), desc=\"Processing...\", ncols=75, disable=kwargs.get('disableProgressBar', False)):\n            ims = [imgs[i]] if isinstance(imgs[i], str) else imgs[i]\n\n            ims_origin = None\n            ims_ = []\n            if not audio_input:\n                for im in ims:\n                    if is_base64(im):\n                        temp = base64img2temp(im)\n                        ims_ += [temp]\n                    elif is_url(im):\n                        temp = url2temp(im)\n                        ims_ += [temp]\n                    else:\n                        pass\n            else:\n                for j in range(len(ims)):\n                    im = ims[j]\n                    if is_url(im):\n                        if clips is not None:\n                            clip = clips[j]\n                            temp = sound_url_to_temp(im, clip)\n                            ims_ += [temp]\n                        else:\n                            temp = sound_url_to_temp(im)\n                            ims_ += [temp]\n                    else:\n                        pass\n\n            if len(ims_) == len(ims):\n                ims_origin = ims\n                ims = ims_\n\n            try:\n                r = None\n                try_times = 0\n                while r is None and try_times &lt;= 5:\n                    r = self._mtmd(llm,\n                                   mp,\n                                   system,\n                                   prompt,\n                                   ims,\n                                   temperature=temp,\n                                   top_k=top_k,\n                                   top_p=top_p,\n                                   ctx_size=ctx_size,\n                                   schema=schema,\n                                   audio_input = audio_input)\n                    r = extract_last_json(r)\n                    try_times += 1\n\n                if r is None:\n                    r = 'Bad response'\n                dic['responses'] += [r]\n                dic['data'] += [ims] if ims_origin is None else [ims_origin]\n\n                if len(ims_) &gt;= 1:\n                    for each in ims_:\n                        try:\n                            os.remove(each)\n                        except:\n                            pass\n            except Exception as e:\n                print(e)\n                pass\n\n        self.results = dic\n        return self.to_df(output=True)\n\n    def to_df(self, output: bool = True) -&gt; Any:\n        \"\"\"\n            Convert the output from an MLLM reponse (from .batch_inference) into a DataFrame.\n\n            Args:\n                output (bool): Whether to return a DataFrame. Defaults to True.\n            Returns:\n                pd.DataFrame: A DataFrame containing responses and associated metadata.\n        \"\"\"\n\n        if self.results is not None:\n            df_list = []\n            responses = self.results['responses']\n            imgs = self.results['data']\n\n            for inx in range(len(responses)):\n                r = responses[inx]\n                i = imgs[inx]\n\n                r = pd.DataFrame(r['responses'])\n                r = responses_to_wide_all_columns(r)\n                for j in range(len(i)):\n                    r[f'data_{j + 1}'] = i[j]\n\n                df_list += [r]\n            self.df = pd.concat(df_list, ignore_index=True)\n            if output:\n                return self.df\n            return None\n        else:\n            return None\n\n    def _mtmd(self,\n              llm: str = None,\n              mp: str = None,\n              system_message: str = '',\n              prompt: str = '',\n              imgs: list = None,\n              temperature: float = 0.2,\n              top_k: int = 40,\n              top_p: float = 0.9,\n              ctx_size:int = 4096,\n              # threads:int = -1,\n              # batch_size:int = 512,\n              # gpu_layers:int = -1,\n              schema = None,\n              audio_input = False):\n        '''\n\n        Args:\n            llm (str): model path\n            mp (str):\n            system_message (str, optional):\n            prompt (str): prompt to start generation with\n            imgs (list): list of image paths\n            temperature (float): temperature (default: 0.2)\n            top_k (float): top-k sampling (default: 40, 0 = disabled)\n            top_p (float): top-p sampling (default: 0.9, 1.0 = disabled)\n            ctx_size (int): size of the prompt context (default: 4096, 0 = loaded from model)\n        '''\n        if imgs is not None:\n            imgs = [Path(img) for img in imgs]\n            imgs = [[\"--image\" if not audio_input else \"--audio\", str(i)] for i in imgs]\n            imgs = [item for sublist in imgs for item in sublist]\n\n        cmd = [\"llama-mtmd-cli\",\n               \"-p\", system_message + prompt\n        ]\n\n        if mp is not None:\n            lm = Path(llm)\n            mp = Path(mp)\n            cmd = cmd + [\"-m\", str(lm), \"--mmproj\", str(mp)]\n        else:\n            cmd = cmd + [\"-hf\", str(llm)]\n\n        if imgs is not None:\n            cmd = cmd + imgs\n\n        if schema is not None:\n            cmd = cmd + [\"-j\", schema_json(schema, inline_refs=True)]\n\n        cmd = cmd + [\"--temp\", f\"{temperature}\",\n                     \"--top-k\", f\"{top_k}\",\n                     \"--top-p\", f\"{top_p}\",\n                     \"-c\", f\"{ctx_size}\",\n                     # \"-t\", f\"{threads}\",\n                     # \"-ub\", f\"{batch_size}\",\n                     # \"-ngl\", f\"{gpu_layers}\"\n                     ]\n\n        try:\n            res = subprocess.run(cmd, check=True, text=True, capture_output=True)\n        except subprocess.CalledProcessError as e:\n            print(\"===== STDERR =====\")\n            print(e.stderr)\n            print(\"===== STDOUT =====\")\n            print(e.stdout)\n            print(\"Return code:\", e.returncode)\n            print(\"Command:\", e.cmd)\n            raise\n        raw = res.stdout\n        return raw\n</code></pre>"},{"location":"API_Reference/inference/#urbanworm.inference.llama.InferenceOllama.batch_inference","title":"<code>batch_inference(system='', prompt='', temp=0.0, top_k=20, top_p=0.8, disableProgressBar=False)</code>","text":"<p>Chat with MLLM model for each image.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>(str, optinal)</code> <p>The system message.</p> <code>''</code> <code>prompt</code> <code>str</code> <p>The prompt message.</p> <code>''</code> <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>20</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> <code>disableProgressBar</code> <code>bool</code> <p>The progress bar for showing the progress of data analysis over the units.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>list A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)</p> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>def batch_inference(self,\n                    system: str = '',\n                    prompt: str = '',\n                    temp: float = 0.0,\n                    top_k: int = 20,\n                    top_p: float = 0.8,\n                    disableProgressBar: bool = False) -&gt; dict:\n    '''\n    Chat with MLLM model for each image.\n\n    Args:\n        system (str, optinal): The system message.\n        prompt (str): The prompt message.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n        disableProgressBar (bool): The progress bar for showing the progress of data analysis over the units.\n\n    Returns:\n        list A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)\n    '''\n\n    ollama.pull(self.llm, stream=True)\n    dic = {'responses': [], 'data': []}\n\n    if self.batch_images is not None:\n        imgs = self.batch_images\n    else:\n        imgs = self.imgs\n\n    schema = create_format(self.schema)\n\n    multiImgInput = False\n    if isinstance(imgs[0], list) or isinstance(imgs[0], tuple):\n        multiImgInput = True\n\n    for i in tqdm(range(len(imgs)), desc=\"Processing...\", ncols=75, disable=disableProgressBar):\n        img = imgs[i]\n        try:\n            r = self._mtmd(model=self.llm,\n                           system=system, prompt=prompt,\n                           img=img if multiImgInput else [img],\n                           temp=temp, top_k=top_k, top_p=top_p,\n                           schema=schema,\n                           one_shot_lr=[],\n                           multiImgInput=multiImgInput)\n            rr = r.responses\n        except Exception as e:\n            # Log and continue; capture an error stub so downstream stays consistent\n            self.logger.warning(\"batch_inference: image %d failed (%s). Continuing.\", i, e)\n            rr = {'error': str(e), 'data': None}\n\n        dic['responses'] += [rr]\n        dic['data'] += [imgs[i]]\n    self.results = dic\n    return self.to_df(output=True)\n</code></pre>"},{"location":"API_Reference/inference/#urbanworm.inference.llama.InferenceOllama.one_inference","title":"<code>one_inference(system='', prompt='', image=None, audio=None, temp=0.0, top_k=20.0, top_p=0.8)</code>","text":"<p>Chat with MLLM model with one image.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>The system message.</p> <code>''</code> <code>prompt</code> <code>str</code> <p>The prompt message.</p> <code>''</code> <code>image</code> <code>str | list[str] | tuple[str]</code> <p>The image path.</p> <code>None</code> <code>audio</code> <code>str | list[str] | tuple[str]</code> <p>The audio path.</p> <code>None</code> <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>int</code> <p>The top_k value.</p> <code>20.0</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> Notes <p>Ollama currently does not support audio input. The argument <code>audio</code> is just a placeholder for the future development.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary includes questions/messages, responses/answers</p> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>def one_inference(self,\n                  system: str = '',\n                  prompt: str = '',\n                  image: str | list[str] | tuple[str] = None,\n                  audio: str | list[str] | tuple[str] = None,\n                  temp: float = 0.0,\n                  top_k: int = 20.0,\n                  top_p: float = 0.8):\n\n    '''\n    Chat with MLLM model with one image.\n\n    Args:\n        system (str, optional): The system message.\n        prompt (str): The prompt message.\n        image (str | list[str] | tuple[str]): The image path.\n        audio (str | list[str] | tuple[str]): The audio path.\n        temp (float): The temperature value.\n        top_k (int): The top_k value.\n        top_p (float): The top_p value.\n\n    Notes:\n        Ollama currently does not support audio input.\n        The argument `audio` is just a placeholder for the future development.\n\n    Returns:\n        dict: A dictionary includes questions/messages, responses/answers\n    '''\n\n    ollama.pull(self.llm, stream=True)\n    audio_input = False\n    multiImg = False\n    if image is None and audio is not None:\n        image = audio\n        audio_input = True\n    if image is not None:\n        img = image\n    else:\n        img = self.img\n    if isinstance(img, list) or isinstance(img, tuple):\n        if not isinstance(img[0], str):\n            self.logger.warning(\"a list of images can only be a flatten list\")\n        multiImg = True\n    else:\n        img = [img]\n\n    schema = create_format(self.schema)\n\n    dic = {'responses': [], 'data': []}\n    r = self._mtmd(model=self.llm,\n                   system=system, prompt=prompt,\n                   img=img,\n                   temp=temp, top_k=top_k, top_p=top_p,\n                   schema=schema,\n                   one_shot_lr=[],\n                   multiImgInput=multiImg)\n    dic['responses'] += [r.responses]\n    dic['data'] += [img]\n    return response2df(dic)\n</code></pre>"},{"location":"API_Reference/inference/#urbanworm.inference.llama.InferenceOllama.to_df","title":"<code>to_df(output=True)</code>","text":"<p>Convert the output from an MLLM reponse (from .batch_inference) into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>bool</code> <p>Whether to return a DataFrame. Defaults to True.</p> <code>True</code> <p>Returns:     pd.DataFrame: A DataFrame containing responses and associated metadata.     str: An error message if <code>.batch_inference()</code> has not been run or if the format is unsupported.</p> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>def to_df(self, output: bool = True) -&gt; pd.DataFrame | str:\n    \"\"\"\n    Convert the output from an MLLM reponse (from .batch_inference) into a DataFrame.\n\n    Args:\n        output (bool): Whether to return a DataFrame. Defaults to True.\n    Returns:\n        pd.DataFrame: A DataFrame containing responses and associated metadata.\n        str: An error message if `.batch_inference()` has not been run or if the format is unsupported.\n    \"\"\"\n\n    if self.results is not None:\n        self.df = response2df(self.results)\n        if output:\n            return self.df\n    return None\n</code></pre>"},{"location":"API_Reference/inference/#urbanworm.inference.llama.InferenceLlamacpp.batch_inference","title":"<code>batch_inference(system='', prompt='', temp=0.2, top_k=20, top_p=0.8, ctx_size=4096, audio_input=False)</code>","text":"<p>Chat with MLLM model for each image in a list. Args:     system (str, optional): The system message.     prompt (str): The prompt message.     temp (float): The temperature value.     top_k (float): The top_k value.     top_p (float): The top_p value.     ctx_size (int): Size of context (The default is 4096)     audio_input (bool): False Returns: response from MLLM as a dataframe</p> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>def batch_inference(self,\n                    system: str = '',\n                    prompt: str = '',\n                    temp: float = 0.2,\n                    top_k: int = 20,\n                    top_p: float = 0.8,\n                    ctx_size: int = 4096,\n                    audio_input = False):\n    '''\n        Chat with MLLM model for each image in a list.\n        Args:\n            system (str, optional): The system message.\n            prompt (str): The prompt message.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            ctx_size (int): Size of context (The default is 4096)\n            audio_input (bool): False\n        Returns: response from MLLM as a dataframe\n    '''\n\n    dic = {'responses': [], 'data': []}\n    llm = self.llm\n    mp = self.mp\n    clips = None\n    if not audio_input:\n        if self.batch_images is not None:\n            imgs = self.batch_images\n        else:\n            imgs = self.imgs\n    else:\n        if self.batch_audios is not None:\n            imgs = self.batch_audios\n            clips = self.batch_audios_slice\n        else:\n            imgs = self.audios\n\n    schema = create_format(self.schema)\n\n    for i in tqdm(range(len(imgs)), desc=\"Processing...\", ncols=75, disable=kwargs.get('disableProgressBar', False)):\n        ims = [imgs[i]] if isinstance(imgs[i], str) else imgs[i]\n\n        ims_origin = None\n        ims_ = []\n        if not audio_input:\n            for im in ims:\n                if is_base64(im):\n                    temp = base64img2temp(im)\n                    ims_ += [temp]\n                elif is_url(im):\n                    temp = url2temp(im)\n                    ims_ += [temp]\n                else:\n                    pass\n        else:\n            for j in range(len(ims)):\n                im = ims[j]\n                if is_url(im):\n                    if clips is not None:\n                        clip = clips[j]\n                        temp = sound_url_to_temp(im, clip)\n                        ims_ += [temp]\n                    else:\n                        temp = sound_url_to_temp(im)\n                        ims_ += [temp]\n                else:\n                    pass\n\n        if len(ims_) == len(ims):\n            ims_origin = ims\n            ims = ims_\n\n        try:\n            r = None\n            try_times = 0\n            while r is None and try_times &lt;= 5:\n                r = self._mtmd(llm,\n                               mp,\n                               system,\n                               prompt,\n                               ims,\n                               temperature=temp,\n                               top_k=top_k,\n                               top_p=top_p,\n                               ctx_size=ctx_size,\n                               schema=schema,\n                               audio_input = audio_input)\n                r = extract_last_json(r)\n                try_times += 1\n\n            if r is None:\n                r = 'Bad response'\n            dic['responses'] += [r]\n            dic['data'] += [ims] if ims_origin is None else [ims_origin]\n\n            if len(ims_) &gt;= 1:\n                for each in ims_:\n                    try:\n                        os.remove(each)\n                    except:\n                        pass\n        except Exception as e:\n            print(e)\n            pass\n\n    self.results = dic\n    return self.to_df(output=True)\n</code></pre>"},{"location":"API_Reference/inference/#urbanworm.inference.llama.InferenceLlamacpp.one_inference","title":"<code>one_inference(system='', prompt='', image=None, audio=None, temp=0.2, top_k=20, top_p=0.8, ctx_size=4096, audio_input=False)</code>","text":"<p>Chat with MLLM model with one image. Args:      system (str, optional): The system message.      prompt (str): The prompt message.      image (str | list | tuple, optional): The image path.      audio (str | list | tuple, optional): The audio path.      temp (float): The temperature value.      top_k (int): The top_k value.      top_p (float): The top_p value.      ctx_size (int): Size of context (The default is 4096)      audio_input (bool, optional): Whether to run inference with audio input</p> <p>Returns: response from MLLM as a dataframe</p> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>def one_inference(self,\n                  system: str = '',\n                  prompt: str = '',\n                  image: str | list | tuple = None,\n                  audio: str | list | tuple = None,\n                  temp: float = 0.2,\n                  top_k: int = 20,\n                  top_p: float = 0.8,\n                  ctx_size: int = 4096,\n                  audio_input: bool = False\n                  ) -&gt; Any:\n    '''\n        Chat with MLLM model with one image.\n        Args:\n             system (str, optional): The system message.\n             prompt (str): The prompt message.\n             image (str | list | tuple, optional): The image path.\n             audio (str | list | tuple, optional): The audio path.\n             temp (float): The temperature value.\n             top_k (int): The top_k value.\n             top_p (float): The top_p value.\n             ctx_size (int): Size of context (The default is 4096)\n             audio_input (bool, optional): Whether to run inference with audio input\n\n        Returns: response from MLLM as a dataframe\n    '''\n\n    llm = self.llm\n    mp = self.mp\n\n    if not audio_input:\n        if image is not None:\n            im = [image] if isinstance(image, str) else image\n        else:\n            im = [self.img] if isinstance(self.img, str) else self.img\n\n    else:\n        if audio is not None:\n            im = [audio] if isinstance(audio, str) else audio\n        else:\n            im = [self.audio] if isinstance(self.audio, str) else self.audio\n\n    if isinstance(im, list) or isinstance(im, tuple):\n        if not isinstance(im[0], str):\n            self.logger.warning(\"a list of images can only be a flatten list\")\n            return None\n\n    # ims_origin = None\n    im_ = []\n    if not audio_input:\n        for i in im:\n            if is_base64(i):\n                temp = base64img2temp(i)\n                im_ += [temp]\n            elif is_url(i):\n                temp = url2temp(i)\n                im_ += [temp]\n            else:\n                pass\n    else:\n        for i in range(len(im)):\n            if is_url(im[i]):\n                temp = sound_url_to_temp(im[i])\n                im_ += [temp]\n            else:\n                pass\n\n    if len(im_) == len(im):\n        # ims_origin = im\n        im = im_\n\n    if llm is None:\n        self.logger.warning(\"model cannot be None\")\n        return None\n\n    schema = create_format(self.schema)\n\n    r = self._mtmd(llm, mp,\n                   system,\n                   prompt,\n                   im,\n                   temperature=temp,\n                   top_k=top_k,\n                   top_p=top_p,\n                   ctx_size=ctx_size,\n                   schema=schema,\n                   audio_input=audio_input)\n    r = extract_last_json(r)\n    r = pd.DataFrame(r['responses'])\n    df = responses_to_wide_all_columns(r)\n    # df['data'] = ''\n    # df.loc[0, 'data'] = im\n    if len(im_) &gt;= 1:\n        for each in im_:\n            try:\n                os.remove(each)\n            except:\n                pass\n    return df\n</code></pre>"},{"location":"API_Reference/inference/#urbanworm.inference.llama.InferenceLlamacpp.to_df","title":"<code>to_df(output=True)</code>","text":"<p>Convert the output from an MLLM reponse (from .batch_inference) into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>bool</code> <p>Whether to return a DataFrame. Defaults to True.</p> <code>True</code> <p>Returns:     pd.DataFrame: A DataFrame containing responses and associated metadata.</p> Source code in <code>urbanworm/inference/llama.py</code> <pre><code>def to_df(self, output: bool = True) -&gt; Any:\n    \"\"\"\n        Convert the output from an MLLM reponse (from .batch_inference) into a DataFrame.\n\n        Args:\n            output (bool): Whether to return a DataFrame. Defaults to True.\n        Returns:\n            pd.DataFrame: A DataFrame containing responses and associated metadata.\n    \"\"\"\n\n    if self.results is not None:\n        df_list = []\n        responses = self.results['responses']\n        imgs = self.results['data']\n\n        for inx in range(len(responses)):\n            r = responses[inx]\n            i = imgs[inx]\n\n            r = pd.DataFrame(r['responses'])\n            r = responses_to_wide_all_columns(r)\n            for j in range(len(i)):\n                r[f'data_{j + 1}'] = i[j]\n\n            df_list += [r]\n        self.df = pd.concat(df_list, ignore_index=True)\n        if output:\n            return self.df\n        return None\n    else:\n        return None\n</code></pre>"},{"location":"Examples/1_basic_inference/","title":"1 basic inference","text":"<p>p# Basic usage of inference module for image-based inference In this tutorial, we will be using the <code>inference</code> module from <code>urban-worm</code>, which supports three frameworks to run MLLMs: Ollama (built on top of llama.cpp) and Llama.cpp to showcase inference with single and multiple images with InternVL3.</p> <p>Three type of output schema will be demonstrated for inference:</p> <ul> <li>plain text generation</li> <li>multiple questions with binary answers</li> <li>multiple choices</li> </ul> In\u00a0[1]: Copied! <pre>from urbanworm.inference.llama import InferenceLlamacpp, InferenceOllama\n</pre> from urbanworm.inference.llama import InferenceLlamacpp, InferenceOllama <p>First, let's set up some schema for defining output format and prompts for demonstrating inference tasks.</p> In\u00a0[2]: Copied! <pre># define the schema for model output\n\n# this the default built-in schema for plain text generation\nnormal_format = {\n    \"questions\": (str, ...),\n    \"answer\": (str, ...),\n}\n\n# binary answer\nbool_format = {\n    \"questions\": (str, ...),\n    \"answer\": (bool, ...),\n}\n\n# multiple choice\nfrom typing import Literal\nmultiple_choice_format = {\n    \"questions\": (str, ...),\n    \"answer\": (Literal['occupied', 'unoccupied'], ...),\n    \"explanation\": (str, ...),\n}\n\n# define the inference task and emphasize the output format in the prompt\nmulti_questions_prompt =  '''\n    Question 1 - Is there any damage on the roof?\n    Question 2 - Is any window broken or boarded?\n    Question 3 - Is any door broken, missing, or boarded?\n\n    For each question, you have to respond in the following format:\n    yes (true) / no (false)\n'''\n\nmulti_choice_prompt = '''\n    Does the house look occupied?\n    For each question, you have to respond in the following format:\n    'occupied' / 'unoccupied'\n'''\n</pre> # define the schema for model output  # this the default built-in schema for plain text generation normal_format = {     \"questions\": (str, ...),     \"answer\": (str, ...), }  # binary answer bool_format = {     \"questions\": (str, ...),     \"answer\": (bool, ...), }  # multiple choice from typing import Literal multiple_choice_format = {     \"questions\": (str, ...),     \"answer\": (Literal['occupied', 'unoccupied'], ...),     \"explanation\": (str, ...), }  # define the inference task and emphasize the output format in the prompt multi_questions_prompt =  '''     Question 1 - Is there any damage on the roof?     Question 2 - Is any window broken or boarded?     Question 3 - Is any door broken, missing, or boarded?      For each question, you have to respond in the following format:     yes (true) / no (false) '''  multi_choice_prompt = '''     Does the house look occupied?     For each question, you have to respond in the following format:     'occupied' / 'unoccupied' ''' <p>We will be using three street views that capture a single residential property from different angles:</p> In\u00a0[3]: Copied! <pre># build constructor\n# All these three images in constructor will be used together for a single inference\ndata = InferenceOllama(llm='hf.co/ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0',\n                       image=[\"./data/img_1.jpg\",\n                              \"./data/img_2.jpg\",\n                              \"./data/img_3.jpg\",],\n                       schema=normal_format)\n# inference\nresult = data.one_inference(prompt='what is the color of the house?')\nresult\n</pre> # build constructor # All these three images in constructor will be used together for a single inference data = InferenceOllama(llm='hf.co/ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0',                        image=[\"./data/img_1.jpg\",                               \"./data/img_2.jpg\",                               \"./data/img_3.jpg\",],                        schema=normal_format) # inference result = data.one_inference(prompt='what is the color of the house?') result Out[3]: questions1 answer1 data 0 What is the color of the house? The house in each image appears to be light-co... [./data/img_1.jpg, ./data/img_2.jpg, ./data/im... In\u00a0[4]: Copied! <pre>result['answer1'][0]\n</pre> result['answer1'][0] Out[4]: <pre>\"The images depict a two-story house with white siding and multiple windows. The yard appears to be fenced, and there's an assortment of items near the entrance such as trash bins and possibly gardening tools. There is also a sidewalk leading up to the front door.\"</pre> In\u00a0[5]: Copied! <pre># image can also be provided for a single inference\ndata.schema = bool_format # replace the output format\nresult = data.one_inference(prompt=multi_questions_prompt,\n                            image=\"./data/img_1.jpg\")\nresult\n</pre> # image can also be provided for a single inference data.schema = bool_format # replace the output format result = data.one_inference(prompt=multi_questions_prompt,                             image=\"./data/img_1.jpg\") result Out[5]: questions1 answer1 questions2 answer2 questions3 answer3 data 0 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? True [./data/img_1.jpg] In\u00a0[16]: Copied! <pre># multiple choice\ndata.schema = multiple_choice_format # replace the output format\nresult = data.one_inference(prompt=multi_choice_prompt,\n                            image=\"./data/img_1.jpg\")\nresult\n</pre> # multiple choice data.schema = multiple_choice_format # replace the output format result = data.one_inference(prompt=multi_choice_prompt,                             image=\"./data/img_1.jpg\") result Out[16]: questions1 answer1 explanation1 data 0 Does the house look occupied? unoccupied The porch area appears empty and there are no ... [./data/img_1.jpg] In\u00a0[10]: Copied! <pre># build constructor\ndata = InferenceLlamacpp(\n    # if model amd mmproj are already downloaded,\n    # you can directly specify the path to model files in the constructor, for example:\n    # llm = \"model/InternVL3-8B-Instruct-Q8_0.gguf\"\n    # mp = \"model/mmproj-InternVL3-8B-Instruct-Q8_0.gguf\"\n\n    # you can also just provide model's hf repo id and its quant directly:\n    llm='ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0',\n    image=[\"./data/img_1.jpg\",\n           \"./data/img_2.jpg\",\n           \"./data/img_3.jpg\",], # All these three images in constructor will be used together for the inference\n    # schema=normal_format\n)\n</pre> # build constructor data = InferenceLlamacpp(     # if model amd mmproj are already downloaded,     # you can directly specify the path to model files in the constructor, for example:     # llm = \"model/InternVL3-8B-Instruct-Q8_0.gguf\"     # mp = \"model/mmproj-InternVL3-8B-Instruct-Q8_0.gguf\"      # you can also just provide model's hf repo id and its quant directly:     llm='ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0',     image=[\"./data/img_1.jpg\",            \"./data/img_2.jpg\",            \"./data/img_3.jpg\",], # All these three images in constructor will be used together for the inference     # schema=normal_format ) In\u00a0[14]: Copied! <pre># inference\nresult = data.one_inference(prompt='what is the color of the house?')\nresult\n</pre> # inference result = data.one_inference(prompt='what is the color of the house?') result Out[14]: questions1 answer1 data 0 What is the color of the house? The house in each image appears to be light-co... [./data/img_1.jpg, ./data/img_2.jpg, ./data/im... In\u00a0[18]: Copied! <pre># single image inference\ndata.schema = bool_format\nresult = data.one_inference(prompt=multi_questions_prompt, image=\"./data/img_1.jpg\")\nresult\n</pre> # single image inference data.schema = bool_format result = data.one_inference(prompt=multi_questions_prompt, image=\"./data/img_1.jpg\") result Out[18]: questions1 answer1 questions2 answer2 questions3 answer3 data 0 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? True [./data/img_1.jpg] In\u00a0[17]: Copied! <pre># multiple choice\ndata.schema = multiple_choice_format # replace the output format\nresult = data.one_inference(prompt=multi_choice_prompt,\n                            image=\"./data/img_1.jpg\")\nresult\n</pre> # multiple choice data.schema = multiple_choice_format # replace the output format result = data.one_inference(prompt=multi_choice_prompt,                             image=\"./data/img_1.jpg\") result Out[17]: questions1 answer1 explanation1 data 0 Does the house look occupied? unoccupied The porch area appears empty and there are no ... [./data/img_1.jpg] In\u00a0[4]: Copied! <pre>data = InferenceOllama(llm='hf.co/ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0',\n                       schema=bool_format)\ndata.imgs = [\n    [\"./data/img_1.jpg\",\n     \"./data/img_2.jpg\",],\n    [\"./data/img_2.jpg\",\n     \"./data/img_3.jpg\",]\n]\n\n# uncommnet the code below to do batched single-image inference\n# data.imgs = [\n#     [\"./data/img_1.jpg\",\n#      \"./data/img_2.jpg\",\n#      \"./data/img_3.jpg\",]\n# ]\n\ndata.batch_inference(prompt=multi_questions_prompt)\n</pre> data = InferenceOllama(llm='hf.co/ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0',                        schema=bool_format) data.imgs = [     [\"./data/img_1.jpg\",      \"./data/img_2.jpg\",],     [\"./data/img_2.jpg\",      \"./data/img_3.jpg\",] ]  # uncommnet the code below to do batched single-image inference # data.imgs = [ #     [\"./data/img_1.jpg\", #      \"./data/img_2.jpg\", #      \"./data/img_3.jpg\",] # ]  data.batch_inference(prompt=multi_questions_prompt) <pre>Processing...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:23&lt;00:00, 11.56s/it]\n</pre> Out[4]: questions1 answer1 questions2 answer2 questions3 answer3 data 0 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? True [./data/img_1.jpg, ./data/img_2.jpg] 1 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? True [./data/img_2.jpg, ./data/img_3.jpg] In\u00a0[5]: Copied! <pre>data.results\n</pre> data.results Out[5]: <pre>{'responses': [[QnA(questions='Is there any damage on the roof?', answer=False),\n   QnA(questions='Is any window broken or boarded?', answer=False),\n   QnA(questions='Is any door broken, missing, or boarded?', answer=True)],\n  [QnA(questions='Is there any damage on the roof?', answer=False),\n   QnA(questions='Is any window broken or boarded?', answer=False),\n   QnA(questions='Is any door broken, missing, or boarded?', answer=True)]],\n 'data': [['./data/img_1.jpg', './data/img_2.jpg'],\n  ['./data/img_2.jpg', './data/img_3.jpg']]}</pre> In\u00a0[6]: Copied! <pre>data.df\n</pre> data.df Out[6]: questions1 answer1 questions2 answer2 questions3 answer3 data 0 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? True [./data/img_1.jpg, ./data/img_2.jpg] 1 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? True [./data/img_2.jpg, ./data/img_3.jpg] In\u00a0[3]: Copied! <pre>data = InferenceLlamacpp(llm='ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0', schema=bool_format)\n# pack images in a nested list to batch multiple-image inference\ndata.imgs = [\n    [\"./data/img_1.jpg\",\n     \"./data/img_2.jpg\",],\n    [\"./data/img_2.jpg\",\n     \"./data/img_3.jpg\",]\n]\n\n# uncommnet the code below to batch single-image inference\n# data.imgs = [\n#     [\"./data/img_1.jpg\",\n#      \"./data/img_2.jpg\",\n#      \"./data/img_3.jpg\",]\n# ]\n\ndata.batch_inference(prompt=multi_questions_prompt)\n</pre> data = InferenceLlamacpp(llm='ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0', schema=bool_format) # pack images in a nested list to batch multiple-image inference data.imgs = [     [\"./data/img_1.jpg\",      \"./data/img_2.jpg\",],     [\"./data/img_2.jpg\",      \"./data/img_3.jpg\",] ]  # uncommnet the code below to batch single-image inference # data.imgs = [ #     [\"./data/img_1.jpg\", #      \"./data/img_2.jpg\", #      \"./data/img_3.jpg\",] # ]  data.batch_inference(prompt=multi_questions_prompt) <pre>Processing...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:16&lt;00:00,  8.16s/it]\n</pre> Out[3]: questions_1 answer_1 questions_2 answer_2 questions_3 answer_3 data_1 data_2 0 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? False ./data/img_1.jpg ./data/img_2.jpg 1 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? False ./data/img_2.jpg ./data/img_3.jpg In\u00a0[4]: Copied! <pre>data.df\n</pre> data.df Out[4]: questions_1 answer_1 questions_2 answer_2 questions_3 answer_3 data_1 data_2 0 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? False ./data/img_1.jpg ./data/img_2.jpg 1 Is there any damage on the roof? False Is any window broken or boarded? False Is any door broken, missing, or boarded? False ./data/img_2.jpg ./data/img_3.jpg"},{"location":"Examples/1_basic_inference/#1-one-time-inference","title":"1 one-time inference\u00b6","text":""},{"location":"Examples/1_basic_inference/#11-ollama","title":"1.1 Ollama\u00b6","text":""},{"location":"Examples/1_basic_inference/#12-llamacpp","title":"1.2 Llama.cpp\u00b6","text":""},{"location":"Examples/1_basic_inference/#2-batched-inference-with-multiple-image-input","title":"2 Batched inference with multiple-image input\u00b6","text":"<p>To implement batched multi-image input for inference, we just need to pack images (path) into a nested list/tuple.</p>"},{"location":"Examples/1_basic_inference/#21-ollama","title":"2.1 Ollama\u00b6","text":""},{"location":"Examples/1_basic_inference/#22-llamacpp","title":"2.2 Llama.cpp\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/","title":"Batched input with geolocated data","text":"In\u00a0[1]: Copied! <pre>from urbanworm.dataset import GeoTaggedData\nfrom urbanworm.inference.llama import InferenceLlamacpp\n</pre> from urbanworm.dataset import GeoTaggedData from urbanworm.inference.llama import InferenceLlamacpp In\u00a0[2]: Copied! <pre># Import api keys\nwith open(\"mapillary_key.txt\", 'r') as file:\n    mapillary_key = file.read()\nwith open(\"flickr_key.txt\", 'r') as file:\n    flickr_key = file.read()\nwith open(\"freesound_key.txt\", 'r') as file:\n    freesound_key = file.read()\n</pre> # Import api keys with open(\"mapillary_key.txt\", 'r') as file:     mapillary_key = file.read() with open(\"flickr_key.txt\", 'r') as file:     flickr_key = file.read() with open(\"freesound_key.txt\", 'r') as file:     freesound_key = file.read() In\u00a0[3]: Copied! <pre># get building footprints from OSM\n\n# Initiate the constructor\ngtd = GeoTaggedData()\n# Define the area of interest using a bounding box (bbox)\nbbox = (-83.208003,42.374646,-83.206608,42.375328) # in Detroit, USA\n# we can just get house with no more than 200 square meter (single family houses with garage excluded)\ngtd.getBuildings(bbox, min_area=60, max_area=200)\n</pre> # get building footprints from OSM  # Initiate the constructor gtd = GeoTaggedData() # Define the area of interest using a bounding box (bbox) bbox = (-83.208003,42.374646,-83.206608,42.375328) # in Detroit, USA # we can just get house with no more than 200 square meter (single family houses with garage excluded) gtd.getBuildings(bbox, min_area=60, max_area=200) <pre>14 buildings found in the bounding box.\n</pre> In\u00a0[4]: Copied! <pre>gtd.units.plot()\n</pre> gtd.units.plot() Out[4]: <pre>&lt;Axes: &gt;</pre> <p>For each house location, we find nearby (\u226430 m) panoramic Mapillary images, picks up to 3 that are not too close together in sequence, and outputs 90\u00b0 perspective crops that are reoriented to center on the house.</p> In\u00a0[4]: Copied! <pre>gtd.get_svi_from_locations(key = mapillary_key, # api key\n                           distance = 30,       # only search for available street view with 30 meters from the house location\n                           pano = True,         # only search for 360-degree street view images\n                           reoriented = True,   # reorient and crop the street view images to make them only frame the house at the center of scene\n                           multi_num = 3,       # return three closest street views from the house location\n                           fov = 80,            # The field of view in degrees for the reoriented images\n                           interval = 2,        # The interval between each street view (i.g, `interval = 2` means there should be two available images between two collected images)\n                           year = (2024, 2025), # only search for images captured between 2024 and 2025\n                           time_of_day = 'day'  # only search for images captured during the daytime\n                           )\n</pre> gtd.get_svi_from_locations(key = mapillary_key, # api key                            distance = 30,       # only search for available street view with 30 meters from the house location                            pano = True,         # only search for 360-degree street view images                            reoriented = True,   # reorient and crop the street view images to make them only frame the house at the center of scene                            multi_num = 3,       # return three closest street views from the house location                            fov = 80,            # The field of view in degrees for the reoriented images                            interval = 2,        # The interval between each street view (i.g, `interval = 2` means there should be two available images between two collected images)                            year = (2024, 2025), # only search for images captured between 2024 and 2025                            time_of_day = 'day'  # only search for images captured during the daytime                            ) <pre>  0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> <p>The metadata is stored in the constructor, including:</p> <ul> <li>mapillary image id,</li> <li>sequence id,</li> <li>when it was captured,</li> <li>the original orientation angle,</li> <li>image coordinates,</li> <li>and the house location index.</li> </ul> In\u00a0[8]: Copied! <pre>gtd.svi_metadata.head(5)\n</pre> gtd.svi_metadata.head(5) Out[8]: id sequence captured_at compass_angle image_lon image_lat url loc_id 0 1009175311394842 ThVwcIdPek1fQ6z4Dbv3KJ 2025-5-5-17 268.39 -83.206960 42.374651 https://scontent-det1-1.xx.fbcdn.net/m1/v/t6/A... 0 1 1223726255791358 ThVwcIdPek1fQ6z4Dbv3KJ 2025-5-5-17 268.57 -83.206838 42.374655 https://scontent-det1-1.xx.fbcdn.net/m1/v/t6/A... 0 2 1261874768838904 ThVwcIdPek1fQ6z4Dbv3KJ 2025-5-5-17 267.95 -83.207081 42.374648 https://scontent-det1-1.xx.fbcdn.net/m1/v/t6/A... 0 0 1009175311394842 ThVwcIdPek1fQ6z4Dbv3KJ 2025-5-5-17 268.39 -83.206960 42.374651 https://scontent-det1-1.xx.fbcdn.net/m1/v/t6/A... 1 1 1223726255791358 ThVwcIdPek1fQ6z4Dbv3KJ 2025-5-5-17 268.57 -83.206838 42.374655 https://scontent-det1-1.xx.fbcdn.net/m1/v/t6/A... 1 <p>The data information is also stored in a dictionary format for data downloading and processing in the future.</p> <p>In this case study, since the image has been reoriented and cropped, the images have been store in base64 format inside the dataset constructor.</p> In\u00a0[10]: Copied! <pre>gtd.svis['loc_id'][0], gtd.svis['id'][0], gtd.svis['data'][0][:500]\n</pre> gtd.svis['loc_id'][0], gtd.svis['id'][0], gtd.svis['data'][0][:500] Out[10]: <pre>(0,\n '1009175311394842',\n 'iVBORw0KGgoAAAANSUhEUgAAArwAAAH0CAIAAABQO2mIAAAgAElEQVR4AezBCdSld0Em+Of5v+vd77cvtS+pVGUlCSEJISwBZVFAGpVBW0fsmXaZse3Rc2bsHrtPj4N6WsFxnN5s5ygytqA2jYAggqwJBJCEVJZKbam9vv377n7f+27/Z27dyldUpSohtjNndLp+P+bWAsQmCaBISLkEQxeAtSJBEpskkLiIxJAECaAFwCEQI4JkSeISCVcjMSThciRISPiOCOEqkgBwBFexEK6FIDYJwlUICRoiAQgXOJCRQIKEBAmgAHEItLIcAgVhRBIAQwNAEJ5HxFVISCAhYYiEIIJWFgCHQEG4kiwxQiNciaCgIZL46yMoiCAACc9DQsKQBGNgLUiQkEBCwhAJQXgBBK0sAAECSAIwEkiCVpZDoCAABHEtxP8LBBAQQAhXIP4/phEaI2gIhIERhBGCgggCEIT/h0jCiGislTG0VsbQWllrXdeR')</pre> In\u00a0[\u00a0]: Copied! <pre>gtd.download_to_dir(data='svi',\n                    to_dir='/svi_download',\n                    prefix='svi')\n# after downloading all images, the local path of images will be stored, which allows the inference constructor to access local images\ngtd.svis['path'][:5]\n</pre> gtd.download_to_dir(data='svi',                     to_dir='/svi_download',                     prefix='svi') # after downloading all images, the local path of images will be stored, which allows the inference constructor to access local images gtd.svis['path'][:5] In\u00a0[5]: Copied! <pre># indicate that the street view images wil be used\ngtd.set_images('svi')\n# pass the dataset to the inference constructor\ndata = InferenceLlamacpp(geo_tagged_data=gtd)\n</pre> # indicate that the street view images wil be used gtd.set_images('svi') # pass the dataset to the inference constructor data = InferenceLlamacpp(geo_tagged_data=gtd) In\u00a0[6]: Copied! <pre># pack images by their sample locations\ndata.pack_by_location()\n</pre> # pack images by their sample locations data.pack_by_location() In\u00a0[8]: Copied! <pre>from typing import Literal\n\nprompt = '''\n    Question: Does this house look occupied if it is not a vacant lot?\n\n    **An occupied house means that the house is not abandoned and\n    some people may live in this house even if there is not people outside**\n'''\n\n# specify model\ndata.llm = 'ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0'\n# define output schema\ndata.schema = {\"answer\": (Literal['occupied', 'unoccupied', 'vacant'], ...),\n               \"explanation\": (str, ...),}\n# inference\ndata.batch_inference(prompt=prompt)\n</pre> from typing import Literal  prompt = '''     Question: Does this house look occupied if it is not a vacant lot?      **An occupied house means that the house is not abandoned and     some people may live in this house even if there is not people outside** '''  # specify model data.llm = 'ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0' # define output schema data.schema = {\"answer\": (Literal['occupied', 'unoccupied', 'vacant'], ...),                \"explanation\": (str, ...),} # inference data.batch_inference(prompt=prompt) <pre>Processing...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [01:36&lt;00:00,  7.40s/it]\n</pre> Out[8]: answer_1 explanation_1 data_1 data_2 data_3 0 occupied The house has a well-maintained lawn and a fen... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 1 occupied The house appears to be occupied because it is... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 2 occupied The house in the image appears to be occupied ... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 3 occupied The house is not a vacant lot, so it is likely... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 4 occupied The house is not a vacant lot, so it is likely... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 5 occupied The house is not a vacant lot, so it is likely... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 6 occupied The house is not a vacant lot, so it is likely... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 7 occupied The house is not a vacant lot, so it is likely... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 8 occupied The house is not a vacant lot, so it is likely... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 9 occupied The house in the image appears to be occupied ... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 10 occupied The house has a car parked in the driveway, wh... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 11 occupied The house looks occupied because there is a ca... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... 12 occupied The house has a car parked in the driveway and... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... /var/folders/fb/4kj6xrcs195bxml3gxcrrkq80000gn... In\u00a0[3]: Copied! <pre># set a center point for spatially querying data\ngtd = GeoTaggedData(locations=[[114.176773,22.302554]])\ngtd.get_photo_from_location(key=flickr_key,\n                            distance=1000, # only searching for data within the distance from the given center point\n                            max_return=200, # only return the given number of photos\n                            exclude_personal_photo = True, # drop personal photos using opencv/face_detection_yunet\n                            )\n</pre> # set a center point for spatially querying data gtd = GeoTaggedData(locations=[[114.176773,22.302554]]) gtd.get_photo_from_location(key=flickr_key,                             distance=1000, # only searching for data within the distance from the given center point                             max_return=200, # only return the given number of photos                             exclude_personal_photo = True, # drop personal photos using opencv/face_detection_yunet                             ) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <p>There are more than a half of photos detected as personal photos</p> In\u00a0[8]: Copied! <pre>len(gtd.photo_metadata)\n</pre> len(gtd.photo_metadata) Out[8]: <pre>93</pre> <p>The metadata is stored in the constructor, including:</p> <ul> <li>Flickr photo id,</li> <li>title of the sound,</li> <li>owner,</li> <li>when it was recorded,</li> <li>sound coordinates,</li> <li>...</li> </ul> In\u00a0[4]: Copied! <pre>gtd.photo_metadata.head(5)\n</pre> gtd.photo_metadata.head(5) Out[4]: loc_id id title owner datetaken latitude longitude distance_m tags views license url 18 0 55039184398 Observatory Road, Hong Kong / \u5929\u6587\u81fa\u9053,\u9999\u6e2f 23502041@N06 2026-01-13 19:15:41 22.301397 114.174430 273.225805 11 0 https://live.staticflickr.com/65535/5503918439... 115 0 55033921774 [Big Bee Taxi]Kaiyi X3 Pro EV WN4210 200369775@N04 2026-01-04 16:43:13 22.303136 114.180794 418.698089 1 0 https://live.staticflickr.com/65535/5503392177... 78 0 55036639646 20260110 \u91d1\u5229\u5b9d\u9999\u6e2f\u4e00\u65e5\u904a 30199947@N07 2026-01-10 13:43:11 22.301252 114.172058 506.210597 \u9999\u6e2f \u91d1\u5229\u5bf6 kongkong 20 0 https://live.staticflickr.com/65535/5503663964... 215 0 54993258227 IMG_7521 204002042@N04 2025-12-04 10:07:34 22.303761 114.171822 526.726446 6 0 https://live.staticflickr.com/65535/5499325822... 212 0 54994140806 IMG_7519 204002042@N04 2025-12-04 10:02:15 22.303791 114.171830 526.791740 8 0 https://live.staticflickr.com/65535/5499414080... In\u00a0[5]: Copied! <pre>gtd.photos['id'][0], gtd.photos['data'][0]\n</pre> gtd.photos['id'][0], gtd.photos['data'][0] Out[5]: <pre>('55039184398',\n 'https://live.staticflickr.com/65535/55039184398_17460374cf_o.jpg')</pre> In\u00a0[9]: Copied! <pre># download data to directory\ngtd.download_to_dir(data='photo',\n                    to_dir='/Users/xiaohaoyang/Downloads/test_download',\n                    prefix='test_download')\n</pre> # download data to directory gtd.download_to_dir(data='photo',                     to_dir='/Users/xiaohaoyang/Downloads/test_download',                     prefix='test_download') In\u00a0[6]: Copied! <pre># indicate that the flickr photos wil be used\ngtd.set_images('photo')\n# pass the dataset to the inference constructor\ndata = InferenceLlamacpp(geo_tagged_data=gtd)\n</pre> # indicate that the flickr photos wil be used gtd.set_images('photo') # pass the dataset to the inference constructor data = InferenceLlamacpp(geo_tagged_data=gtd) In\u00a0[13]: Copied! <pre>from typing import Literal\n\nprompt = '''\n    please answer the following questions (make a guess) after seeing the photo:\n    What is mian thing/focus capture in the photo and is this indoor or outdoor and dose the photo capture any outdoor urban or natural scenery?\n'''\n\n# specify model\ndata.llm = 'ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0'\n# define output schema\ndata.schema = {\"focus\": (str, ...),\n               \"indoor_outdoor\": (Literal['indoor', 'outdoor'], ...),\n               \"scenery\": (Literal['neither', 'urban', 'nature', 'both'], ...),}\n# inference\nresult = data.batch_inference(prompt=prompt)\nresult.head(5)\n</pre> from typing import Literal  prompt = '''     please answer the following questions (make a guess) after seeing the photo:     What is mian thing/focus capture in the photo and is this indoor or outdoor and dose the photo capture any outdoor urban or natural scenery? '''  # specify model data.llm = 'ggml-org/InternVL3-8B-Instruct-GGUF:Q8_0' # define output schema data.schema = {\"focus\": (str, ...),                \"indoor_outdoor\": (Literal['indoor', 'outdoor'], ...),                \"scenery\": (Literal['neither', 'urban', 'nature', 'both'], ...),} # inference result = data.batch_inference(prompt=prompt) result.head(5) <pre>Processing...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 93/93 [09:03&lt;00:00,  5.84s/it]\n</pre> Out[13]: focus_1 indoor_outdoor_1 scenery_1 data_1 0 The main focus of the photo is the brightly li... indoor urban https://live.staticflickr.com/65535/5503918439... 1 The main focus of the photo is the busy urban ... outdoor urban https://live.staticflickr.com/65535/5503392177... 2 The main focus of the photo is the colorful, r... outdoor urban https://live.staticflickr.com/65535/5503663964... 3 The main focus of the photo is a plate of food... indoor urban https://live.staticflickr.com/65535/5499325822... 4 The main focus of the photo is a plate of food... indoor urban https://live.staticflickr.com/65535/5499414080... In\u00a0[9]: Copied! <pre># set a center point for spatially querying data\ngtd = GeoTaggedData(locations=[[139.726978,35.658524]])\ngtd.get_sound_from_location(key=freesound_key,\n                            query='field_recording', # search for field recordings\n                            distance=5000, #\n                            max_return=200, #\n                            duration=(20, 6000), # only search for recording with a duration between 15 and 6000 seconds\n                            slice_duration=10, # need to mark the checkpoints every 10 second for clipping the sound\n                            slice_max_num=2, # only need two clips from each sound\n                            silent=False\n                            )\n</pre> # set a center point for spatially querying data gtd = GeoTaggedData(locations=[[139.726978,35.658524]]) gtd.get_sound_from_location(key=freesound_key,                             query='field_recording', # search for field recordings                             distance=5000, #                             max_return=200, #                             duration=(20, 6000), # only search for recording with a duration between 15 and 6000 seconds                             slice_duration=10, # need to mark the checkpoints every 10 second for clipping the sound                             slice_max_num=2, # only need two clips from each sound                             silent=False                             ) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre>len(gtd.audio_metadata)\n</pre> len(gtd.audio_metadata) Out[10]: <pre>41</pre> In\u00a0[11]: Copied! <pre>gtd.audio_metadata.head(5)\n</pre> gtd.audio_metadata.head(5) Out[11]: loc_id id name username license created duration tags geotag latitude ... url page_url description num_downloads avg_rating slice preview-hq-mp3 preview-hq-ogg preview-lq-mp3 preview-lq-ogg 18 0 686617 Rapid Male Chanting in Temple in Japan calebjay https://creativecommons.org/licenses/by/4.0/ 2023-05-10T07:47:39Z 28.7270 [acapella, a-cappella, acappella, field-record... 35.64929634007673 139.74117500000023 35.649296 ... https://freesound.org/people/calebjay/sounds/6... https://freesound.org/people/calebjay/sounds/6... A man chanting in a temple in Japan. The templ... 151 5.000000 [[0, 10000], [10000, 20000]] https://cdn.freesound.org/previews/686/686617_... https://cdn.freesound.org/previews/686/686617_... https://cdn.freesound.org/previews/686/686617_... https://cdn.freesound.org/previews/686/686617_... 1 0 135207 train from ebisu to shibuya \u6075\u6bd4\u5bff djgriffin http://creativecommons.org/publicdomain/zero/1.0/ 2011-11-19T06:20:01Z 160.5000 [board, dr-1, ebisu, field, japan, platform, r... 35.6467 139.71012 35.646700 ... https://freesound.org/people/djgriffin/sounds/... https://freesound.org/people/djgriffin/sounds/... a field recording using the tascam dr-1 from e... 486 4.888889 [[0, 10000], [10000, 20000]] https://cdn.freesound.org/previews/135/135207_... https://cdn.freesound.org/previews/135/135207_... https://cdn.freesound.org/previews/135/135207_... https://cdn.freesound.org/previews/135/135207_... 36 0 440606 Gong in zen buddhism florianreichelt http://creativecommons.org/publicdomain/zero/1.0/ 2018-09-17T14:08:42Z 32.6200 [bell, buddhism, buddhist, cymbal, drum, gong,... 35.6580390655 139.749506012 35.658039 ... https://freesound.org/people/florianreichelt/s... https://freesound.org/people/florianreichelt/s... We recorded this sound while our trip through ... 1093 4.277778 [[0, 10000], [10000, 20000]] https://cdn.freesound.org/previews/440/440606_... https://cdn.freesound.org/previews/440/440606_... https://cdn.freesound.org/previews/440/440606_... https://cdn.freesound.org/previews/440/440606_... 38 0 440600 buddhsim monk is playing a gong in zen buddhism florianreichelt http://creativecommons.org/publicdomain/zero/1.0/ 2018-09-17T14:08:32Z 38.1747 [bang, bell, blacksmith, buddhism, clang, ding... 35.6577128899 139.749545429 35.657713 ... https://freesound.org/people/florianreichelt/s... https://freesound.org/people/florianreichelt/s... We recorded this sound while our trip through ... 203 4.888889 [[0, 10000], [10000, 20000]] https://cdn.freesound.org/previews/440/440600_... https://cdn.freesound.org/previews/440/440600_... https://cdn.freesound.org/previews/440/440600_... https://cdn.freesound.org/previews/440/440600_... 33 0 799435 Hie Shrine,Shichi-Go-San,Tsuri Taiko (Noise Fi... Hinoki.owo https://creativecommons.org/licenses/by/4.0/ 2025-04-18T11:25:21Z 52.0000 [ambiance, ambience, ambient, asia, background... 35.674787 139.739845 35.674787 ... https://freesound.org/people/Hinoki.owo/sounds... https://freesound.org/people/Hinoki.owo/sounds... Late November of 2024, the visit of Hie Shrine... 13 5.000000 [[0, 10000], [10000, 20000]] https://cdn.freesound.org/previews/799/799435_... https://cdn.freesound.org/previews/799/799435_... https://cdn.freesound.org/previews/799/799435_... https://cdn.freesound.org/previews/799/799435_... <p>5 rows \u00d7 22 columns</p> In\u00a0[5]: Copied! <pre>gtd.audios['id'][0], gtd.audios['data'][0], gtd.audios['slice'][0]\n</pre> gtd.audios['id'][0], gtd.audios['data'][0], gtd.audios['slice'][0] Out[5]: <pre>(686617,\n 'https://cdn.freesound.org/previews/686/686617_13137374-hq.mp3',\n [0, 10000])</pre> In\u00a0[\u00a0]: Copied! <pre>gtd.download_to_dir(data='audio',\n                    to_dir='/audio_download',\n                    prefix='audio_download')\n</pre> gtd.download_to_dir(data='audio',                     to_dir='/audio_download',                     prefix='audio_download') In\u00a0[13]: Copied! <pre># pass the dataset to the inference constructor\ndata = InferenceLlamacpp(geo_tagged_data=gtd)\n</pre> # pass the dataset to the inference constructor data = InferenceLlamacpp(geo_tagged_data=gtd) In\u00a0[17]: Copied! <pre>from typing import Literal\n\nprompt = '''\n    Please answer the following questions after listening the audio:\n    Can you clearly hear wind sounds?\n\n    Your answer should be yes / no\n'''\n\n# specify model\ndata.llm = 'ggml-org/Qwen2.5-Omni-7B-GGUF:Q8_0'\n# define output schema\ndata.schema = {\"answer\": (Literal['yes', 'no'], ...)}\n# inference\nresult = data.batch_inference(prompt=prompt,\n                              audio_input = True # indicate that input data is audio\n                              )\nresult.head(5)\n</pre> from typing import Literal  prompt = '''     Please answer the following questions after listening the audio:     Can you clearly hear wind sounds?      Your answer should be yes / no '''  # specify model data.llm = 'ggml-org/Qwen2.5-Omni-7B-GGUF:Q8_0' # define output schema data.schema = {\"answer\": (Literal['yes', 'no'], ...)} # inference result = data.batch_inference(prompt=prompt,                               audio_input = True # indicate that input data is audio                               ) result.head(5) <pre>Processing...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82/82 [13:19&lt;00:00,  9.75s/it]\n</pre> Out[17]: answer_1 data_1 answer_2 0 no https://cdn.freesound.org/previews/686/686617_... NaN 1 no https://cdn.freesound.org/previews/686/686617_... NaN 2 no https://cdn.freesound.org/previews/135/135207_... NaN 3 no https://cdn.freesound.org/previews/135/135207_... NaN 4 no https://cdn.freesound.org/previews/440/440606_... NaN"},{"location":"Examples/2_inference_geo_located_data/#batched-input-with-geolocated-data","title":"Batched input with geolocated data\u00b6","text":"<p>In this tutorial, we will be using the <code>urbanworm.dataset</code> module to collect geo-located data, including Mapillary street views, Flickr photos, and Freesound recordings. The <code>urbanworm.inference</code> module will be used to inference with InternVL3-8B-Instruct and Qwen2.5-Omni for imagery and audio data.</p> <p>We will be using three case studies to demonstrate what insight may be gained from these datasets:</p> <ol> <li>case study (in Detroit) using street views: Does the house look occupied?</li> <li>case study (in Hongkong) using Flickr photos: What was captured in the photo?</li> <li>case study (in Tokyo) using Freesound recordings: Did you hear the wind?</li> </ol> <p>For each case study, We follow the following steps:</p> <ol> <li>Query and process data</li> <li>Download the dataset</li> <li>Pass the dataset constructor to inference constructor</li> <li>Batch inference</li> </ol> <p>Retrieving data will require api keys of Mapillary, Flickr, and Freesound, which can be requested from:</p> <ul> <li>https://www.mapillary.com/developer/api-documentation</li> <li>https://www.flickr.com/services/api/</li> <li>https://freesound.org/apiv2/apply</li> </ul> <p>Note:</p> <ul> <li>To see all the available street views on Mapillary, please check out Mapillary Map App</li> <li>To see all the available geo-tagged photos on Flickr, please check out everyone's photo on the map</li> <li>To see all the available geo-tagged recordings on Freesound, please check out the map of sounds</li> </ul>"},{"location":"Examples/2_inference_geo_located_data/#1-does-the-house-look-occupied","title":"1 Does the house look occupied?\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#11-retrieve-street-views-at-property-level","title":"1.1 Retrieve street views at property-level\u00b6","text":"<p>Building footprints will be used as proximity for gathering the data.</p>"},{"location":"Examples/2_inference_geo_located_data/#12-download-data-optional-but-recommended","title":"1.2 Download data (optional but recommended)\u00b6","text":"<p>For batched image inference, working with local data can be usually more stable than streaming data. Therefore, downloading data (images or sound recordings) to a directory is highly recommended but optional.</p>"},{"location":"Examples/2_inference_geo_located_data/#13-pass-to-the-inference-constructor","title":"1.3 Pass to the inference constructor\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#14-batched-inference","title":"1.4 Batched inference\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#2-what-was-captured-in-the-photo","title":"2 What was captured in the photo?\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#21-retrieve-flickr-photos-within-a-radius","title":"2.1 Retrieve Flickr photos within a radius\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#22-download-data-optional-but-recommended","title":"2.2 Download data (optional but recommended)\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#23-pass-to-the-inference-constructor","title":"2.3 pass to the inference constructor\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#24-inference","title":"2.4 inference\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#3-did-you-hear-the-wind","title":"3 Did you hear the wind?\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#31-retrieve-freesound-recordings-within-a-radius","title":"3.1 Retrieve Freesound recordings within a radius\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#32-download-dataset-optional-but-recommended","title":"3.2 Download dataset (optional but recommended)\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#33-pass-it-to-the-inference-constructor","title":"3.3 Pass it to the inference constructor\u00b6","text":""},{"location":"Examples/2_inference_geo_located_data/#34-inference","title":"3.4 Inference\u00b6","text":""}]}