{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Urban-Worm","text":"<p>A python package for studying urban environment imagery with Llama vison model</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Urban-Worm is a Python library that integrates remote sensing imagery, street view data, and multimodal model to assess urban units. Using APIs for data collection and vision-language models for inference, Urban-Worm is designed to support the automation of the evaluation for urban environments, including roof integrity, structural condition, landscape quality, and urban perception.</p> <p> </p>"},{"location":"#features","title":"Features","text":"<ul> <li>run vision-language models locally with local datasets and remain information privacy</li> <li>download building footprints from OSM and global building released by Bing map </li> <li>search and clip aerial and street view images (via APIs) based on urban units such as parcel and building footprint data</li> <li>automatically calibrate the oritation of panorama street view and the extent of aerial image</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The package is heavily built on Ollama client and Ollama-python. Credit goes to the developers of these projects.</p> <ul> <li>ollama</li> <li>ollama-python</li> <li>structured outputs</li> </ul> <p>The functionality about sourcing and processing GIS data (satellite &amp; street view imagery) and 360-degree street view image processing is built on the following open projects. Credit goes to the developers of these projects.</p> <ul> <li>tms2geotiff</li> <li>GlobalMLBuildingFootprints</li> <li>Mapillary API</li> <li>Equirec2Perspec</li> </ul> <p>The development of this package is supported and inspired by the city of Detroit.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#step-1-install-ollama-client","title":"Step 1: install Ollama client","text":"<p>Please make sure Ollama is installed before installing urban-worm</p>"},{"location":"installation/#linux","title":"Linux","text":"<p>For Linux, users can also install ollama by running in the terminal:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"installation/#mac","title":"MAC","text":"<p>For MacOS, users can also install ollama using <code>brew</code>:</p> <pre><code>brew install ollama\n</code></pre> <p>To install <code>brew</code>, run in the terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>After installing <code>brew</code>, you will see a following instruction:</p> <pre><code>==&gt; Next steps:\n- Run these commands in your terminal to add Homebrew to your PATH:\n    echo &gt;&gt; /Users/yourusername/.bash_profile\n    echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/yourusername/.bash_profile\n    eval \"$(/opt/homebrew/bin/brew shellenv)\"\n</code></pre>"},{"location":"installation/#windows","title":"Windows","text":"<p>Windows users should directly install the Ollama client</p>"},{"location":"installation/#step-2-install-gdal-first","title":"Step 2: install GDAL first","text":"<p>For macOS, Linux, and Windows users, <code>gdal</code> may need to be installed at very begining using <code>conda</code>. </p>"},{"location":"installation/#install-conda","title":"install conda","text":"<p>Please download and install Anaconda to use <code>conda</code>.</p>"},{"location":"installation/#install-gdal","title":"install GDAL","text":"<p>If the installation method above does not work, try to install with <code>conda</code>:</p> <pre><code> conda install -c conda-forge gdal\n</code></pre> <p>Mac users may install <code>gdal</code> (if the installation method below does not work, try to install with conda):</p> <pre><code> brew install gdal\n</code></pre>"},{"location":"installation/#note","title":"Note","text":"<p>if you come across error like <code>conda command not found</code> when using <code>conda</code>, please refer following solutions to add Conda to the PATH:</p> <ul> <li>Linux: <code>export PATH=~/anaconda3/bin:$PATH</code> (source)</li> <li>Mac: <code>export PATH=\"/home/username/miniconda/bin:$PATH\"</code>. Please make sure to replace <code>/home/username/miniconda</code> with your actual path (source)</li> <li>Windows: Open Anaconda Prompt &gt; Check Conda Installed Location: <code>where conda</code> &gt; Open Advanced System Settings &gt; Click on Environment Variables &gt; Edit Path &gt; Add New Path: </li> </ul> <pre><code> C:\\Users\\&lt;username&gt;\\Anaconda3\\Scripts\n C:\\Users\\&lt;username&gt;\\Anaconda3\n C:\\Users\\&lt;username&gt;\\Anaconda3\\Library\\bin\n</code></pre> <p>(source)</p>"},{"location":"installation/#step-3-install-urabn-worm-from-pypi","title":"Step 3: install urabn-worm from PyPi","text":"<p>The package urabnworm can be installed with <code>pip</code>:</p> <pre><code>pip install urban-worm \n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use urban-worm in a project:</p> <pre><code>from urbanworm import UrbanDataSet\n</code></pre>"},{"location":"usage/#single-image","title":"single-image","text":"<pre><code># load a local image\ndata = UrbanDataSet(image = './docs/data/test1.jpg')\nsystem = '''\n    Given a top view image, you are going to roughly estimate house conditions. Your answer should be based only on your observation. \n    The format of your response must include question, answer (yes or no), explanation (within 50 words)\n'''\nprompt = '''\n    Is there any damage on the roof?\n'''\ndata.oneImgChat(system=system, prompt=prompt)\n# output:\n# {'question': 'Is there any damage on the roof?',\n#  'answer': 'no',\n#  'explanation': 'No visible signs of damage or wear on the roof',\n#  'img': '/9j/4AAQSkZ...'}\n</code></pre>"},{"location":"usage/#multiple-images-using-osm-data-and-mapillary-api","title":"multiple images using OSM data and Mapillary API","text":"<p>Get building footprints as units and collect satellite and street view images based on each unit. Finally, chat with MLLM model for each unit based on collected images.</p> <p>To get a token/key to access data via mapillary api, please create an acount and apply on Mapillary</p> <pre><code>bbox = (-83.235572,42.348092,-83.235154,42.348806)\ndata = UrbanDataSet()\ndata.bbox2Buildings(bbox, source='osm')\n\nsystem = '''\n    Given a top view image or street view images, you are going to roughly estimate house conditions. \n    Your answer should be based only on your observation. \n    The format of your response must include question, answer (yes or no), explanation (within 50 words) for each question.\n'''\n\nprompt = {\n    'top': '''\n        Is there any damage on the roof?\n    ''',\n    'street': '''\n        Is the wall missing or damaged?\n        Is the yard maintained well?\n    '''\n}\n\n# add the Mapillary key\ndata.mapillary_key = 'MLY|......'\n# use both the aerial and street view images (with type='both')\ndata.loopUnitChat(system=system, prompt=prompt, type='both', epsg=2253)\n# convert results into GeoDataframe\ndata.to_gdf()\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/","title":"UrbanDataSet","text":"<p>Dataset class for urban imagery inference using MLLM.</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>class UrbanDataSet:\n    '''\n    Dataset class for urban imagery inference using MLLM.\n    '''\n    def __init__(self, image=None, images:list=None, units:str=None, \n                 format:Response=None, mapillary_key:int=None, random_sample:int=None):\n        '''\n        Add data or api key\n\n        Args:\n            image (str): The path to the image.\n            images (list): The list of image paths.\n            units (str): The path to the shapefile.\n            format (Response): The response format.\n            mapillary_key (str): The Mapillary API key.\n            random_sample (int): The number of random samples.\n        '''\n\n        if image != None and detect_input_type(image) == 'image_path':\n            self.img = encode_image_to_base64(image)\n        else:\n            self.img = image\n\n        if images != None and detect_input_type(images[0]) == 'image_path':\n            self.imgs = [encode_image_to_base64(im) for im in images]\n        else:\n            self.imgs = images\n\n        if random_sample != None and units != None:\n            self.units = loadSHP(units).sample(random_sample)\n        elif random_sample == None and units != None:\n            self.units = loadSHP(units)\n        else:\n            self.units = units\n\n        if format == None:\n            self.format = Response()\n        else:\n            self.format = format\n\n        self.mapillary_key = mapillary_key\n\n        self.results = None\n\n    def preload_model(self, model_name):\n        \"\"\"\n        Ensures that the required Ollama model is available.\n        If not, it automatically pulls the model.\n\n        Args:\n            model_name (str): model name\n        \"\"\"\n        import ollama\n\n        try:\n            ollama.pull(model_name)\n\n        except Exception as e:\n            print(f\"Warning: Ollama is not installed or failed to check models: {e}\")\n            print(\"Please install Ollama client: https://github.com/ollama/ollama/tree/main\")\n            raise RuntimeError(\"Ollama not available. Install it before running.\")\n\n    def bbox2Buildings(self, bbox:list|tuple, source:str='osm', \n                       min_area:float|int=0, max_area:float|int=None, \n                       random_sample:int=None) -&gt; str:\n        '''\n        Extract buildings from OpenStreetMap using the bbox.\n\n        Args:\n            bbox (list or tuple): The bounding box.\n            source (str): The source of the buildings. ['osm', 'being']\n            min_area (float or int): The minimum area.\n            max_area (float or int): The maximum area.\n            random_sample (int): The number of random samples.\n\n        Returns:\n            str: The number of buildings found in the bounding box\n        '''\n\n        if source not in ['osm', 'being']:\n            raise Exception(f'{source} is not supported')\n\n        if source == 'osm':\n            buildings = getOSMbuildings(bbox, min_area, max_area)\n        elif source == 'being':\n            buildings = getGlobalMLBuilding(bbox, min_area, max_area)\n        if buildings is None or buildings.empty:\n            if source == 'osm':\n                return \"No buildings found in the bounding box. Please check https://overpass-turbo.eu/ for areas with buildings.\"\n            if source == 'being':\n                return \"No buildings found in the bounding box. Please check https://github.com/microsoft/GlobalMLBuildingFootprints for areas with buildings.\"\n        if random_sample != None:\n            buildings = buildings.sample(random_sample)\n        self.units = buildings\n        return f\"{len(buildings)} buildings found in the bounding box.\"\n\n    def oneImgChat(self, model:str='llama3.2-vision',system:str=None, prompt:str=None, \n                   temp:float=0.0, top_k:float=0.8, top_p:float=0.8,\n                   saveImg:bool=True) -&gt; dict:\n\n        '''\n        Chat with MLLM model with one image.\n\n        Args:\n            model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n            system (optinal): The system message.\n            prompt (str): The prompt message.\n            img (str): The image path.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            saveImg (bool): The saveImg for save each image in base64 format in the output.\n\n        Returns:\n            dict: A dictionary includes questions/messages, responses/answers, and image base64 (if required) \n        '''\n\n        if model not in ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']:\n            raise Exception(f'{model} is not supported')\n        self.preload_model(model)\n\n        print(\"Inference starts ...\")\n        r = self.LLM_chat(model=model, system=system, prompt=prompt, img=[self.img], \n                          temp=temp, top_k=top_k, top_p=top_p)\n        r = dict(r.responses[0])\n        if saveImg:\n            r['img'] = self.img\n        return r\n\n    def loopImgChat(self, model:str='llama3.2-vision', system:str=None, prompt:str=None, \n                    temp:float=0.0, top_k:float=0.8, top_p:float=0.8, saveImg:bool=True, \n                    progressBar:bool=False) -&gt; list:\n        '''\n        Chat with MLLM model for each image.\n\n        Args:\n            model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n            system (str, optinal): The system message.\n            prompt (str): The prompt message.\n            temp (float): The temperature value.\n            top_k (float): The top_k value.\n            top_p (float): The top_p value.\n            saveImg (bool): The saveImg for save each image in base64 format in the output.\n            progressBar (bool): The progress bar for showing the progress of data analysis over the units\n\n        Returns:\n            list A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)\n        '''\n\n        if model not in ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']:\n            raise Exception(f'{model} is not supported')\n        self.preload_model(model)\n\n        from tqdm import tqdm\n\n        res = []\n        for i in tqdm(range(len(self.imgs)), desc=\"Processing...\", ncols=75, disable=progressBar):\n        # for i in range(len(self.imgs)):\n            img = self.imgs[i]\n            r = self.LLM_chat(model=model, system=system, prompt=prompt, img=[img], \n                              temp=temp, top_k=top_k, top_p=top_p)\n            r = dict(r.responses[0])\n            if saveImg:\n                im = {'img': img}\n                res += [{**r, **im}]\n            else:\n                res += [r]\n        return res\n\n    def loopUnitChat(self, model:str='llama3.2-vision', system:str=None, prompt:dict=None, \n                     temp:float=0.0, top_k:float=0.8, top_p:float=0.8, \n                     type:str='top', epsg:int=None, multi:bool=False, \n                     sv_fov:int=80, sv_pitch:int=10, sv_size:list|tuple=(300,400),\n                     saveImg:bool=True, progressBar:bool=False) -&gt; dict:\n        \"\"\"\n        Chat with the MLLM model for each spatial unit in the shapefile.\n\n        This function loops through all units (e.g., buildings or parcels) in `self.units`, \n        generates top and/or street view images, and prompts a language model \n        with custom messages. It stores results in `self.results`.\n\n        When finished, your self.results object looks like this:\n        ```python\n        {\n            'from_loopUnitChat': {\n                'lon': [...],\n                'lat': [...],\n                'top_view': [[QnA, QnA, ...], ...],      # Optional\n                'street_view': [[QnA, QnA, ...], ...],   # Optional\n            },\n            'base64_imgs': {\n                'top_view_base64': [...],      # Optional\n                'street_view_base64': [...],   # Optional\n            }\n        }\n        ```\n\n        Example prompt:\n            prompt = {\n                \"top\": \"\n                    Is there any damage on the roof?\n                \",\n                \"street\": \"\n                    Is the wall missing or damaged? \n                    Is the yard maintained well?\n                \"\n            }\n\n        Args:\n            model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n            system (str, optional): System message to guide the LLM behavior.\n            prompt (dict): Dictionary containing the prompts for 'top' and/or 'street' views.\n            temp (float, optional): Temperature for generation randomness. Defaults to 0.0.\n            top_k (float, optional): Top-k sampling parameter. Defaults to 0.8.\n            top_p (float, optional): Top-p sampling parameter. Defaults to 0.8.\n            type (str, optional): Which image type(s) to use: \"top\", \"street\", or \"both\". Defaults to \"top\".\n            epsg (int, optional): EPSG code for coordinate transformation. Required if type includes \"street\".\n            multi (bool, optional): Whether to return multiple SVIs per unit. Defaults to False.\n            sv_fov (int, optional): Field of view for street view. Defaults to 80.\n            sv_pitch (int, optional): Pitch angle for street view. Defaults to 10.\n            sv_size (list, tuple, optional): Size (height, width) for street view images. Defaults to (300, 400).\n            saveImg (bool, optional): Whether to save images (as base64 strings) in output. Defaults to True.\n            progressBar (bool, optional): Whether to show progress bar. Defaults to False.\n\n        Returns:\n            dict: A dictionary containing prompts, responses, and (optionally) image data for each unit.\n        \"\"\"\n\n        if model not in ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']:\n            raise Exception(f'{model} is not supported')\n        self.preload_model(model)\n\n        from tqdm import tqdm\n\n        if type == 'top' and 'top' not in prompt:\n            return \"Please provide prompt for top view images when type='top'\"\n        if type == 'street' and 'street' not in prompt:\n            return \"Please provide prompt for street view images when type='street'\"\n        if type == 'both' and 'top' not in prompt and 'street' not in prompt:\n            return \"Please provide prompt for both top and street view images when type='both'\"\n\n        dic = {\n            \"lon\": [],\n            \"lat\": [],\n        }\n\n        top_view_imgs = {'top_view_base64':[]}\n        street_view_imgs = {'street_view_base64':[]}\n\n        for i in tqdm(range(len(self.units)), desc=\"Processing...\", ncols=75, disable=progressBar):\n        # for i in range(len(self.units)):\n            # Get the extent of one polygon from the filtered GeoDataFrame\n            polygon = self.units.geometry.iloc[i]\n            centroid = polygon.centroid\n\n            dic['lon'].append(centroid.x)\n            dic['lat'].append(centroid.y)\n\n            # process street view image\n            if (type == 'street' or type == 'both') and epsg != None and self.mapillary_key != None:\n                input_svis = getSV(centroid, epsg, self.mapillary_key, multi=multi, \n                                   fov=sv_fov, pitch=sv_pitch, height=sv_size[0], width=sv_size[1])\n                if len(input_svis) != 0:\n                    # save imgs\n                    if saveImg:\n                        street_view_imgs['street_view_base64'] += [input_svis]\n                    # inference\n                    res = self.LLM_chat(model=model,\n                                        system=system, \n                                        prompt=prompt[\"street\"], \n                                        img=input_svis, \n                                        temp=temp, \n                                        top_k=top_k, \n                                        top_p=top_p)\n                    # initialize the list\n                    if i == 0:\n                        dic['street_view'] = []\n                    if multi:\n                        dic['street_view'] += [res]\n                    else:\n                        dic['street_view'] += [res.responses]\n                else:\n                    dic['lon'].pop()\n                    dic['lat'].pop()\n                    continue\n\n            # process aerial image\n            if type == 'top' or type == 'both':\n                # Convert meters to degrees dynamically based on latitude\n                # Approximate adjustment (5 meters)\n                degree_offset = meters_to_degrees(5, centroid.y)  # Convert 5m to degrees\n                polygon = polygon.buffer(degree_offset)\n                # Compute bounding box\n                minx, miny, maxx, maxy = polygon.bounds\n                bbox = [minx, miny, maxx, maxy]\n\n                # Create a temporary file\n                with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as temp_file:\n                    image = temp_file.name\n                # Download data using tms_to_geotiff\n                tms_to_geotiff(output=image, bbox=bbox, zoom=22, \n                               source=\"SATELLITE\", \n                               overwrite=True)\n                # Clip the image with the polygon\n                with rasterio.open(image) as src:\n                    # Reproject the polygon back to match raster CRS\n                    polygon = self.units.to_crs(src.crs).geometry.iloc[i]\n                    out_image, out_transform = mask(src, [polygon], crop=True)\n                    out_meta = src.meta.copy()\n\n                out_meta.update({\n                    \"driver\": \"JPEG\",\n                    \"height\": out_image.shape[1],\n                    \"width\": out_image.shape[2],\n                    \"transform\": out_transform,\n                    \"count\": 3\n                })\n\n                # Create a temporary file for the clipped JPEG\n                with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as temp_jpg:\n                    clipped_image = temp_jpg.name\n                with rasterio.open(clipped_image, \"w\", **out_meta) as dest:\n                    dest.write(out_image)\n                # clean up temp file\n                os.remove(image)\n\n                # convert image into base64\n                clipped_image_base64 = encode_image_to_base64(clipped_image)\n                top_view_imgs['top_view_base64'] += [clipped_image_base64]\n\n                # process aerial image\n                top_res = self.LLM_chat(model=model,\n                                    system=system, \n                                    prompt=prompt[\"top\"], \n                                    img=[clipped_image], \n                                    temp=temp, \n                                    top_k=top_k, \n                                    top_p=top_p)\n                # initialize the list\n                if i == 0:\n                    dic['top_view'] = []\n                if saveImg:\n                    dic['top_view'].append(top_res.responses)\n\n                # clean up temp file\n                os.remove(clipped_image)\n\n        self.results = {'from_loopUnitChat':dic, 'base64_imgs':{**top_view_imgs, **street_view_imgs}}\n        return dic\n\n    def to_gdf(self) -&gt; gpd.GeoDataFrame | str:\n        \"\"\"\n        Convert the output from an MLLM response (from .loopUnitChat) into a GeoDataFrame.\n\n        This method extracts coordinates, questions, responses, and base64-encoded input images\n        from the stored `self.results` object, and formats them into a structured GeoDataFrame.\n\n        Returns:\n            gpd.GeoDataFrame: A GeoDataFrame containing spatial responses and associated metadata.\n            str: An error message if `.loopUnitChat()` has not been run or if the format is unsupported.\n        \"\"\"\n\n        import pandas as pd\n        import copy\n\n        if self.results is not None:\n            if 'from_loopUnitChat' in self.results:\n                res_df = response2gdf(self.results['from_loopUnitChat'])\n                img_dic = copy.deepcopy(self.results['base64_imgs'])\n                if img_dic['top_view_base64'] != [] or img_dic['street_view_base64'] != []:\n                    if img_dic['top_view_base64'] == []:\n                        img_dic.pop(\"top_view_base64\")\n                    if img_dic['street_view_base64'] == []:\n                        img_dic.pop(\"street_view_base64\")\n                    imgs_df = pd.DataFrame(img_dic)\n                    return pd.concat([res_df, imgs_df], axis=1)\n                else:\n                    return res_df\n            else:\n                return \"This method can only support the output of '.loopUnitChat()' method\"\n        else:\n            return \"This method can only be called after running the '.loopUnitChat()' method\"\n\n    def LLM_chat(self, model:str='llama3.2-vision', system:str=None, prompt:str=None, \n                 img:list[str]=None, temp:float=None, top_k:float=None, top_p:float=None) -&gt; Union[\"Response\", list[\"QnA\"]]:\n        '''\n        Chat with the LLM model with a list of images.\n\n        Depending on the number of images provided, the method will:\n        - Return a single Response object if only one image is provided.\n        - Return a list of QnA objects if multiple images are provided (e.g., aerial and street views).\n\n        Args:\n            model (str): Model name.\n            system (str): The system message guiding the LLM.\n            prompt (str): The user prompt to the LLM.\n            img (list[str]): A list of image paths (1 or 3 recommended).\n            temp (float, optional): Temperature parameter for response randomness.\n            top_k (float, optional): Top-K sampling filter.\n            top_p (float, optional): Top-P (nucleus) sampling filter.\n\n        Returns:\n            Union[Response, list[QnA]]: A Response object if a single reply is generated,\n            or a list of QnA objects for multi-turn/image-question responses.\n        '''\n\n        if prompt != None and img != None:\n            if len(img) == 1:\n                return self.chat(model, system, prompt, img[0], temp, top_k, top_p)\n            elif len(img) == 3:\n                res = []\n                system = f'You are analyzing aerial or street view images. For street view, you should just foucus on the building and yard in the middle. {system}'\n                for i in range(len(img)):\n                    r = self.chat(model, system, prompt, img[i], temp, top_k, top_p)\n                    res += [r.responses]\n                return res\n\n    def chat(self, model:str='llama3.2-vision', system:str=None, prompt:str=None, \n             img=None, temp=None, top_k:float=None, top_p:float=None) -&gt; Response:\n        '''\n        Chat with the LLM model using a system message, prompt, and optional image.\n\n        Args:\n            model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n            system (str): The system-level instruction for the model.\n            prompt (str): The user message or question.\n            img (str): Path to a single image to be sent to the model.\n            temp (float, optional): Sampling temperature for generation (higher = more random).\n            top_k (float, optional): Top-k sampling parameter.\n            top_p (float, optional): Top-p (nucleus) sampling parameter.\n\n        Returns:\n            Response: Parsed response from the LLM, returned as a `Response` object.\n        '''\n\n        res = ollama.chat(\n            model=model,\n            format=self.format.model_json_schema(),\n            messages=[\n                {\n                    'role': 'system',\n                    'content': system\n                },\n                {\n                    'role': 'user',\n                    'content': prompt,\n                    'images': [img]\n                }\n            ],\n            options={\n                \"temperature\":temp,\n                \"top_k\":top_k,\n                \"top_p\":top_p\n            }\n        )\n        return self.format.model_validate_json(res.message.content)\n\n    # def generate(self, system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None) -&gt; Response:\n    #     '''\n    #     Chat with the LLM model using a system message, prompt, and optional image.\n\n    #     Args:\n    #         system (str): The system-level instruction for the model.\n    #         prompt (str): The user message or question.\n    #         img (str): Path to a single image to be sent to the model.\n    #         temp (float, optional): Sampling temperature for generation (higher = more random).\n    #         top_k (float, optional): Top-k sampling parameter.\n    #         top_p (float, optional): Top-p (nucleus) sampling parameter.\n\n    #     Returns:\n    #         Response: Parsed response from the LLM, returned as a `Response` object.\n    #     '''\n    #     res = ollama.generate(\n    #         model='llama3.2-vision',\n    #         format=self.format.model_json_schema(),\n    #         system=system,\n    #         prompt=prompt,\n    #         images=[img],\n    #         options={\n    #             \"temperature\":temp,\n    #             \"top_k\":top_k,\n    #             \"top_p\":top_p\n    #         }\n    #     )\n    #     return self.format.model_validate_json(res.response)\n\n    def plotBase64(self, img:str):\n        '''\n        plot a single base64 image\n\n        Args:\n            img (str): image base64 string\n        '''\n        plot_base64_image(img)\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.LLM_chat","title":"<code>LLM_chat(model='llama3.2-vision', system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None)</code>","text":"<p>Chat with the LLM model with a list of images.</p> <p>Depending on the number of images provided, the method will: - Return a single Response object if only one image is provided. - Return a list of QnA objects if multiple images are provided (e.g., aerial and street views).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name.</p> <code>'llama3.2-vision'</code> <code>system</code> <code>str</code> <p>The system message guiding the LLM.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The user prompt to the LLM.</p> <code>None</code> <code>img</code> <code>list[str]</code> <p>A list of image paths (1 or 3 recommended).</p> <code>None</code> <code>temp</code> <code>float</code> <p>Temperature parameter for response randomness.</p> <code>None</code> <code>top_k</code> <code>float</code> <p>Top-K sampling filter.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>Top-P (nucleus) sampling filter.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Response, list[QnA]]</code> <p>Union[Response, list[QnA]]: A Response object if a single reply is generated,</p> <code>Union[Response, list[QnA]]</code> <p>or a list of QnA objects for multi-turn/image-question responses.</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def LLM_chat(self, model:str='llama3.2-vision', system:str=None, prompt:str=None, \n             img:list[str]=None, temp:float=None, top_k:float=None, top_p:float=None) -&gt; Union[\"Response\", list[\"QnA\"]]:\n    '''\n    Chat with the LLM model with a list of images.\n\n    Depending on the number of images provided, the method will:\n    - Return a single Response object if only one image is provided.\n    - Return a list of QnA objects if multiple images are provided (e.g., aerial and street views).\n\n    Args:\n        model (str): Model name.\n        system (str): The system message guiding the LLM.\n        prompt (str): The user prompt to the LLM.\n        img (list[str]): A list of image paths (1 or 3 recommended).\n        temp (float, optional): Temperature parameter for response randomness.\n        top_k (float, optional): Top-K sampling filter.\n        top_p (float, optional): Top-P (nucleus) sampling filter.\n\n    Returns:\n        Union[Response, list[QnA]]: A Response object if a single reply is generated,\n        or a list of QnA objects for multi-turn/image-question responses.\n    '''\n\n    if prompt != None and img != None:\n        if len(img) == 1:\n            return self.chat(model, system, prompt, img[0], temp, top_k, top_p)\n        elif len(img) == 3:\n            res = []\n            system = f'You are analyzing aerial or street view images. For street view, you should just foucus on the building and yard in the middle. {system}'\n            for i in range(len(img)):\n                r = self.chat(model, system, prompt, img[i], temp, top_k, top_p)\n                res += [r.responses]\n            return res\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.__init__","title":"<code>__init__(image=None, images=None, units=None, format=None, mapillary_key=None, random_sample=None)</code>","text":"<p>Add data or api key</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image.</p> <code>None</code> <code>images</code> <code>list</code> <p>The list of image paths.</p> <code>None</code> <code>units</code> <code>str</code> <p>The path to the shapefile.</p> <code>None</code> <code>format</code> <code>Response</code> <p>The response format.</p> <code>None</code> <code>mapillary_key</code> <code>str</code> <p>The Mapillary API key.</p> <code>None</code> <code>random_sample</code> <code>int</code> <p>The number of random samples.</p> <code>None</code> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def __init__(self, image=None, images:list=None, units:str=None, \n             format:Response=None, mapillary_key:int=None, random_sample:int=None):\n    '''\n    Add data or api key\n\n    Args:\n        image (str): The path to the image.\n        images (list): The list of image paths.\n        units (str): The path to the shapefile.\n        format (Response): The response format.\n        mapillary_key (str): The Mapillary API key.\n        random_sample (int): The number of random samples.\n    '''\n\n    if image != None and detect_input_type(image) == 'image_path':\n        self.img = encode_image_to_base64(image)\n    else:\n        self.img = image\n\n    if images != None and detect_input_type(images[0]) == 'image_path':\n        self.imgs = [encode_image_to_base64(im) for im in images]\n    else:\n        self.imgs = images\n\n    if random_sample != None and units != None:\n        self.units = loadSHP(units).sample(random_sample)\n    elif random_sample == None and units != None:\n        self.units = loadSHP(units)\n    else:\n        self.units = units\n\n    if format == None:\n        self.format = Response()\n    else:\n        self.format = format\n\n    self.mapillary_key = mapillary_key\n\n    self.results = None\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.bbox2Buildings","title":"<code>bbox2Buildings(bbox, source='osm', min_area=0, max_area=None, random_sample=None)</code>","text":"<p>Extract buildings from OpenStreetMap using the bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list or tuple</code> <p>The bounding box.</p> required <code>source</code> <code>str</code> <p>The source of the buildings. ['osm', 'being']</p> <code>'osm'</code> <code>min_area</code> <code>float or int</code> <p>The minimum area.</p> <code>0</code> <code>max_area</code> <code>float or int</code> <p>The maximum area.</p> <code>None</code> <code>random_sample</code> <code>int</code> <p>The number of random samples.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The number of buildings found in the bounding box</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def bbox2Buildings(self, bbox:list|tuple, source:str='osm', \n                   min_area:float|int=0, max_area:float|int=None, \n                   random_sample:int=None) -&gt; str:\n    '''\n    Extract buildings from OpenStreetMap using the bbox.\n\n    Args:\n        bbox (list or tuple): The bounding box.\n        source (str): The source of the buildings. ['osm', 'being']\n        min_area (float or int): The minimum area.\n        max_area (float or int): The maximum area.\n        random_sample (int): The number of random samples.\n\n    Returns:\n        str: The number of buildings found in the bounding box\n    '''\n\n    if source not in ['osm', 'being']:\n        raise Exception(f'{source} is not supported')\n\n    if source == 'osm':\n        buildings = getOSMbuildings(bbox, min_area, max_area)\n    elif source == 'being':\n        buildings = getGlobalMLBuilding(bbox, min_area, max_area)\n    if buildings is None or buildings.empty:\n        if source == 'osm':\n            return \"No buildings found in the bounding box. Please check https://overpass-turbo.eu/ for areas with buildings.\"\n        if source == 'being':\n            return \"No buildings found in the bounding box. Please check https://github.com/microsoft/GlobalMLBuildingFootprints for areas with buildings.\"\n    if random_sample != None:\n        buildings = buildings.sample(random_sample)\n    self.units = buildings\n    return f\"{len(buildings)} buildings found in the bounding box.\"\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.chat","title":"<code>chat(model='llama3.2-vision', system=None, prompt=None, img=None, temp=None, top_k=None, top_p=None)</code>","text":"<p>Chat with the LLM model using a system message, prompt, and optional image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']</p> <code>'llama3.2-vision'</code> <code>system</code> <code>str</code> <p>The system-level instruction for the model.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The user message or question.</p> <code>None</code> <code>img</code> <code>str</code> <p>Path to a single image to be sent to the model.</p> <code>None</code> <code>temp</code> <code>float</code> <p>Sampling temperature for generation (higher = more random).</p> <code>None</code> <code>top_k</code> <code>float</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>Top-p (nucleus) sampling parameter.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Parsed response from the LLM, returned as a <code>Response</code> object.</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def chat(self, model:str='llama3.2-vision', system:str=None, prompt:str=None, \n         img=None, temp=None, top_k:float=None, top_p:float=None) -&gt; Response:\n    '''\n    Chat with the LLM model using a system message, prompt, and optional image.\n\n    Args:\n        model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n        system (str): The system-level instruction for the model.\n        prompt (str): The user message or question.\n        img (str): Path to a single image to be sent to the model.\n        temp (float, optional): Sampling temperature for generation (higher = more random).\n        top_k (float, optional): Top-k sampling parameter.\n        top_p (float, optional): Top-p (nucleus) sampling parameter.\n\n    Returns:\n        Response: Parsed response from the LLM, returned as a `Response` object.\n    '''\n\n    res = ollama.chat(\n        model=model,\n        format=self.format.model_json_schema(),\n        messages=[\n            {\n                'role': 'system',\n                'content': system\n            },\n            {\n                'role': 'user',\n                'content': prompt,\n                'images': [img]\n            }\n        ],\n        options={\n            \"temperature\":temp,\n            \"top_k\":top_k,\n            \"top_p\":top_p\n        }\n    )\n    return self.format.model_validate_json(res.message.content)\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.loopImgChat","title":"<code>loopImgChat(model='llama3.2-vision', system=None, prompt=None, temp=0.0, top_k=0.8, top_p=0.8, saveImg=True, progressBar=False)</code>","text":"<p>Chat with MLLM model for each image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']</p> <code>'llama3.2-vision'</code> <code>system</code> <code>(str, optinal)</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The prompt message.</p> <code>None</code> <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> <code>saveImg</code> <code>bool</code> <p>The saveImg for save each image in base64 format in the output.</p> <code>True</code> <code>progressBar</code> <code>bool</code> <p>The progress bar for showing the progress of data analysis over the units</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>list A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def loopImgChat(self, model:str='llama3.2-vision', system:str=None, prompt:str=None, \n                temp:float=0.0, top_k:float=0.8, top_p:float=0.8, saveImg:bool=True, \n                progressBar:bool=False) -&gt; list:\n    '''\n    Chat with MLLM model for each image.\n\n    Args:\n        model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n        system (str, optinal): The system message.\n        prompt (str): The prompt message.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n        saveImg (bool): The saveImg for save each image in base64 format in the output.\n        progressBar (bool): The progress bar for showing the progress of data analysis over the units\n\n    Returns:\n        list A list of dictionaries. Each dict includes questions/messages, responses/answers, and image base64 (if required)\n    '''\n\n    if model not in ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']:\n        raise Exception(f'{model} is not supported')\n    self.preload_model(model)\n\n    from tqdm import tqdm\n\n    res = []\n    for i in tqdm(range(len(self.imgs)), desc=\"Processing...\", ncols=75, disable=progressBar):\n    # for i in range(len(self.imgs)):\n        img = self.imgs[i]\n        r = self.LLM_chat(model=model, system=system, prompt=prompt, img=[img], \n                          temp=temp, top_k=top_k, top_p=top_p)\n        r = dict(r.responses[0])\n        if saveImg:\n            im = {'img': img}\n            res += [{**r, **im}]\n        else:\n            res += [r]\n    return res\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.loopUnitChat","title":"<code>loopUnitChat(model='llama3.2-vision', system=None, prompt=None, temp=0.0, top_k=0.8, top_p=0.8, type='top', epsg=None, multi=False, sv_fov=80, sv_pitch=10, sv_size=(300, 400), saveImg=True, progressBar=False)</code>","text":"<p>Chat with the MLLM model for each spatial unit in the shapefile.</p> <p>This function loops through all units (e.g., buildings or parcels) in <code>self.units</code>,  generates top and/or street view images, and prompts a language model  with custom messages. It stores results in <code>self.results</code>.</p> <p>When finished, your self.results object looks like this:</p> <pre><code>{\n    'from_loopUnitChat': {\n        'lon': [...],\n        'lat': [...],\n        'top_view': [[QnA, QnA, ...], ...],      # Optional\n        'street_view': [[QnA, QnA, ...], ...],   # Optional\n    },\n    'base64_imgs': {\n        'top_view_base64': [...],      # Optional\n        'street_view_base64': [...],   # Optional\n    }\n}\n</code></pre> Example prompt <p>prompt = {     \"top\": \"         Is there any damage on the roof?     \",     \"street\": \"         Is the wall missing or damaged?          Is the yard maintained well?     \" }</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']</p> <code>'llama3.2-vision'</code> <code>system</code> <code>str</code> <p>System message to guide the LLM behavior.</p> <code>None</code> <code>prompt</code> <code>dict</code> <p>Dictionary containing the prompts for 'top' and/or 'street' views.</p> <code>None</code> <code>temp</code> <code>float</code> <p>Temperature for generation randomness. Defaults to 0.0.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>Top-k sampling parameter. Defaults to 0.8.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 0.8.</p> <code>0.8</code> <code>type</code> <code>str</code> <p>Which image type(s) to use: \"top\", \"street\", or \"both\". Defaults to \"top\".</p> <code>'top'</code> <code>epsg</code> <code>int</code> <p>EPSG code for coordinate transformation. Required if type includes \"street\".</p> <code>None</code> <code>multi</code> <code>bool</code> <p>Whether to return multiple SVIs per unit. Defaults to False.</p> <code>False</code> <code>sv_fov</code> <code>int</code> <p>Field of view for street view. Defaults to 80.</p> <code>80</code> <code>sv_pitch</code> <code>int</code> <p>Pitch angle for street view. Defaults to 10.</p> <code>10</code> <code>sv_size</code> <code>(list, tuple)</code> <p>Size (height, width) for street view images. Defaults to (300, 400).</p> <code>(300, 400)</code> <code>saveImg</code> <code>bool</code> <p>Whether to save images (as base64 strings) in output. Defaults to True.</p> <code>True</code> <code>progressBar</code> <code>bool</code> <p>Whether to show progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing prompts, responses, and (optionally) image data for each unit.</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def loopUnitChat(self, model:str='llama3.2-vision', system:str=None, prompt:dict=None, \n                 temp:float=0.0, top_k:float=0.8, top_p:float=0.8, \n                 type:str='top', epsg:int=None, multi:bool=False, \n                 sv_fov:int=80, sv_pitch:int=10, sv_size:list|tuple=(300,400),\n                 saveImg:bool=True, progressBar:bool=False) -&gt; dict:\n    \"\"\"\n    Chat with the MLLM model for each spatial unit in the shapefile.\n\n    This function loops through all units (e.g., buildings or parcels) in `self.units`, \n    generates top and/or street view images, and prompts a language model \n    with custom messages. It stores results in `self.results`.\n\n    When finished, your self.results object looks like this:\n    ```python\n    {\n        'from_loopUnitChat': {\n            'lon': [...],\n            'lat': [...],\n            'top_view': [[QnA, QnA, ...], ...],      # Optional\n            'street_view': [[QnA, QnA, ...], ...],   # Optional\n        },\n        'base64_imgs': {\n            'top_view_base64': [...],      # Optional\n            'street_view_base64': [...],   # Optional\n        }\n    }\n    ```\n\n    Example prompt:\n        prompt = {\n            \"top\": \"\n                Is there any damage on the roof?\n            \",\n            \"street\": \"\n                Is the wall missing or damaged? \n                Is the yard maintained well?\n            \"\n        }\n\n    Args:\n        model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n        system (str, optional): System message to guide the LLM behavior.\n        prompt (dict): Dictionary containing the prompts for 'top' and/or 'street' views.\n        temp (float, optional): Temperature for generation randomness. Defaults to 0.0.\n        top_k (float, optional): Top-k sampling parameter. Defaults to 0.8.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 0.8.\n        type (str, optional): Which image type(s) to use: \"top\", \"street\", or \"both\". Defaults to \"top\".\n        epsg (int, optional): EPSG code for coordinate transformation. Required if type includes \"street\".\n        multi (bool, optional): Whether to return multiple SVIs per unit. Defaults to False.\n        sv_fov (int, optional): Field of view for street view. Defaults to 80.\n        sv_pitch (int, optional): Pitch angle for street view. Defaults to 10.\n        sv_size (list, tuple, optional): Size (height, width) for street view images. Defaults to (300, 400).\n        saveImg (bool, optional): Whether to save images (as base64 strings) in output. Defaults to True.\n        progressBar (bool, optional): Whether to show progress bar. Defaults to False.\n\n    Returns:\n        dict: A dictionary containing prompts, responses, and (optionally) image data for each unit.\n    \"\"\"\n\n    if model not in ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']:\n        raise Exception(f'{model} is not supported')\n    self.preload_model(model)\n\n    from tqdm import tqdm\n\n    if type == 'top' and 'top' not in prompt:\n        return \"Please provide prompt for top view images when type='top'\"\n    if type == 'street' and 'street' not in prompt:\n        return \"Please provide prompt for street view images when type='street'\"\n    if type == 'both' and 'top' not in prompt and 'street' not in prompt:\n        return \"Please provide prompt for both top and street view images when type='both'\"\n\n    dic = {\n        \"lon\": [],\n        \"lat\": [],\n    }\n\n    top_view_imgs = {'top_view_base64':[]}\n    street_view_imgs = {'street_view_base64':[]}\n\n    for i in tqdm(range(len(self.units)), desc=\"Processing...\", ncols=75, disable=progressBar):\n    # for i in range(len(self.units)):\n        # Get the extent of one polygon from the filtered GeoDataFrame\n        polygon = self.units.geometry.iloc[i]\n        centroid = polygon.centroid\n\n        dic['lon'].append(centroid.x)\n        dic['lat'].append(centroid.y)\n\n        # process street view image\n        if (type == 'street' or type == 'both') and epsg != None and self.mapillary_key != None:\n            input_svis = getSV(centroid, epsg, self.mapillary_key, multi=multi, \n                               fov=sv_fov, pitch=sv_pitch, height=sv_size[0], width=sv_size[1])\n            if len(input_svis) != 0:\n                # save imgs\n                if saveImg:\n                    street_view_imgs['street_view_base64'] += [input_svis]\n                # inference\n                res = self.LLM_chat(model=model,\n                                    system=system, \n                                    prompt=prompt[\"street\"], \n                                    img=input_svis, \n                                    temp=temp, \n                                    top_k=top_k, \n                                    top_p=top_p)\n                # initialize the list\n                if i == 0:\n                    dic['street_view'] = []\n                if multi:\n                    dic['street_view'] += [res]\n                else:\n                    dic['street_view'] += [res.responses]\n            else:\n                dic['lon'].pop()\n                dic['lat'].pop()\n                continue\n\n        # process aerial image\n        if type == 'top' or type == 'both':\n            # Convert meters to degrees dynamically based on latitude\n            # Approximate adjustment (5 meters)\n            degree_offset = meters_to_degrees(5, centroid.y)  # Convert 5m to degrees\n            polygon = polygon.buffer(degree_offset)\n            # Compute bounding box\n            minx, miny, maxx, maxy = polygon.bounds\n            bbox = [minx, miny, maxx, maxy]\n\n            # Create a temporary file\n            with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as temp_file:\n                image = temp_file.name\n            # Download data using tms_to_geotiff\n            tms_to_geotiff(output=image, bbox=bbox, zoom=22, \n                           source=\"SATELLITE\", \n                           overwrite=True)\n            # Clip the image with the polygon\n            with rasterio.open(image) as src:\n                # Reproject the polygon back to match raster CRS\n                polygon = self.units.to_crs(src.crs).geometry.iloc[i]\n                out_image, out_transform = mask(src, [polygon], crop=True)\n                out_meta = src.meta.copy()\n\n            out_meta.update({\n                \"driver\": \"JPEG\",\n                \"height\": out_image.shape[1],\n                \"width\": out_image.shape[2],\n                \"transform\": out_transform,\n                \"count\": 3\n            })\n\n            # Create a temporary file for the clipped JPEG\n            with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as temp_jpg:\n                clipped_image = temp_jpg.name\n            with rasterio.open(clipped_image, \"w\", **out_meta) as dest:\n                dest.write(out_image)\n            # clean up temp file\n            os.remove(image)\n\n            # convert image into base64\n            clipped_image_base64 = encode_image_to_base64(clipped_image)\n            top_view_imgs['top_view_base64'] += [clipped_image_base64]\n\n            # process aerial image\n            top_res = self.LLM_chat(model=model,\n                                system=system, \n                                prompt=prompt[\"top\"], \n                                img=[clipped_image], \n                                temp=temp, \n                                top_k=top_k, \n                                top_p=top_p)\n            # initialize the list\n            if i == 0:\n                dic['top_view'] = []\n            if saveImg:\n                dic['top_view'].append(top_res.responses)\n\n            # clean up temp file\n            os.remove(clipped_image)\n\n    self.results = {'from_loopUnitChat':dic, 'base64_imgs':{**top_view_imgs, **street_view_imgs}}\n    return dic\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.oneImgChat","title":"<code>oneImgChat(model='llama3.2-vision', system=None, prompt=None, temp=0.0, top_k=0.8, top_p=0.8, saveImg=True)</code>","text":"<p>Chat with MLLM model with one image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']</p> <code>'llama3.2-vision'</code> <code>system</code> <code>optinal</code> <p>The system message.</p> <code>None</code> <code>prompt</code> <code>str</code> <p>The prompt message.</p> <code>None</code> <code>img</code> <code>str</code> <p>The image path.</p> required <code>temp</code> <code>float</code> <p>The temperature value.</p> <code>0.0</code> <code>top_k</code> <code>float</code> <p>The top_k value.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top_p value.</p> <code>0.8</code> <code>saveImg</code> <code>bool</code> <p>The saveImg for save each image in base64 format in the output.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary includes questions/messages, responses/answers, and image base64 (if required)</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def oneImgChat(self, model:str='llama3.2-vision',system:str=None, prompt:str=None, \n               temp:float=0.0, top_k:float=0.8, top_p:float=0.8,\n               saveImg:bool=True) -&gt; dict:\n\n    '''\n    Chat with MLLM model with one image.\n\n    Args:\n        model (str): Model name. Defaults to \"llama3.2-vision\". ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']\n        system (optinal): The system message.\n        prompt (str): The prompt message.\n        img (str): The image path.\n        temp (float): The temperature value.\n        top_k (float): The top_k value.\n        top_p (float): The top_p value.\n        saveImg (bool): The saveImg for save each image in base64 format in the output.\n\n    Returns:\n        dict: A dictionary includes questions/messages, responses/answers, and image base64 (if required) \n    '''\n\n    if model not in ['granite3.2-vision', 'llama3.2-vision', 'gemma3', 'gemma3:1b', 'gemma3:12b', 'minicpm-v']:\n        raise Exception(f'{model} is not supported')\n    self.preload_model(model)\n\n    print(\"Inference starts ...\")\n    r = self.LLM_chat(model=model, system=system, prompt=prompt, img=[self.img], \n                      temp=temp, top_k=top_k, top_p=top_p)\n    r = dict(r.responses[0])\n    if saveImg:\n        r['img'] = self.img\n    return r\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.plotBase64","title":"<code>plotBase64(img)</code>","text":"<p>plot a single base64 image</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str</code> <p>image base64 string</p> required Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def plotBase64(self, img:str):\n    '''\n    plot a single base64 image\n\n    Args:\n        img (str): image base64 string\n    '''\n    plot_base64_image(img)\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.preload_model","title":"<code>preload_model(model_name)</code>","text":"<p>Ensures that the required Ollama model is available. If not, it automatically pulls the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>model name</p> required Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def preload_model(self, model_name):\n    \"\"\"\n    Ensures that the required Ollama model is available.\n    If not, it automatically pulls the model.\n\n    Args:\n        model_name (str): model name\n    \"\"\"\n    import ollama\n\n    try:\n        ollama.pull(model_name)\n\n    except Exception as e:\n        print(f\"Warning: Ollama is not installed or failed to check models: {e}\")\n        print(\"Please install Ollama client: https://github.com/ollama/ollama/tree/main\")\n        raise RuntimeError(\"Ollama not available. Install it before running.\")\n</code></pre>"},{"location":"API_Reference/UrbanDataSet/#urbanworm.UrbanDataSet.UrbanDataSet.to_gdf","title":"<code>to_gdf()</code>","text":"<p>Convert the output from an MLLM response (from .loopUnitChat) into a GeoDataFrame.</p> <p>This method extracts coordinates, questions, responses, and base64-encoded input images from the stored <code>self.results</code> object, and formats them into a structured GeoDataFrame.</p> <p>Returns:</p> Name Type Description <code>GeoDataFrame | str</code> <p>gpd.GeoDataFrame: A GeoDataFrame containing spatial responses and associated metadata.</p> <code>str</code> <code>GeoDataFrame | str</code> <p>An error message if <code>.loopUnitChat()</code> has not been run or if the format is unsupported.</p> Source code in <code>urbanworm/UrbanDataSet.py</code> <pre><code>def to_gdf(self) -&gt; gpd.GeoDataFrame | str:\n    \"\"\"\n    Convert the output from an MLLM response (from .loopUnitChat) into a GeoDataFrame.\n\n    This method extracts coordinates, questions, responses, and base64-encoded input images\n    from the stored `self.results` object, and formats them into a structured GeoDataFrame.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame containing spatial responses and associated metadata.\n        str: An error message if `.loopUnitChat()` has not been run or if the format is unsupported.\n    \"\"\"\n\n    import pandas as pd\n    import copy\n\n    if self.results is not None:\n        if 'from_loopUnitChat' in self.results:\n            res_df = response2gdf(self.results['from_loopUnitChat'])\n            img_dic = copy.deepcopy(self.results['base64_imgs'])\n            if img_dic['top_view_base64'] != [] or img_dic['street_view_base64'] != []:\n                if img_dic['top_view_base64'] == []:\n                    img_dic.pop(\"top_view_base64\")\n                if img_dic['street_view_base64'] == []:\n                    img_dic.pop(\"street_view_base64\")\n                imgs_df = pd.DataFrame(img_dic)\n                return pd.concat([res_df, imgs_df], axis=1)\n            else:\n                return res_df\n        else:\n            return \"This method can only support the output of '.loopUnitChat()' method\"\n    else:\n        return \"This method can only be called after running the '.loopUnitChat()' method\"\n</code></pre>"},{"location":"API_Reference/format_creation/","title":"Format creation","text":"<p>Create a generic <code>Response</code> model using a dynamically defined Pydantic schema.</p> <p>This function allows you to define a custom set of fields (e.g., for QnA-style data), and returns a typed <code>Response[CustomQnA]</code> model class.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>dict</code> <p>A dictionary of field definitions for the inner model. Example:     fields = {         \"question\": (str, ...),         \"answer\": (str, ...),         \"explanation\": (str, ...),     }</p> required <p>Returns:</p> Type Description <code>Type[Response]</code> <p>Type[Response]: A <code>Response</code> model class parameterized with the dynamically created schema.</p> Source code in <code>urbanworm/format_creation.py</code> <pre><code>def create_format(fields: dict) -&gt; Type[Response]:\n    \"\"\"\n    Create a generic `Response` model using a dynamically defined Pydantic schema.\n\n    This function allows you to define a custom set of fields (e.g., for QnA-style data),\n    and returns a typed `Response[CustomQnA]` model class.\n\n    Args:\n        fields (dict): A dictionary of field definitions for the inner model.\n            Example:\n                fields = {\n                    \"question\": (str, ...),\n                    \"answer\": (str, ...),\n                    \"explanation\": (str, ...),\n                }\n\n    Returns:\n        Type[Response]: A `Response` model class parameterized with the dynamically created schema.\n    \"\"\"\n\n    # Dynamically create the model\n    CustomQnA = schema(fields)\n    return Response[CustomQnA]\n</code></pre>"},{"location":"API_Reference/pano2pers/","title":"Pano2pers","text":"<p>Covert paronoma to perspective</p> Source code in <code>urbanworm/pano2pers.py</code> <pre><code>class Equirectangular:\n    '''\n    Covert paronoma to perspective\n    '''\n\n    def __init__(self, img_path:str=None, img_url:str=None):\n        '''\n        Add image\n\n        Args:\n            img_path (str): Image path\n            img_url (str): Image URL\n        '''\n        if img_path != None:\n            self._img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        elif img_url != None:\n            self._img = self.read_url2img(img_url)\n        [self._height, self._width, _] = self._img.shape\n\n    def read_url2img(self, url:str) -&gt; np.ndarray:\n        '''\n        Read image from a URL\n\n        Args:\n            url (str): Image URL\n\n        Returns: \n            np.ndarray: The image as a NumPy array.\n        '''\n        resp = urlopen(url, timeout=100)\n        image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n        return image\n\n    def GetPerspective(self, FOV:float, THETA:float, PHI:float, height:int, width:int, RADIUS:int = 128) -&gt; str:\n        \"\"\"\n        Convert an equirectangular panorama image to a perspective view.\n\n        This function computes the perspective projection of a 360\u00b0 panorama image \n        based on field of view and view angles, returning the perspective as a \n        base64-encoded PNG image (useful for web/LLM APIs).\n\n        Args:\n            FOV (float): Field of view in degrees.\n            THETA (float): Horizontal viewing angle (left/right), in degrees.\n            PHI (float): Vertical viewing angle (up/down), in degrees.\n            height (int): Height of the output image.\n            width (int): Width of the output image.\n            RADIUS (int, optional): Projection sphere radius. Defaults to 128.\n\n        Returns:\n            str: A base64-encoded PNG string representing the perspective view.\n        \"\"\"\n\n        # THETA is left/right angle, PHI is up/down angle, both in degree\n        equ_h = self._height\n        equ_w = self._width\n        equ_cx = (equ_w - 1) / 2.0\n        equ_cy = (equ_h - 1) / 2.0\n\n        wFOV = FOV\n        hFOV = float(height) / width * wFOV\n\n        c_x = (width - 1) / 2.0\n        c_y = (height - 1) / 2.0\n\n        wangle = (180 - wFOV) / 2.0\n        w_len = 2 * RADIUS * np.sin(np.radians(wFOV / 2.0)) / np.sin(np.radians(wangle))\n        w_interval = w_len / (width - 1)\n\n        hangle = (180 - hFOV) / 2.0\n        h_len = 2 * RADIUS * np.sin(np.radians(hFOV / 2.0)) / np.sin(np.radians(hangle))\n        h_interval = h_len / (height - 1)\n        x_map = np.zeros([height, width], np.float32) + RADIUS\n        y_map = np.tile((np.arange(0, width) - c_x) * w_interval, [height, 1])\n        z_map = -np.tile((np.arange(0, height) - c_y) * h_interval, [width, 1]).T\n        D = np.sqrt(x_map**2 + y_map**2 + z_map**2)\n        # xyz = np.zeros([height, width, 3], np.float)\n        xyz = np.zeros([height, width, 3], np.float32)\n        xyz[:, :, 0] = (RADIUS / D * x_map)[:, :]\n        xyz[:, :, 1] = (RADIUS / D * y_map)[:, :]\n        xyz[:, :, 2] = (RADIUS / D * z_map)[:, :]\n\n        y_axis = np.array([0.0, 1.0, 0.0], np.float32)\n        z_axis = np.array([0.0, 0.0, 1.0], np.float32)\n        [R1, _] = cv2.Rodrigues(z_axis * np.radians(THETA))\n        [R2, _] = cv2.Rodrigues(np.dot(R1, y_axis) * np.radians(-PHI))\n\n        xyz = xyz.reshape([height * width, 3]).T\n        xyz = np.dot(R1, xyz)\n        xyz = np.dot(R2, xyz).T\n        lat = np.arcsin(xyz[:, 2] / RADIUS)\n        # lon = np.zeros([height * width], np.float)\n        lon = np.zeros([height * width], np.float32)\n        theta = np.arctan(xyz[:, 1] / xyz[:, 0])\n        idx1 = xyz[:, 0] &gt; 0\n        idx2 = xyz[:, 1] &gt; 0\n\n        idx3 = ((1 - idx1) * idx2).astype(np.bool_)\n        idx4 = ((1 - idx1) * (1 - idx2)).astype(np.bool_)\n\n        lon[idx1] = theta[idx1]\n        lon[idx3] = theta[idx3] + np.pi\n        lon[idx4] = theta[idx4] - np.pi\n\n        lon = lon.reshape([height, width]) / np.pi * 180\n        lat = -lat.reshape([height, width]) / np.pi * 180\n        lon = lon / 180 * equ_cx + equ_cx\n        lat = lat / 90 * equ_cy + equ_cy\n\n        persp = cv2.remap(self._img, lon.astype(np.float32), lat.astype(np.float32), cv2.INTER_CUBIC, borderMode=cv2.BORDER_WRAP)\n        # Convert for Ollama\n        _, buffer = cv2.imencode('.png', persp)\n        img_base64 = base64.b64encode(buffer).decode('utf-8')\n        return img_base64\n</code></pre>"},{"location":"API_Reference/pano2pers/#urbanworm.pano2pers.Equirectangular.GetPerspective","title":"<code>GetPerspective(FOV, THETA, PHI, height, width, RADIUS=128)</code>","text":"<p>Convert an equirectangular panorama image to a perspective view.</p> <p>This function computes the perspective projection of a 360\u00b0 panorama image  based on field of view and view angles, returning the perspective as a  base64-encoded PNG image (useful for web/LLM APIs).</p> <p>Parameters:</p> Name Type Description Default <code>FOV</code> <code>float</code> <p>Field of view in degrees.</p> required <code>THETA</code> <code>float</code> <p>Horizontal viewing angle (left/right), in degrees.</p> required <code>PHI</code> <code>float</code> <p>Vertical viewing angle (up/down), in degrees.</p> required <code>height</code> <code>int</code> <p>Height of the output image.</p> required <code>width</code> <code>int</code> <p>Width of the output image.</p> required <code>RADIUS</code> <code>int</code> <p>Projection sphere radius. Defaults to 128.</p> <code>128</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A base64-encoded PNG string representing the perspective view.</p> Source code in <code>urbanworm/pano2pers.py</code> <pre><code>def GetPerspective(self, FOV:float, THETA:float, PHI:float, height:int, width:int, RADIUS:int = 128) -&gt; str:\n    \"\"\"\n    Convert an equirectangular panorama image to a perspective view.\n\n    This function computes the perspective projection of a 360\u00b0 panorama image \n    based on field of view and view angles, returning the perspective as a \n    base64-encoded PNG image (useful for web/LLM APIs).\n\n    Args:\n        FOV (float): Field of view in degrees.\n        THETA (float): Horizontal viewing angle (left/right), in degrees.\n        PHI (float): Vertical viewing angle (up/down), in degrees.\n        height (int): Height of the output image.\n        width (int): Width of the output image.\n        RADIUS (int, optional): Projection sphere radius. Defaults to 128.\n\n    Returns:\n        str: A base64-encoded PNG string representing the perspective view.\n    \"\"\"\n\n    # THETA is left/right angle, PHI is up/down angle, both in degree\n    equ_h = self._height\n    equ_w = self._width\n    equ_cx = (equ_w - 1) / 2.0\n    equ_cy = (equ_h - 1) / 2.0\n\n    wFOV = FOV\n    hFOV = float(height) / width * wFOV\n\n    c_x = (width - 1) / 2.0\n    c_y = (height - 1) / 2.0\n\n    wangle = (180 - wFOV) / 2.0\n    w_len = 2 * RADIUS * np.sin(np.radians(wFOV / 2.0)) / np.sin(np.radians(wangle))\n    w_interval = w_len / (width - 1)\n\n    hangle = (180 - hFOV) / 2.0\n    h_len = 2 * RADIUS * np.sin(np.radians(hFOV / 2.0)) / np.sin(np.radians(hangle))\n    h_interval = h_len / (height - 1)\n    x_map = np.zeros([height, width], np.float32) + RADIUS\n    y_map = np.tile((np.arange(0, width) - c_x) * w_interval, [height, 1])\n    z_map = -np.tile((np.arange(0, height) - c_y) * h_interval, [width, 1]).T\n    D = np.sqrt(x_map**2 + y_map**2 + z_map**2)\n    # xyz = np.zeros([height, width, 3], np.float)\n    xyz = np.zeros([height, width, 3], np.float32)\n    xyz[:, :, 0] = (RADIUS / D * x_map)[:, :]\n    xyz[:, :, 1] = (RADIUS / D * y_map)[:, :]\n    xyz[:, :, 2] = (RADIUS / D * z_map)[:, :]\n\n    y_axis = np.array([0.0, 1.0, 0.0], np.float32)\n    z_axis = np.array([0.0, 0.0, 1.0], np.float32)\n    [R1, _] = cv2.Rodrigues(z_axis * np.radians(THETA))\n    [R2, _] = cv2.Rodrigues(np.dot(R1, y_axis) * np.radians(-PHI))\n\n    xyz = xyz.reshape([height * width, 3]).T\n    xyz = np.dot(R1, xyz)\n    xyz = np.dot(R2, xyz).T\n    lat = np.arcsin(xyz[:, 2] / RADIUS)\n    # lon = np.zeros([height * width], np.float)\n    lon = np.zeros([height * width], np.float32)\n    theta = np.arctan(xyz[:, 1] / xyz[:, 0])\n    idx1 = xyz[:, 0] &gt; 0\n    idx2 = xyz[:, 1] &gt; 0\n\n    idx3 = ((1 - idx1) * idx2).astype(np.bool_)\n    idx4 = ((1 - idx1) * (1 - idx2)).astype(np.bool_)\n\n    lon[idx1] = theta[idx1]\n    lon[idx3] = theta[idx3] + np.pi\n    lon[idx4] = theta[idx4] - np.pi\n\n    lon = lon.reshape([height, width]) / np.pi * 180\n    lat = -lat.reshape([height, width]) / np.pi * 180\n    lon = lon / 180 * equ_cx + equ_cx\n    lat = lat / 90 * equ_cy + equ_cy\n\n    persp = cv2.remap(self._img, lon.astype(np.float32), lat.astype(np.float32), cv2.INTER_CUBIC, borderMode=cv2.BORDER_WRAP)\n    # Convert for Ollama\n    _, buffer = cv2.imencode('.png', persp)\n    img_base64 = base64.b64encode(buffer).decode('utf-8')\n    return img_base64\n</code></pre>"},{"location":"API_Reference/pano2pers/#urbanworm.pano2pers.Equirectangular.__init__","title":"<code>__init__(img_path=None, img_url=None)</code>","text":"<p>Add image</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Image path</p> <code>None</code> <code>img_url</code> <code>str</code> <p>Image URL</p> <code>None</code> Source code in <code>urbanworm/pano2pers.py</code> <pre><code>def __init__(self, img_path:str=None, img_url:str=None):\n    '''\n    Add image\n\n    Args:\n        img_path (str): Image path\n        img_url (str): Image URL\n    '''\n    if img_path != None:\n        self._img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    elif img_url != None:\n        self._img = self.read_url2img(img_url)\n    [self._height, self._width, _] = self._img.shape\n</code></pre>"},{"location":"API_Reference/pano2pers/#urbanworm.pano2pers.Equirectangular.read_url2img","title":"<code>read_url2img(url)</code>","text":"<p>Read image from a URL</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Image URL</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The image as a NumPy array.</p> Source code in <code>urbanworm/pano2pers.py</code> <pre><code>def read_url2img(self, url:str) -&gt; np.ndarray:\n    '''\n    Read image from a URL\n\n    Args:\n        url (str): Image URL\n\n    Returns: \n        np.ndarray: The image as a NumPy array.\n    '''\n    resp = urlopen(url, timeout=100)\n    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n    return image\n</code></pre>"},{"location":"API_Reference/utils/","title":"Utils","text":"<p>Retrieve the closest street view image(s) near a coordinate using the Mapillary API.</p> <p>Parameters:</p> Name Type Description Default <code>centroid</code> <p>The coordinates (geometry.centroid of GeoDataFrame)</p> required <code>epsg</code> <code>int</code> <p>EPSG code for projecting the coordinates.</p> required <code>key</code> <code>str</code> <p>Mapillary API access token.</p> required <code>multi</code> <code>bool</code> <p>Whether to return multiple SVIs (default is False).</p> <code>False</code> <code>fov</code> <code>int</code> <p>Field of view in degrees for the perspective image. Defaults to 80.</p> <code>80</code> <code>heading</code> <code>int</code> <p>Camera heading in degrees. If None, it will be computed based on the house orientation.</p> <code>None</code> <code>pitch</code> <code>int</code> <p>Camera pitch angle. Defaults to 10.</p> <code>10</code> <code>height</code> <code>int</code> <p>Height in pixels of the returned image. Defaults to 300.</p> <code>300</code> <code>width</code> <code>int</code> <p>Width in pixels of the returned image. Defaults to 400.</p> <code>400</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of images in base64 format</p> Source code in <code>urbanworm/utils.py</code> <pre><code>def getSV(centroid, epsg:int, key:str, multi:bool=False, \n          fov:int=80, heading:int=None, pitch:int=10, \n          height:int=300, width:int=400) -&gt; list[str]:\n    \"\"\"\n    Retrieve the closest street view image(s) near a coordinate using the Mapillary API.\n\n    Args:\n        centroid: The coordinates (geometry.centroid of GeoDataFrame)\n        epsg (int): EPSG code for projecting the coordinates.\n        key (str): Mapillary API access token.\n        multi (bool, optional): Whether to return multiple SVIs (default is False).\n        fov (int, optional): Field of view in degrees for the perspective image. Defaults to 80.\n        heading (int, optional): Camera heading in degrees. If None, it will be computed based on the house orientation.\n        pitch (int, optional): Camera pitch angle. Defaults to 10.\n        height (int, optional): Height in pixels of the returned image. Defaults to 300.\n        width (int, optional): Width in pixels of the returned image. Defaults to 400.\n\n    Returns:\n        list[str]: A list of images in base64 format\n    \"\"\"\n    bbox = projection(centroid, epsg)\n    url = f\"https://graph.mapillary.com/images?access_token={key}&amp;fields=id,compass_angle,thumb_2048_url,geometry&amp;bbox={bbox}&amp;is_pano=true\"\n    # while not response or 'data' not in response:\n    try:\n        response = requests.get(url).json()\n        # find the closest image\n        response = closest(centroid, response, multi)\n\n        svis = []\n        for i in range(len(response)):\n            # Extract Image ID, Compass Angle, image url, and coordinates\n            img_heading = float(response.iloc[i,1])\n            img_url = response.iloc[i,2]\n            image_lon, image_lat = response.iloc[i,5]\n            if heading == None:\n                # calculate bearing to the house\n                bearing_to_house = calculate_bearing(image_lat, image_lon, centroid.y, centroid.x)\n                relative_heading = (bearing_to_house - img_heading) % 360\n            else:\n                relative_heading = heading\n            # reframe image\n            svi = Equirectangular(img_url=img_url)\n            sv = svi.GetPerspective(fov, relative_heading, pitch, height, width, 128)\n            svis.append(sv)\n        return svis\n    except:\n        return []\n</code></pre> <p>Get building footprints within a bounding box from OpenStreetMap using the Overpass API.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple or list</code> <p>A bounding box in the form (min_lon, min_lat, max_lon, max_lat).</p> required <code>min_area</code> <code>float or int</code> <p>Minimum footprint area in square meters. Defaults to 0.</p> <code>0</code> <code>max_area</code> <code>float or int</code> <p>Maximum footprint area in square meters. If None, no upper limit is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame | None</code> <p>gpd.GeoDataFrame or None: A GeoDataFrame of building footprints if any are found; otherwise, None.</p> Source code in <code>urbanworm/utils.py</code> <pre><code>def getOSMbuildings(bbox:tuple|list, min_area:float|int=0, max_area:float|int=None) -&gt; gpd.GeoDataFrame | None:\n    \"\"\"\n    Get building footprints within a bounding box from OpenStreetMap using the Overpass API.\n\n    Args:\n        bbox (tuple or list): A bounding box in the form (min_lon, min_lat, max_lon, max_lat).\n        min_area (float or int): Minimum footprint area in square meters. Defaults to 0.\n        max_area (float or int, optional): Maximum footprint area in square meters. If None, no upper limit is applied.\n\n    Returns:\n        gpd.GeoDataFrame or None: A GeoDataFrame of building footprints if any are found; otherwise, None.\n    \"\"\"\n    # Extract bounding box coordinates\n    min_lon, min_lat, max_lon, max_lat = bbox\n\n    url = \"https://overpass-api.de/api/interpreter\"\n    query = f\"\"\"\n    [bbox:{max_lat},{max_lon},{min_lat},{min_lon}]\n    [out:json]\n    [timeout:900];\n    (\n        way[\"building\"]({min_lat},{min_lon},{max_lat},{max_lon});\n        relation[\"building\"]({min_lat},{min_lon},{max_lat},{max_lon});\n    );\n    out geom;\n    \"\"\"\n\n    payload = \"data=\" + requests.utils.quote(query)\n    response = requests.post(url, data=payload)\n    data = response.json()\n\n    buildings = []\n    for element in data.get(\"elements\", []):\n        if \"geometry\" in element:\n            coords = [(node[\"lon\"], node[\"lat\"]) for node in element[\"geometry\"]]\n            if len(coords) &gt; 2:  \n                polygon = Polygon(coords)\n                # Approx. conversion to square meters\n                area_m2 = polygon.area * (111320 ** 2)  \n                # Filter buildings by area\n                if area_m2 &gt;= min_area and (max_area is None or area_m2 &lt;= max_area):\n                    buildings.append(polygon)\n\n    if len(buildings) == 0:\n        return None\n    # Convert to GeoDataFrame\n    gdf = gpd.GeoDataFrame(geometry=buildings, crs=\"EPSG:4326\")\n    return gdf\n</code></pre> <p>Fetch building footprints from the Global ML Building dataset within a given bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple or list</code> <p>Bounding box defined as (min_lon, min_lat, max_lon, max_lat).</p> required <code>min_area</code> <code>float or int</code> <p>Minimum building footprint area in square meters. Defaults to 0.0.</p> <code>0.0</code> <code>max_area</code> <code>float or int</code> <p>Maximum building footprint area in square meters. Defaults to None (no upper limit).</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Filtered building footprints within the bounding box.</p> Source code in <code>urbanworm/utils.py</code> <pre><code>def getGlobalMLBuilding(bbox:tuple | list, min_area:float|int=0.0, max_area:float|int=None) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Fetch building footprints from the Global ML Building dataset within a given bounding box.\n\n    Args:\n        bbox (tuple or list): Bounding box defined as (min_lon, min_lat, max_lon, max_lat).\n        min_area (float or int): Minimum building footprint area in square meters. Defaults to 0.0.\n        max_area (float or int, optional): Maximum building footprint area in square meters. Defaults to None (no upper limit).\n\n    Returns:\n        gpd.GeoDataFrame: Filtered building footprints within the bounding box.\n    \"\"\"\n    import mercantile\n    from tqdm import tqdm\n    import tempfile\n    from shapely import geometry\n\n    min_lon, min_lat, max_lon, max_lat = bbox\n    aoi_geom = {\n        \"coordinates\": [\n            [\n                [min_lon, min_lat],\n                [min_lon, max_lat],\n                [max_lon, max_lat],\n                [max_lon, min_lat],\n                [min_lon, min_lat]\n            ]\n        ],\n        \"type\": \"Polygon\"\n    }\n    aoi_shape = geometry.shape(aoi_geom)\n    # Extract bounding box coordinates\n    minx, miny, maxx, maxy = aoi_shape.bounds\n    # get tiles intersect bbox\n    quad_keys = set()\n    for tile in list(mercantile.tiles(minx, miny, maxx, maxy, zooms=9)):\n        quad_keys.add(mercantile.quadkey(tile))\n    quad_keys = list(quad_keys)\n    # Download the building footprints for each tile and crop with bbox\n    df = pd.read_csv(\n        \"https://minedbuildings.z5.web.core.windows.net/global-buildings/dataset-links.csv\", dtype=str\n    )\n\n    idx = 0\n    combined_gdf = gpd.GeoDataFrame()\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Download the GeoJSON files for each tile that intersects the input geometry\n        tmp_fns = []\n        for quad_key in tqdm(quad_keys):\n            rows = df[df[\"QuadKey\"] == quad_key]\n            if rows.shape[0] == 1:\n                url = rows.iloc[0][\"Url\"]\n\n                df2 = pd.read_json(url, lines=True)\n                df2[\"geometry\"] = df2[\"geometry\"].apply(geometry.shape)\n\n                gdf = gpd.GeoDataFrame(df2, crs=4326)\n                fn = os.path.join(tmpdir, f\"{quad_key}.geojson\")\n                tmp_fns.append(fn)\n                if not os.path.exists(fn): # Skip if file already exists\n                    gdf.to_file(fn, driver=\"GeoJSON\")\n            elif rows.shape[0] &gt; 1:\n                raise ValueError(f\"Multiple rows found for QuadKey: {quad_key}\")\n            else:\n                raise ValueError(f\"QuadKey not found in dataset: {quad_key}\")\n        # Merge the GeoJSON files into a single file\n        for fn in tmp_fns:\n            gdf = gpd.read_file(fn)  # Read each file into a GeoDataFrame\n            gdf = gdf[gdf.geometry.within(aoi_shape)]  # Filter geometries within the AOI\n            gdf['id'] = range(idx, idx + len(gdf))  # Update 'id' based on idx\n            idx += len(gdf)\n            combined_gdf = pd.concat([combined_gdf,gdf],ignore_index=True)\n\n    # Reproject to a UTM CRS for accurate area measurement\n    utm_crs = combined_gdf.estimate_utm_crs()  \n    # Compute area and filter buildings by area\n    combined_gdf = combined_gdf.to_crs(utm_crs)\n    combined_gdf[\"area_\"] = combined_gdf.geometry.area\n    combined_gdf = combined_gdf[combined_gdf[\"area_\"] &gt;= min_area]  # Filter min area\n    if max_area:\n        combined_gdf = combined_gdf[combined_gdf[\"area_\"] &lt;= max_area]  # Filter max area\n    # Reproject back to WGS84\n    combined_gdf.to_crs('EPSG:4326')\n    return combined_gdf\n</code></pre>"}]}